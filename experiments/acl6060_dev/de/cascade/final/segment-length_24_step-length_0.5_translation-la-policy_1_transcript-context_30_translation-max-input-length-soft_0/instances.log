{"index": 0, "prediction": "Hallo, das ist Xiaowei Zhou von der Harvard University. Ich freue mich sehr, unsere Arbeit \u00fcber Online-Semantic Parsing f\u00fcr Latency Reduction in task-oriented Dialog zu pr\u00e4sentieren. Dies ist eine gemeinsame Arbeit mit Jason, Michael, Anthony und Sam von Microsoft Semantic Machines. Im Aufgabenorientierten Dialog interagiert ein Benutzer mit dem System, das Anfragen von Benutzerautorisierten Referenzen bearbeitet, in der Regel im Sprechen. Ab dem Ende der Benutzerreferenz zur Systemreaktion, gibt es oft eine bemerkenswerte Verz\u00f6gerung. Unter dem Hood wird die User Utterance \u00fcbersetzt in eine ausgef\u00fchrbare Programm, die dann ausgef\u00fchrt wird, damit das System richtig reagieren kann. Hier ist das Programm als semantisches Grafik dargestellt, das die Berechnung beschreibt, wo es eine Funktionsanforderung darstellt und seine Kinder die Argumente sind. Die Netzknoten markieren sofortige Operationen, aber die anderen sind langsam zu durchf\u00fchren. Denken Sie daran, dass im Gegensatz zum einfachen Beispiel hier wir zeigen, diese Programme k\u00f6nnen oft komplizierter Grafiken au\u00dferhalb der Baumstrukturen sein. In diesem Gespr\u00e4ch fragen wir uns die Frage, k\u00f6nnen wir anfangen, das Programm zu generieren und es durchzuf\u00fchren, bevor der Benutzer sogar die Ausgabe beendet, so dass die schnellere Antwort durch das System erreicht werden kann? Dies ist eine Online-Vorhersage und Entscheidungsprobleme. Es gibt viele andere in diesem Bereich. Beispiele umfassen gleichzeitige \u00dcbersetzung, in der ein lebender Dolmetscher eine Sprache in eine andere in Echtzeit \u00fcbersetzt, intelligente Text-Autocompletion, um den Benutzer zu gedenken, und UberPool, die, wo die Fahrer versandt werden, wo sie ben\u00f6tigt werden k\u00f6nnten. die vorhersehbare Nachfrage. All diese Szenarien haben eine Sache gemeinsam, das hei\u00dft, es ist n\u00fctzlich, Entscheidungen zu treffen, bevor Sie alle Eintr\u00e4ge sehen. In unserem Fall werden wir mit dem Online-Semantikparting umgehen, was erwartet werden k\u00f6nnte, schwierig zu sein, da wir davon ausgehen m\u00fcssen, was der Benutzer sagen k\u00f6nnte und es ist auch untersucht mit keine formalen Bewertungsmetrik. Zun\u00e4chst schauen wir uns an, wie ein normales System funktioniert. Es funktioniert offline durch Parsing auf das Programm nur am Ende der Benutzererkl\u00e4rung. Hier wird das Charaktergrafik vorausgesagt, nachdem alle Informationen zu sehen sind. Im Gegensatz dazu bieten wir ein Online-System an, das auf jedem Austerance-Prefix abgelaufen kann. Zum Beispiel jedes Mal, wenn wir einen neuen Token sehen, prognostizieren wir ein neues Grafik. Bitte beachten Sie, dass Fehler auftreten k\u00f6nnen. An der Position, an der Pool-Party mit Barack Obama, erhalten wir einen Graf mit den richtigen Knoten auf die Person und das Ereignis Gegenstand, aber bekommen wir die falsche Timing-Information. Dieser Prozess geht weiter, bis wir die vollst\u00e4ndige Benutzererlaubnis erhalten. Dies wird die Timeline der Durchf\u00fchrung beeinflussen. Im Offline-System erhalten wir das Programm-Grafik am Ende, damit das System an diesem Punkt ausf\u00fchren kann. Denken Sie daran, dass die grauen Knoten schnelle Operationen sind, so dass wir nur die Ausf\u00fchrungszeitlinie der farbigen langsamen Funktionen ber\u00fccksichtigen. Zun\u00e4chst k\u00f6nnen diese beiden Personenfunktionen parallel ausgef\u00fchrt werden, wobei wei\u00dfe aus der roten Box hervorgehoben werden, und sie haben keine Abh\u00e4ngigkeit von anderen Funktionen. Danach kann die Node erstellen Ereignis dann ausgef\u00fchrt werden, nachdem Sie Ergebnisse aus niedrigeren Noden erhalten, und dann die obere Funktion erreicht, so dass das ganze Programm abgeschlossen ist. Der Durchf\u00fchrungsprozess ist auf die Programmabh\u00e4ngigkeitsstruktur beschr\u00e4nkt, wo einige Operationen nicht parallel gemacht werden k\u00f6nnen, was zu einer bemerkenswerten Verz\u00f6gerung f\u00fchrt. Danke f\u00fcr die Aufsicht. In unserem Online-System, wo wir vorhersagen, wie wir gehen, kann die Ausf\u00fchrung des Programms fr\u00fcher beginnen. Hier, auf der Vorwahl nach Obama, vorhersagen wir sicher, dass die FindPerson-Funktion im Programm sein sollte, aber der Rest kann Fehler enthalten, da sie ausger\u00e4umt werden. Die Ausf\u00fchrung der Node kann sofort als Schlamm begonnen werden. Wieder, mit mehr Tokens, vorhersagen wir ein v\u00f6llig neues Grafik, aber ein Teil davon ist bereits durchgef\u00fchrt, so m\u00fcssen wir nur die \u00fcbrigen Knoten ber\u00fccksichtigen, die wir auch vertrauen. Hier finden Sie eine weitere Person kann parallel ausgef\u00fchrt werden. Wieder, wir k\u00f6nnen falsche Vorhersagen haben. Mit mehr Text haben wir mehr F\u00e4higkeit, es richtig zu machen, wie zum Beispiel die Ereigniszeit hier, wo AM auch richtig vorausgesagt wird. Dann k\u00f6nnen wir den Rest nach dem Programmabh\u00e4ngigkeitsstruktur ausf\u00fchren. Durch die \u00dcbersetzung der Ausf\u00fchrungszeitlinie mit der Ausf\u00fchrungszeitlinie sparen wir eine gro\u00dfe Menge Zeit. Deshalb stellen wir die Aufgabe der online semantischen Parsing vor, wenn die grundlegende Annahme ist, dass die Ausf\u00fchrungszeitpunkt die Vorhersagezeit des Modells kombiniert. So k\u00f6nnen wir nur Zeit verdienen, indem wir fr\u00fcher vorhersagen. Eine andere Annahme ist, dass als Vorhersage und Vollstreckung im Hintergrund passiert, der f\u00fcr die Benutzer nicht sichtbar ist, es nicht notwendig ist, eine konsistente Parsing-Historie zu erhalten. So reparieren wir von Scratch nach jedem Token. Insbesondere stellen wir einen zweistufigen Ansatz vor. Ein vorgeschlagener Schritt, der einen Graf mit vollst\u00e4ndiger Struktur und einen ausgew\u00e4hlten Schritt vorhersagt, der die Knoten ausw\u00e4hlt, die zu diesem Zeitpunkt ausgef\u00fchrt werden sollten. Wir haben zwei Varianten der vorgeschlagenen Methode. Der erste Ansatz kombiniert eine Sprachmodellvollst\u00e4ndigung mit vollem Ausdruck zum Grafikparsing. Insbesondere ist der Pr\u00e4fix Aptobama zuerst durch einen feinen BART-Sprachmodell abgeschlossen und dann in ein Programm mit vollem Offline Parser \u00fcbersetzt. Der zweite Ansatz vorschreibt direkt das Programm von Nutzer-Utterance-Prefix. Dies wird durch das tunen einer einzigen online parser zum \u00fcbersetzen auf das Zielgrafik von jedem prefix erreicht. Dies erleichtert das Modell das richtige Vorhersagen zu lernen. Ein bisschen detaillierter, wie generieren wir die Grafiken? Wir formulieren das Problem durch die Erzeugung einer seriellen Version des Grafs. Jede Node oder Edge wird durch eine Aktion dargestellt. Hier beginnen wir mit dem ersten Knoten. Die Nummer unten erfasst den absoluten Index in der Aktionsgeschichte. Dann bekommen wir den zweiten Knoten. Nachfolgend ist die Grenze zwischen ihnen. Es enth\u00e4lt den Indikator zum Index eines vorherigen Knoten und das Edge-Label. Null bedeutet hier den neuesten Knoten mit dem von der Zeroth-Aktion generierten Knoten zu verbinden. Und der n\u00e4chste Node, der n\u00e4chste Rand, dieser Prozess geht weiter, bis wir die vollst\u00e4ndige Grafik erzeugen. Das Grundmodell basiert auf einem Transformator mit einem Selbstpunktionsmechanismus. \u00c4hnlich wie eine vorherige \u00dcbergangsbasis. Nach der Erstellung eines vollst\u00e4ndigen Grafs erhalten wir die Wahrscheinlichkeiten auf Aktionsniveau, die den verschiedenen Teilen des Grafs entsprechen. Wir w\u00e4hlen Vertrauen auf der Grundlage der heuristischen Grenze, die ausgef\u00fchrt werden soll. Sp\u00e4ter werden wir die Grenze \u00e4ndern, um verschiedene Tradeoffs zwischen der Lateenzminderung und der Ausf\u00fchrungskosten zu erreichen. F\u00fcr die formelle Bewertung der Online-Methoden stellen wir eine endg\u00fcltige Lateenzminderung oder eine FLR-Metrik vor. Hier ist ein Feedback dar\u00fcber, wie ein Offline-System die Ausf\u00fchrungszeitlinie beendet. In Online-Systemen ist die Durchf\u00fchrung mit der Auszahlungszeitlinie \u00fcberschritten, so dass sie fr\u00fcher endet. FLR wird als eine Verringerung der Zeit im Vergleich zum Offline-System definiert, die mit dem Ende der Ausf\u00fchrung markiert ist. Wir f\u00fchren Experimente auf zwei gro\u00dfen konvertierenden semantischen Parsing-Datensetten wie MCUFLO und 3DST durch. Unser Graph-basiertes Parser wenn offline betrieben wird, erreicht state-of-the-art-Performance bei Parsing auf beiden Datens\u00e4tzen. Das Outline-Complete-Modell erreicht auch nicht-triviale blaue Gewinne im Vergleich zu der einfachen Baseline der Node-Complete. Nun, schauen wir uns die Vorhersage-Genauigkeit unseres Prefix-to-Graph Parser an. Wir testen die matched F1 score von graph tuples zwischen der generation und go-graph invalidation data in y-axis f\u00fcr jede prefix length in x-axis vertreten durch prozentuale. Jede dieser Kurven repr\u00e4sentiert ein anderes Modell mit dem einzigen Unterschied in Trainingsdaten. Dies ist der Offline-Parser, und wir vermischen in Prefix-Daten in verschiedenen L\u00e4ngen um das Modell zu einem Online-Parser zu \u00fcbertragen. Zum Beispiel bedeutet die Legende Prefix 80%+, dass das Modell mit Prefix-Daten mit Prefix-L\u00e4nge gr\u00f6\u00dfer als 80% der vollst\u00e4ndigen Austerenzl\u00e4nge trainiert wird. Der obere linke Ecke ist das Designbereich. Wie wir sehen k\u00f6nnen, ist der Offline-Parser in der schwarzen Kurve nicht gut auf den Prefix-Daten. Wenn wir mehr Vorurteile im Training vermischen, erhebt die Kurve oben und links und funktioniert besser auf der gesamten Vorurteile. Allerdings ist die vollst\u00e4ndige Auswirkung der Ausgangsleistung im oberen rechten Punkt nicht beeintr\u00e4chtigt. Basierend auf diesen starken Ergebnissen, wie viel Latenz reduzieren wir? Wir messen die Zeit durch die Anzahl der Quell-Token und simulieren verschiedene Funktions-Exekutionszeiten. Die Kurven zeigen den Tradeoff zwischen der FLR-Metrik und dem Ausf\u00fchrungskosten, der durch die Anzahl der \u00fcberm\u00e4\u00dfigen Funktionsanrufe gemessen wird, die nicht richtig sind. Dies wird durch die Ver\u00e4nderung der Subgraph Selektionsgrenze erreicht. Eine h\u00f6here Grenze w\u00e4hlt weniger Funktionen von Fehler, erh\u00e4lt aber eine geringere FLR, w\u00e4hrend die niedrigere Grenze aggressiver ausgew\u00e4hlt und Programme ausf\u00fchrt. Wir vergleichen die beiden Ans\u00e4tze, die wir vorgeschlagen haben und die Grundlinie, die nichts anderes macht als direkt den Offline Parser f\u00fcr Online-Nutzung anzuwenden. Die obere linke Region hat die beste FLR und Kostentradeoff. Wir sehen, dass beide unserer Methoden die Baseline mit einer gro\u00dfen Marge besiegen und sie mehr \u00e4hnlich auf 3D ausf\u00fchren. Die DST. W\u00e4hrend die individuelle Funktion schneller ist, neigt es dazu, mehr Exekutionen und niedrigere Lateenzreduktionsr\u00e4ume zu haben. W\u00e4hrend die individuelle Funktions\u00fcbung langsamer ist, gibt es mehr Platz f\u00fcr die Verbesserung der FLR. Unsere beiden Ans\u00e4tze erzielen bessere Leistung in verschiedenen Kostenregionen. Im Allgemeinen haben wir bis zu 63% relative Laktationsreduktion erreicht, abh\u00e4ngig von der Ausf\u00fchrungszeit und den erlaubten Kosten. Schlie\u00dflich haben wir einen Bruch der durchschnittlichen Latenzreduktion in Tokens f\u00fcr jede Art der Funktionsnode, wenn die zul\u00e4ssige Kosten 3 Run-Exektionen sind. Wie wir sehen k\u00f6nnen, gibt es Gewinne \u00fcberall im Board. Es gibt auch einige Funktionen, auf denen wir eine beeindruckende Latenzreduktion erhalten, wo die rote Bar viel l\u00e4nger ist, wie z. B. Manager und Empf\u00e4nger finden. Diese sind Low-Level-Funktionen, die nicht viel von anderen abh\u00e4ngig sind. Abschlie\u00dfend stellen wir online semantische Parsing als eine neue Aufgabe zur Erforschung mit der strengen Latenzreduktion, die mit einem starken Graph-basierten semantischen Parser vereinbar ist, wir haben eine relativ gute Latenzreduktion entweder durch einen Pipeline-Ansatz mit LLM-Abschluss erzielt. ein volles Parser, oder direkt durch einen gelehrten Parser auf den Prefix. Dar\u00fcber hinaus kann unser Ansatz ein allgemeines Rahmen sein und auf andere Ausf\u00fchrbare semantische Darstellungen in verschiedenen Bereichen angewandt werden. Die k\u00fcnftigen Arbeiten k\u00f6nnten intelligenter Prognose und Implementierungsintegrationsmethoden erforschen. Vielen Dank f\u00fcr Ihre H\u00f6ren.", "delays": [2500.0, 2500.0, 2500.0, 3000.0, 3500.0, 3500.0, 4500.0, 4500.0, 4500.0, 5500.0, 6000.0, 6000.0, 6000.0, 6500.0, 7000.0, 8500.0, 9000.0, 9000.0, 9000.0, 9000.0, 10000.0, 12000.0, 12000.0, 12000.0, 12000.0, 12000.0, 13000.0, 13000.0, 13000.0, 13000.0, 13000.0, 13000.0, 13000.0, 14000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15500.0, 20000.0, 20000.0, 21500.0, 21500.0, 22000.0, 22000.0, 22000.0, 22000.0, 22500.0, 22500.0, 24000.0, 24000.0, 26000.0, 26000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28000.0, 29000.0, 29000.0, 29000.0, 30500.0, 31000.0, 31000.0, 31000.0, 31500.0, 31500.0, 31500.0, 35000.0, 35000.0, 35000.0, 36000.0, 36000.0, 36000.0, 36500.0, 36500.0, 36500.0, 36500.0, 36500.0, 37000.0, 37500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 43000.0, 43000.0, 43000.0, 44000.0, 44000.0, 44000.0, 44000.0, 45000.0, 46500.0, 46500.0, 46500.0, 47000.0, 48000.0, 49000.0, 49000.0, 49000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 54000.0, 55000.0, 55000.0, 55000.0, 57000.0, 57000.0, 57000.0, 57000.0, 57000.0, 57000.0, 58000.0, 58000.0, 58000.0, 59000.0, 59000.0, 59000.0, 59000.0, 60000.0, 60000.0, 60500.0, 60500.0, 60500.0, 61000.0, 61000.0, 61500.0, 62000.0, 62500.0, 62500.0, 63000.0, 64500.0, 67500.0, 67500.0, 67500.0, 67500.0, 67500.0, 68500.0, 68500.0, 69000.0, 69000.0, 69000.0, 70000.0, 70000.0, 70000.0, 70500.0, 70500.0, 71500.0, 71500.0, 71500.0, 71500.0, 71500.0, 72000.0, 73000.0, 74500.0, 74500.0, 74500.0, 74500.0, 74500.0, 77000.0, 77000.0, 77000.0, 77000.0, 77000.0, 77000.0, 77000.0, 77000.0, 80000.0, 80000.0, 80000.0, 80000.0, 80000.0, 80000.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 82500.0, 83500.0, 83500.0, 84000.0, 84000.0, 84000.0, 84500.0, 84500.0, 85500.0, 88000.0, 88000.0, 88000.0, 88000.0, 88000.0, 88000.0, 89000.0, 89000.0, 90000.0, 90500.0, 91000.0, 91000.0, 91000.0, 92000.0, 92000.0, 92000.0, 96000.0, 96000.0, 96000.0, 96000.0, 96000.0, 96000.0, 96000.0, 96000.0, 98500.0, 99000.0, 99000.0, 99000.0, 99000.0, 99000.0, 100000.0, 100000.0, 100000.0, 100000.0, 100000.0, 100000.0, 103000.0, 103000.0, 103000.0, 105500.0, 105500.0, 105500.0, 105500.0, 105500.0, 105500.0, 105500.0, 105500.0, 105500.0, 106000.0, 106000.0, 107000.0, 107500.0, 107500.0, 108000.0, 108000.0, 108500.0, 110000.0, 110000.0, 110000.0, 111000.0, 111000.0, 111500.0, 112000.0, 112000.0, 113500.0, 114000.0, 114000.0, 114000.0, 115500.0, 115500.0, 115500.0, 117000.0, 117000.0, 117000.0, 117500.0, 117500.0, 117500.0, 118000.0, 118000.0, 118000.0, 118000.0, 118500.0, 119000.0, 119500.0, 120000.0, 120000.0, 121500.0, 121500.0, 121500.0, 126500.0, 126500.0, 126500.0, 126500.0, 126500.0, 127500.0, 127500.0, 127500.0, 127500.0, 127500.0, 127500.0, 128000.0, 129500.0, 129500.0, 129500.0, 131000.0, 131000.0, 131500.0, 131500.0, 131500.0, 133000.0, 133000.0, 133000.0, 133000.0, 133500.0, 133500.0, 134500.0, 134500.0, 136500.0, 137000.0, 138000.0, 139000.0, 139000.0, 139000.0, 139000.0, 139000.0, 139000.0, 139000.0, 139000.0, 140500.0, 140500.0, 141000.0, 141500.0, 141500.0, 142500.0, 142500.0, 143000.0, 143000.0, 143500.0, 144000.0, 144000.0, 144000.0, 144000.0, 146500.0, 147000.0, 147000.0, 147000.0, 147000.0, 147000.0, 148000.0, 148000.0, 148500.0, 148500.0, 149000.0, 149000.0, 150500.0, 150500.0, 150500.0, 158500.0, 158500.0, 158500.0, 158500.0, 158500.0, 158500.0, 158500.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160000.0, 160500.0, 161000.0, 161000.0, 161000.0, 163000.0, 163000.0, 163000.0, 163000.0, 164000.0, 164000.0, 165500.0, 165500.0, 165500.0, 165500.0, 168000.0, 168000.0, 168000.0, 168000.0, 173000.0, 173000.0, 173000.0, 173000.0, 173000.0, 173000.0, 173000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 175000.0, 176000.0, 176500.0, 176500.0, 176500.0, 183000.0, 183000.0, 183000.0, 183000.0, 183000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 184000.0, 188500.0, 188500.0, 188500.0, 190500.0, 190500.0, 190500.0, 190500.0, 190500.0, 191500.0, 191500.0, 191500.0, 193000.0, 193000.0, 193000.0, 193000.0, 193000.0, 193000.0, 193000.0, 193000.0, 193500.0, 193500.0, 194500.0, 194500.0, 198000.0, 198000.0, 198000.0, 198000.0, 204000.0, 204000.0, 204000.0, 204000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 209000.0, 211500.0, 211500.0, 211500.0, 211500.0, 214500.0, 214500.0, 214500.0, 214500.0, 214500.0, 214500.0, 215000.0, 215000.0, 215000.0, 216000.0, 216000.0, 216000.0, 216000.0, 216000.0, 216000.0, 216000.0, 219000.0, 240000.0, 240000.0, 240000.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 240500.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241000.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 242500.0, 242500.0, 242500.0, 243000.0, 245000.0, 245000.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 247000.0, 247500.0, 248000.0, 248000.0, 248000.0, 248000.0, 250500.0, 250500.0, 250500.0, 250500.0, 250500.0, 250500.0, 252500.0, 252500.0, 253500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 260000.0, 260000.0, 260000.0, 260000.0, 260000.0, 260000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261500.0, 262500.0, 263000.0, 263000.0, 263000.0, 263500.0, 264000.0, 264000.0, 264000.0, 264000.0, 265000.0, 265000.0, 265000.0, 266500.0, 268000.0, 268000.0, 268000.0, 269000.0, 269000.0, 269000.0, 271000.0, 271000.0, 271000.0, 271000.0, 273500.0, 273500.0, 274500.0, 274500.0, 277000.0, 277000.0, 277000.0, 277000.0, 277000.0, 277500.0, 282500.0, 282500.0, 282500.0, 282500.0, 282500.0, 282500.0, 283500.0, 283500.0, 283500.0, 283500.0, 283500.0, 284000.0, 285500.0, 285500.0, 285500.0, 287500.0, 287500.0, 287500.0, 287500.0, 287500.0, 287500.0, 287500.0, 288000.0, 290000.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 292500.0, 292500.0, 292500.0, 292500.0, 292500.0, 294500.0, 294500.0, 294500.0, 294500.0, 294500.0, 295500.0, 296000.0, 296000.0, 296000.0, 296000.0, 296000.0, 296500.0, 297000.0, 297500.0, 298500.0, 299000.0, 301000.0, 301500.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 302000.0, 303000.0, 303500.0, 304500.0, 304500.0, 304500.0, 306000.0, 307000.0, 307000.0, 307000.0, 307000.0, 307000.0, 307000.0, 307000.0, 308500.0, 309000.0, 309000.0, 310500.0, 310500.0, 310500.0, 310500.0, 313500.0, 313500.0, 313500.0, 313500.0, 313500.0, 313500.0, 314000.0, 315000.0, 315500.0, 315500.0, 317000.0, 317000.0, 317000.0, 317000.0, 317000.0, 317500.0, 319500.0, 319500.0, 319500.0, 322500.0, 322500.0, 322500.0, 322500.0, 322500.0, 322500.0, 325000.0, 325000.0, 325000.0, 325000.0, 325000.0, 325000.0, 325000.0, 326500.0, 326500.0, 326500.0, 328000.0, 328000.0, 330000.0, 330000.0, 330000.0, 330000.0, 330000.0, 330000.0, 331500.0, 333500.0, 333500.0, 333500.0, 333500.0, 334000.0, 335000.0, 335500.0, 335500.0, 338000.0, 338000.0, 338000.0, 338000.0, 338500.0, 340000.0, 341500.0, 341500.0, 341500.0, 341500.0, 341500.0, 341500.0, 342500.0, 342500.0, 342500.0, 346500.0, 346500.0, 346500.0, 346500.0, 346500.0, 346500.0, 347500.0, 348000.0, 348000.0, 350000.0, 350000.0, 352000.0, 352000.0, 352000.0, 352000.0, 353000.0, 353000.0, 353000.0, 353000.0, 353000.0, 353000.0, 353000.0, 353000.0, 353000.0, 354000.0, 354000.0, 354500.0, 354500.0, 355000.0, 358500.0, 358500.0, 358500.0, 358500.0, 362000.0, 362000.0, 362000.0, 362000.0, 362000.0, 362000.0, 362000.0, 362000.0, 363500.0, 364000.0, 364000.0, 364000.0, 364000.0, 364000.0, 364000.0, 364500.0, 365500.0, 365500.0, 365500.0, 365500.0, 367000.0, 367000.0, 367500.0, 367500.0, 368000.0, 368500.0, 371000.0, 371000.0, 371000.0, 372500.0, 372500.0, 372500.0, 372500.0, 372500.0, 372500.0, 372500.0, 373500.0, 375500.0, 375500.0, 376500.0, 376500.0, 376500.0, 376500.0, 376500.0, 376500.0, 376500.0, 379000.0, 379000.0, 379000.0, 379000.0, 379000.0, 379000.0, 380500.0, 380500.0, 380500.0, 380500.0, 380500.0, 380500.0, 381500.0, 381500.0, 382500.0, 382500.0, 384000.0, 384000.0, 385000.0, 385000.0, 385000.0, 385000.0, 386000.0, 386000.0, 387000.0, 387500.0, 387500.0, 390000.0, 390000.0, 390000.0, 390000.0, 390000.0, 390500.0, 392000.0, 392000.0, 392000.0, 392000.0, 392000.0, 392000.0, 394500.0, 395000.0, 395000.0, 395000.0, 395000.0, 395000.0, 395000.0, 395000.0, 395000.0, 396000.0, 396500.0, 396500.0, 396500.0, 397000.0, 398000.0, 398000.0, 398000.0, 399500.0, 400500.0, 400500.0, 400500.0, 401000.0, 401000.0, 401000.0, 402000.0, 402000.0, 403500.0, 403500.0, 404500.0, 404500.0, 407000.0, 408000.0, 409500.0, 409500.0, 409500.0, 410000.0, 410000.0, 412000.0, 412000.0, 412000.0, 412500.0, 412500.0, 412500.0, 413000.0, 414500.0, 414500.0, 414500.0, 414500.0, 414500.0, 414500.0, 415500.0, 415500.0, 416500.0, 418500.0, 418500.0, 418500.0, 418500.0, 420500.0, 420500.0, 420500.0, 420500.0, 420500.0, 420500.0, 421500.0, 422000.0, 422000.0, 422000.0, 422500.0, 423500.0, 423500.0, 424500.0, 424500.0, 424500.0, 424500.0, 425000.0, 425500.0, 426000.0, 428500.0, 428500.0, 428500.0, 430000.0, 431000.0, 431000.0, 431000.0, 431000.0, 432000.0, 434000.0, 434000.0, 434000.0, 434000.0, 434500.0, 435000.0, 436000.0, 436000.0, 436000.0, 437000.0, 437000.0, 437500.0, 438000.0, 438000.0, 438000.0, 438000.0, 438500.0, 439500.0, 441000.0, 441000.0, 442000.0, 444000.0, 444000.0, 444000.0, 444000.0, 444000.0, 445500.0, 447000.0, 447000.0, 447000.0, 447000.0, 447000.0, 447000.0, 447000.0, 448000.0, 448500.0, 448500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 451500.0, 452000.0, 455500.0, 455500.0, 455500.0, 455500.0, 455500.0, 455500.0, 455500.0, 455500.0, 455500.0, 457500.0, 457500.0, 457500.0, 458000.0, 458000.0, 458500.0, 463000.0, 463000.0, 463000.0, 463000.0, 463000.0, 463000.0, 463000.0, 463000.0, 465000.0, 465000.0, 465000.0, 466500.0, 466500.0, 466500.0, 466500.0, 467000.0, 469500.0, 470000.0, 470000.0, 470000.0, 470000.0, 473000.0, 475500.0, 475500.0, 475500.0, 475500.0, 475500.0, 476000.0, 476000.0, 476000.0, 476000.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 480000.0, 482000.0, 482000.0, 482000.0, 482000.0, 483000.0, 487000.0, 487000.0, 487000.0, 487000.0, 487000.0, 488000.0, 488000.0, 488000.0, 488000.0, 488000.0, 488500.0, 488500.0, 488500.0, 488500.0, 489000.0, 490000.0, 490000.0, 490500.0, 491000.0, 492500.0, 494500.0, 494500.0, 495000.0, 495000.0, 496000.0, 496000.0, 496000.0, 496000.0, 496500.0, 496500.0, 499500.0, 499500.0, 501000.0, 501000.0, 501000.0, 502000.0, 502000.0, 502000.0, 502000.0, 502000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 508000.0, 509000.0, 509500.0, 510500.0, 511000.0, 511000.0, 511000.0, 511000.0, 516500.0, 516500.0, 516500.0, 518500.0, 518500.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519000.0, 519500.0, 521500.0, 521500.0, 521500.0, 521500.0, 525500.0, 525500.0, 525500.0, 525500.0, 525500.0, 527500.0, 527500.0, 527500.0, 527500.0, 527500.0, 527500.0, 527500.0, 529000.0, 529000.0, 529000.0, 529000.0, 529000.0, 529000.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 530500.0, 533000.0, 533000.0, 533000.0, 533000.0, 534000.0, 534000.0, 534000.0, 536000.0, 536500.0, 536500.0, 536500.0, 538000.0, 538000.0, 538000.0, 538000.0, 538000.0, 538500.0, 538500.0, 538500.0, 539000.0, 539000.0, 540500.0, 540500.0, 540500.0, 543500.0, 543500.0, 543500.0, 543500.0, 543500.0, 545000.0, 548500.0, 548500.0, 548500.0, 548500.0, 548500.0, 548500.0, 548500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 551000.0, 552000.0, 552000.0, 553000.0, 553000.0, 553000.0, 553000.0, 554000.0, 554000.0, 555500.0, 555500.0, 557500.0, 557500.0, 557500.0, 557500.0, 557500.0, 559500.0, 559500.0, 560500.0, 561000.0, 561000.0, 561000.0, 561000.0, 561500.0, 561500.0, 565000.0, 565000.0, 565000.0, 565000.0, 565000.0, 565000.0, 565000.0, 565500.0, 566000.0, 566000.0, 566000.0, 566000.0, 567000.0, 567000.0, 567000.0, 568000.0, 568000.0, 568000.0, 569000.0, 569000.0, 571000.0, 571000.0, 571000.0, 571000.0, 572000.0, 572000.0, 572000.0, 572000.0, 573500.0, 573500.0, 574500.0, 574500.0, 574500.0, 574500.0, 576000.0, 576000.0, 578500.0, 578500.0, 578500.0, 578500.0, 578500.0, 579000.0, 581000.0, 581000.0, 581000.0, 581000.0, 582000.0, 582000.0, 582000.0, 582000.0, 582500.0, 582500.0, 582500.0, 583000.0, 583000.0, 583500.0, 584000.0, 584500.0, 584500.0, 585000.0, 585000.0, 585000.0, 585000.0, 587000.0, 587500.0, 591500.0, 591500.0, 591500.0, 591500.0, 591500.0, 592500.0, 592500.0, 592500.0, 592500.0, 592500.0, 592500.0, 592500.0, 592500.0, 593000.0, 595000.0, 596000.0, 598500.0, 598500.0, 598500.0, 598500.0, 598500.0, 598500.0, 598500.0, 599000.0, 599000.0, 599000.0, 599000.0, 599000.0, 599000.0, 599000.0, 600000.0, 600000.0, 600000.0, 600000.0, 600000.0, 602500.0, 602500.0, 604000.0, 604500.0, 604500.0, 606000.0, 606500.0, 607000.0, 607000.0, 607000.0, 607000.0, 607000.0, 608500.0, 609500.0, 612500.0, 612500.0, 612500.0, 612500.0, 614000.0, 614000.0, 614000.0, 615000.0, 615000.0, 615000.0, 615000.0, 615000.0, 615500.0, 615500.0, 616000.0, 617500.0, 617500.0, 617500.0, 617500.0, 619000.0, 619000.0, 619000.0, 620500.0, 620500.0, 620500.0, 621000.0, 623000.0, 623000.0, 625500.0, 625500.0, 625500.0, 625500.0, 628500.0, 628500.0, 628500.0, 628500.0, 629500.0, 629500.0, 631500.0, 631500.0, 631500.0, 631500.0, 631500.0, 635000.0, 635000.0, 635000.0, 636500.0, 636500.0, 636500.0, 636500.0, 636500.0, 638000.0, 638000.0, 638500.0, 639500.0, 640000.0, 640000.0, 640500.0, 640500.0, 641000.0, 642000.0, 642000.0, 643000.0, 643000.0, 643500.0, 645000.0, 645000.0, 645000.0, 646000.0, 646000.0, 646500.0, 646500.0, 647000.0, 647000.0, 647500.0, 648000.0, 648000.0, 648000.0, 649000.0, 649000.0, 650500.0, 650500.0, 652000.0, 653000.0, 653000.0, 653000.0, 654000.0, 654000.0, 655000.0, 655000.0, 655000.0, 655000.0, 655000.0, 655000.0, 656000.0, 656500.0, 656500.0, 658000.0, 658000.0, 658000.0, 658000.0, 658000.0, 658000.0, 658000.0, 659000.0, 659000.0, 661000.0, 661000.0, 661500.0, 662500.0, 665500.0, 665500.0, 665500.0, 665500.0, 667500.0, 669500.0, 669500.0, 671000.0, 671000.0, 671000.0, 671000.0, 671000.0, 671000.0, 671000.0, 671500.0, 671500.0, 672000.0, 674000.0, 674000.0, 675000.0, 675000.0, 675000.0, 677000.0, 677000.0, 677000.0, 677000.0, 677000.0, 677000.0, 677000.0, 678500.0, 678500.0, 678500.0, 678500.0, 678500.0, 679000.0, 680000.0, 680000.0, 680500.0, 680500.0, 681500.0, 682000.0, 682000.0, 683500.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 686000.0, 687000.0, 687500.0, 688500.0, 688500.0, 689000.0, 690000.0, 690000.0, 690000.0, 690000.0, 690500.0, 691000.0, 692000.0, 692500.0, 694000.0, 695500.0, 695500.0, 695500.0, 695500.0, 695500.0, 695500.0, 697500.0, 697500.0, 697500.0, 697500.0, 697500.0, 699000.0, 699000.0, 700000.0, 700000.0, 703007.375, 703007.375, 703007.375, 703007.375, 703007.375], "elapsed": [13255.247116088867, 13255.247116088867, 13255.247116088867, 14149.383306503296, 15059.02123451233, 15059.02123451233, 16781.804084777832, 16781.804084777832, 16781.804084777832, 18840.251684188843, 20278.393268585205, 20278.393268585205, 20278.393268585205, 21316.561937332153, 22429.077863693237, 26308.412075042725, 27698.323249816895, 27698.323249816895, 27698.323249816895, 27698.323249816895, 30084.49649810791, 34797.88684844971, 34797.88684844971, 34797.88684844971, 34797.88684844971, 34797.88684844971, 37661.24224662781, 37661.24224662781, 37661.24224662781, 37661.24224662781, 37661.24224662781, 37661.24224662781, 37661.24224662781, 40320.15872001648, 42981.30655288696, 42981.30655288696, 42981.30655288696, 42981.30655288696, 44352.98848152161, 59613.40928077698, 59613.40928077698, 67017.42005348206, 67017.42005348206, 68585.76774597168, 68585.76774597168, 68585.76774597168, 68585.76774597168, 70234.35068130493, 70234.35068130493, 78848.44970703125, 78848.44970703125, 81959.56587791443, 81959.56587791443, 85394.14119720459, 85394.14119720459, 85394.14119720459, 85394.14119720459, 85394.14119720459, 85394.14119720459, 85394.14119720459, 85394.14119720459, 87325.76608657837, 87325.76608657837, 87325.76608657837, 90328.18102836609, 91480.33380508423, 91480.33380508423, 91480.33380508423, 92565.5746459961, 92565.5746459961, 92565.5746459961, 99661.99374198914, 99661.99374198914, 99661.99374198914, 102253.6416053772, 102253.6416053772, 102253.6416053772, 103529.02460098267, 103529.02460098267, 103529.02460098267, 103529.02460098267, 103529.02460098267, 104678.65371704102, 105860.67795753479, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 118849.60770606995, 123994.33445930481, 123994.33445930481, 123994.33445930481, 126878.03936004639, 126878.03936004639, 126878.03936004639, 126878.03936004639, 129722.73874282837, 134091.3109779358, 134091.3109779358, 134091.3109779358, 139020.36905288696, 142008.23092460632, 144331.14743232727, 144331.14743232727, 144331.14743232727, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 152579.24509048462, 154515.1789188385, 154515.1789188385, 154515.1789188385, 158534.87253189087, 158534.87253189087, 158534.87253189087, 158534.87253189087, 158534.87253189087, 158534.87253189087, 160593.08218955994, 160593.08218955994, 160593.08218955994, 162956.81858062744, 162956.81858062744, 162956.81858062744, 162956.81858062744, 165425.6978034973, 165425.6978034973, 166668.26701164246, 166668.26701164246, 166668.26701164246, 167838.3069038391, 167838.3069038391, 169106.11295700073, 170334.01012420654, 171656.1872959137, 171656.1872959137, 172839.79034423828, 176795.51815986633, 184993.9365386963, 184993.9365386963, 184993.9365386963, 184993.9365386963, 184993.9365386963, 187791.99981689453, 187791.99981689453, 189418.0073738098, 189418.0073738098, 189418.0073738098, 194775.74110031128, 194775.74110031128, 194775.74110031128, 197378.18479537964, 197378.18479537964, 200393.8410282135, 200393.8410282135, 200393.8410282135, 200393.8410282135, 200393.8410282135, 201935.48798561096, 203483.85214805603, 206278.51128578186, 206278.51128578186, 206278.51128578186, 206278.51128578186, 206278.51128578186, 210980.14569282532, 210980.14569282532, 210980.14569282532, 210980.14569282532, 210980.14569282532, 210980.14569282532, 210980.14569282532, 210980.14569282532, 216603.6672592163, 216603.6672592163, 216603.6672592163, 216603.6672592163, 216603.6672592163, 216603.6672592163, 220069.85759735107, 220069.85759735107, 220069.85759735107, 220069.85759735107, 220069.85759735107, 222143.44334602356, 226908.06937217712, 226908.06937217712, 228950.5536556244, 228950.5536556244, 228950.5536556244, 230097.39470481873, 230097.39470481873, 232437.21961975098, 238507.87091255188, 238507.87091255188, 238507.87091255188, 238507.87091255188, 238507.87091255188, 238507.87091255188, 241116.90521240234, 241116.90521240234, 243767.73619651794, 245179.2893409729, 246619.6367740631, 246619.6367740631, 246619.6367740631, 249489.25280570984, 249489.25280570984, 249489.25280570984, 261452.54921913147, 261452.54921913147, 261452.54921913147, 261452.54921913147, 261452.54921913147, 261452.54921913147, 261452.54921913147, 261452.54921913147, 265722.93400764465, 266842.85950660706, 266842.85950660706, 266842.85950660706, 266842.85950660706, 266842.85950660706, 268795.0427532196, 268795.0427532196, 268795.0427532196, 268795.0427532196, 268795.0427532196, 268795.0427532196, 274219.7470664978, 274219.7470664978, 274219.7470664978, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 279801.2640476227, 280976.6912460327, 280976.6912460327, 283424.3438243866, 284641.74342155457, 284641.74342155457, 285853.75690460205, 285853.75690460205, 287006.236076355, 290464.77937698364, 290464.77937698364, 290464.77937698364, 293001.2083053589, 293001.2083053589, 294284.2438220978, 295576.55668258667, 295576.55668258667, 299805.26089668274, 303626.1076927185, 303626.1076927185, 303626.1076927185, 308764.63770866394, 308764.63770866394, 308764.63770866394, 313100.3918647766, 313100.3918647766, 313100.3918647766, 314734.8301410675, 314734.8301410675, 314734.8301410675, 316378.1008720398, 316378.1008720398, 316378.1008720398, 316378.1008720398, 317915.2867794037, 319469.012260437, 321029.09541130066, 322668.81585121155, 322668.81585121155, 325851.8223762512, 325851.8223762512, 325851.8223762512, 333726.57561302185, 333726.57561302185, 333726.57561302185, 333726.57561302185, 333726.57561302185, 335764.4610404968, 335764.4610404968, 335764.4610404968, 335764.4610404968, 335764.4610404968, 335764.4610404968, 336683.16078186035, 339415.0083065033, 339415.0083065033, 339415.0083065033, 342300.5495071411, 342300.5495071411, 343407.5758457184, 343407.5758457184, 343407.5758457184, 346737.4665737152, 346737.4665737152, 346737.4665737152, 346737.4665737152, 347874.32742118835, 347874.32742118835, 350070.52087783813, 350070.52087783813, 355257.2548389435, 356614.1173839569, 359504.03356552124, 362149.0969657898, 362149.0969657898, 362149.0969657898, 362149.0969657898, 362149.0969657898, 362149.0969657898, 362149.0969657898, 362149.0969657898, 368371.31905555725, 368371.31905555725, 370595.8375930786, 371958.4894180298, 371958.4894180298, 374860.05306243896, 374860.05306243896, 376279.60562705994, 376279.60562705994, 377783.33568573, 379360.04161834717, 379360.04161834717, 379360.04161834717, 379360.04161834717, 383237.31565475464, 384166.39733314514, 384166.39733314514, 384166.39733314514, 384166.39733314514, 384166.39733314514, 385900.0759124756, 385900.0759124756, 386889.32156562805, 386889.32156562805, 387845.908164978, 387845.908164978, 390617.0859336853, 390617.0859336853, 390617.0859336853, 408190.66858291626, 408190.66858291626, 408190.66858291626, 408190.66858291626, 408190.66858291626, 408190.66858291626, 408190.66858291626, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 413120.1469898224, 414633.6076259613, 416108.1123352051, 416108.1123352051, 416108.1123352051, 421648.4377384186, 421648.4377384186, 421648.4377384186, 421648.4377384186, 426531.51297569275, 426531.51297569275, 432075.24490356445, 432075.24490356445, 432075.24490356445, 432075.24490356445, 442188.65180015564, 442188.65180015564, 442188.65180015564, 442188.65180015564, 450173.11358451843, 450173.11358451843, 450173.11358451843, 450173.11358451843, 450173.11358451843, 450173.11358451843, 450173.11358451843, 452593.08671951294, 452593.08671951294, 452593.08671951294, 452593.08671951294, 452593.08671951294, 452593.08671951294, 454585.4799747467, 456873.2771873474, 458180.5181503296, 458180.5181503296, 458180.5181503296, 473142.8327560425, 473142.8327560425, 473142.8327560425, 473142.8327560425, 473142.8327560425, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 476687.48807907104, 491096.25244140625, 491096.25244140625, 491096.25244140625, 498031.00180625916, 498031.00180625916, 498031.00180625916, 498031.00180625916, 498031.00180625916, 501079.9067020416, 501079.9067020416, 501079.9067020416, 504460.55912971497, 504460.55912971497, 504460.55912971497, 504460.55912971497, 504460.55912971497, 504460.55912971497, 504460.55912971497, 504460.55912971497, 505314.2499923706, 505314.2499923706, 507021.4865207672, 507021.4865207672, 512780.3132534027, 512780.3132534027, 512780.3132534027, 512780.3132534027, 524917.9043769836, 524917.9043769836, 524917.9043769836, 524917.9043769836, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 527541.2499904633, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 528982.2528362274, 533816.4455890656, 533816.4455890656, 533816.4455890656, 533816.4455890656, 533816.4455890656, 533816.4455890656, 533816.4455890656, 537689.5904541016, 544434.9896907806, 544434.9896907806, 544434.9896907806, 544434.9896907806, 553047.6655960083, 553047.6655960083, 553047.6655960083, 553047.6655960083, 553047.6655960083, 553047.6655960083, 554542.1574115753, 554542.1574115753, 554542.1574115753, 557704.7097682953, 557704.7097682953, 557704.7097682953, 557704.7097682953, 557704.7097682953, 557704.7097682953, 557704.7097682953, 561965.4111862183, 595719.6755409241, 595719.6755409241, 595719.6755409241, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 596906.8644046783, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 598427.4423122406, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 599439.3627643585, 601260.7975006104, 601260.7975006104, 601260.7975006104, 602247.1649646759, 606216.7813777924, 606216.7813777924, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 607444.3447589874, 610401.8332958221, 611616.2576675415, 612750.5948543549, 612750.5948543549, 612750.5948543549, 612750.5948543549, 619197.3052024841, 619197.3052024841, 619197.3052024841, 619197.3052024841, 619197.3052024841, 619197.3052024841, 623834.602355957, 623834.602355957, 629033.520936966, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 638201.9557952881, 647826.943397522, 647826.943397522, 647826.943397522, 647826.943397522, 647826.943397522, 647826.943397522, 650960.4392051697, 650960.4392051697, 650960.4392051697, 650960.4392051697, 650960.4392051697, 650960.4392051697, 652496.4005947113, 655503.8049221039, 657155.2891731262, 657155.2891731262, 657155.2891731262, 658794.0714359283, 660526.0121822357, 660526.0121822357, 660526.0121822357, 660526.0121822357, 662090.3124809265, 662090.3124809265, 662090.3124809265, 664278.3420085907, 666562.2520446777, 666562.2520446777, 666562.2520446777, 668317.4073696136, 668317.4073696136, 668317.4073696136, 671850.891828537, 671850.891828537, 671850.891828537, 671850.891828537, 676527.7535915375, 676527.7535915375, 678753.6525726318, 678753.6525726318, 684535.1531505585, 684535.1531505585, 684535.1531505585, 684535.1531505585, 684535.1531505585, 685771.4948654175, 699908.8571071625, 699908.8571071625, 699908.8571071625, 699908.8571071625, 699908.8571071625, 699908.8571071625, 702575.7534503937, 702575.7534503937, 702575.7534503937, 702575.7534503937, 702575.7534503937, 703995.3401088715, 707930.9086799622, 707930.9086799622, 707930.9086799622, 715731.4095497131, 715731.4095497131, 715731.4095497131, 715731.4095497131, 715731.4095497131, 715731.4095497131, 715731.4095497131, 718073.180437088, 721406.774520874, 723876.0442733765, 723876.0442733765, 723876.0442733765, 723876.0442733765, 723876.0442733765, 725815.8543109894, 725815.8543109894, 725815.8543109894, 725815.8543109894, 725815.8543109894, 729599.7183322906, 729599.7183322906, 729599.7183322906, 729599.7183322906, 729599.7183322906, 731636.8236541748, 732751.4457702637, 732751.4457702637, 732751.4457702637, 732751.4457702637, 732751.4457702637, 733795.0279712677, 734851.5951633453, 735934.9930286407, 738154.871225357, 739437.3466968536, 744451.66182518, 745859.7543239594, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 747291.6469573975, 753277.3694992065, 754607.7432632446, 757179.4240474701, 757179.4240474701, 757179.4240474701, 761308.319568634, 764050.6236553192, 764050.6236553192, 764050.6236553192, 764050.6236553192, 764050.6236553192, 764050.6236553192, 764050.6236553192, 768181.3604831696, 769600.8050441742, 769600.8050441742, 773986.9256019592, 773986.9256019592, 773986.9256019592, 773986.9256019592, 784215.3306007385, 784215.3306007385, 784215.3306007385, 784215.3306007385, 784215.3306007385, 784215.3306007385, 785081.9146633148, 786730.0169467926, 787554.9094676971, 787554.9094676971, 790670.4063415527, 790670.4063415527, 790670.4063415527, 790670.4063415527, 790670.4063415527, 791627.0353794098, 795749.1493225098, 795749.1493225098, 795749.1493225098, 801364.5668029785, 801364.5668029785, 801364.5668029785, 801364.5668029785, 801364.5668029785, 801364.5668029785, 806827.6391029358, 806827.6391029358, 806827.6391029358, 806827.6391029358, 806827.6391029358, 806827.6391029358, 806827.6391029358, 810454.6139240265, 810454.6139240265, 810454.6139240265, 814183.8533878326, 814183.8533878326, 819617.5584793091, 819617.5584793091, 819617.5584793091, 819617.5584793091, 819617.5584793091, 819617.5584793091, 823353.5239696503, 828843.982219696, 828843.982219696, 828843.982219696, 828843.982219696, 830219.7413444519, 833090.6729698181, 836947.0307826996, 836947.0307826996, 843274.2629051208, 843274.2629051208, 843274.2629051208, 843274.2629051208, 844145.0622081757, 846952.5957107544, 849686.5477561951, 849686.5477561951, 849686.5477561951, 849686.5477561951, 849686.5477561951, 849686.5477561951, 851591.9768810272, 851591.9768810272, 851591.9768810272, 859782.0467948914, 859782.0467948914, 859782.0467948914, 859782.0467948914, 859782.0467948914, 859782.0467948914, 861835.0758552551, 862980.197429657, 862980.197429657, 867822.4806785583, 867822.4806785583, 873047.2671985626, 873047.2671985626, 873047.2671985626, 873047.2671985626, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 875652.8968811035, 878293.3514118195, 878293.3514118195, 879644.0668106079, 879644.0668106079, 881024.0225791931, 893175.3923892975, 893175.3923892975, 893175.3923892975, 893175.3923892975, 904834.4373703003, 904834.4373703003, 904834.4373703003, 904834.4373703003, 904834.4373703003, 904834.4373703003, 904834.4373703003, 904834.4373703003, 907330.6424617767, 908314.7900104523, 908314.7900104523, 908314.7900104523, 908314.7900104523, 908314.7900104523, 908314.7900104523, 909254.0085315704, 911087.1522426605, 911087.1522426605, 911087.1522426605, 911087.1522426605, 913936.3088607788, 913936.3088607788, 914923.8936901093, 914923.8936901093, 915869.0705299377, 916813.6012554169, 924482.771396637, 924482.771396637, 924482.771396637, 927723.709821701, 927723.709821701, 927723.709821701, 927723.709821701, 927723.709821701, 927723.709821701, 927723.709821701, 930078.0208110809, 934759.238243103, 934759.238243103, 937188.0574226379, 937188.0574226379, 937188.0574226379, 937188.0574226379, 937188.0574226379, 937188.0574226379, 937188.0574226379, 943339.6100997925, 943339.6100997925, 943339.6100997925, 943339.6100997925, 943339.6100997925, 943339.6100997925, 947456.6268920898, 947456.6268920898, 947456.6268920898, 947456.6268920898, 947456.6268920898, 947456.6268920898, 950169.3031787872, 950169.3031787872, 953062.1664524078, 953062.1664524078, 960590.981721878, 960590.981721878, 962158.6022377014, 962158.6022377014, 962158.6022377014, 962158.6022377014, 963680.6094646454, 963680.6094646454, 965323.7707614899, 966200.5791664124, 966200.5791664124, 971031.5721035004, 971031.5721035004, 971031.5721035004, 971031.5721035004, 971031.5721035004, 972130.918264389, 975174.6599674225, 975174.6599674225, 975174.6599674225, 975174.6599674225, 975174.6599674225, 975174.6599674225, 980633.3620548248, 981859.3986034393, 981859.3986034393, 981859.3986034393, 981859.3986034393, 981859.3986034393, 981859.3986034393, 981859.3986034393, 981859.3986034393, 984028.2747745514, 985197.402715683, 985197.402715683, 985197.402715683, 986427.8416633606, 988723.6249446869, 988723.6249446869, 988723.6249446869, 992449.8314857483, 995040.8127307892, 995040.8127307892, 995040.8127307892, 996486.6440296173, 996486.6440296173, 996486.6440296173, 999162.1441841125, 999162.1441841125, 1003194.8463916779, 1003194.8463916779, 1005970.9439277649, 1005970.9439277649, 1016184.287071228, 1019188.3451938629, 1021396.1336612701, 1021396.1336612701, 1021396.1336612701, 1022284.731388092, 1022284.731388092, 1025597.5661277771, 1025597.5661277771, 1025597.5661277771, 1026607.6943874359, 1026607.6943874359, 1026607.6943874359, 1027553.1151294708, 1030462.554693222, 1030462.554693222, 1030462.554693222, 1030462.554693222, 1030462.554693222, 1030462.554693222, 1032443.4406757355, 1032443.4406757355, 1034397.5045681, 1038774.7657299042, 1038774.7657299042, 1038774.7657299042, 1038774.7657299042, 1043383.0621242523, 1043383.0621242523, 1043383.0621242523, 1043383.0621242523, 1043383.0621242523, 1043383.0621242523, 1045954.6947479248, 1047265.0275230408, 1047265.0275230408, 1047265.0275230408, 1048644.5050239563, 1051282.4351787567, 1051282.4351787567, 1054297.6412773132, 1054297.6412773132, 1054297.6412773132, 1054297.6412773132, 1055741.8251037598, 1057164.7689342499, 1058687.3371601105, 1065432.9488277435, 1065432.9488277435, 1065432.9488277435, 1069793.2362556458, 1075676.5251159668, 1075676.5251159668, 1075676.5251159668, 1075676.5251159668, 1078893.1155204773, 1082806.649684906, 1082806.649684906, 1082806.649684906, 1082806.649684906, 1083644.0093517303, 1084447.7775096893, 1086063.4083747864, 1086063.4083747864, 1086063.4083747864, 1087805.9163093567, 1087805.9163093567, 1088704.3306827545, 1089626.564502716, 1089626.564502716, 1089626.564502716, 1089626.564502716, 1090531.5961837769, 1092311.7485046387, 1094968.983888626, 1094968.983888626, 1096905.1480293274, 1101263.427734375, 1101263.427734375, 1101263.427734375, 1101263.427734375, 1101263.427734375, 1104999.328136444, 1108793.6699390411, 1108793.6699390411, 1108793.6699390411, 1108793.6699390411, 1108793.6699390411, 1108793.6699390411, 1108793.6699390411, 1111436.61236763, 1112824.6307373047, 1112824.6307373047, 1118160.0148677826, 1118160.0148677826, 1118160.0148677826, 1118160.0148677826, 1118160.0148677826, 1118160.0148677826, 1120768.8858509064, 1122216.3817882538, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1134352.1931171417, 1140092.5319194794, 1140092.5319194794, 1140092.5319194794, 1140889.936208725, 1140889.936208725, 1141722.7039337158, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1150125.1800060272, 1154357.7530384064, 1154357.7530384064, 1154357.7530384064, 1157664.624929428, 1157664.624929428, 1157664.624929428, 1157664.624929428, 1158879.4660568237, 1164797.5630760193, 1166101.4664173126, 1166101.4664173126, 1166101.4664173126, 1166101.4664173126, 1173351.1216640472, 1180475.973367691, 1180475.973367691, 1180475.973367691, 1180475.973367691, 1180475.973367691, 1181941.454410553, 1181941.454410553, 1181941.454410553, 1181941.454410553, 1186336.9781970978, 1186336.9781970978, 1186336.9781970978, 1186336.9781970978, 1186336.9781970978, 1193295.9451675415, 1200421.8313694, 1200421.8313694, 1200421.8313694, 1200421.8313694, 1202171.237707138, 1209704.5423984528, 1209704.5423984528, 1209704.5423984528, 1209704.5423984528, 1209704.5423984528, 1212001.2600421906, 1212001.2600421906, 1212001.2600421906, 1212001.2600421906, 1212001.2600421906, 1213134.3109607697, 1213134.3109607697, 1213134.3109607697, 1213134.3109607697, 1214177.9017448425, 1216432.3515892029, 1216432.3515892029, 1217584.24949646, 1218733.533859253, 1222062.0579719543, 1226703.5846710205, 1226703.5846710205, 1230383.2459449768, 1230383.2459449768, 1234238.3658885956, 1234238.3658885956, 1234238.3658885956, 1234238.3658885956, 1235604.9621105194, 1235604.9621105194, 1243333.0688476562, 1243333.0688476562, 1247730.7014465332, 1247730.7014465332, 1247730.7014465332, 1251024.4488716125, 1251024.4488716125, 1251024.4488716125, 1251024.4488716125, 1251024.4488716125, 1260124.4115829468, 1260124.4115829468, 1260124.4115829468, 1260124.4115829468, 1260124.4115829468, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1267083.8351249695, 1268866.866350174, 1269869.0912723541, 1271918.1926250458, 1273084.7568511963, 1273084.7568511963, 1273084.7568511963, 1273084.7568511963, 1284433.6440563202, 1284433.6440563202, 1284433.6440563202, 1289375.381231308, 1289375.381231308, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1290977.2906303406, 1292347.7203845978, 1300169.5783138275, 1300169.5783138275, 1300169.5783138275, 1300169.5783138275, 1311979.054927826, 1311979.054927826, 1311979.054927826, 1311979.054927826, 1311979.054927826, 1318038.0699634552, 1318038.0699634552, 1318038.0699634552, 1318038.0699634552, 1318038.0699634552, 1318038.0699634552, 1318038.0699634552, 1321674.1917133331, 1321674.1917133331, 1321674.1917133331, 1321674.1917133331, 1321674.1917133331, 1321674.1917133331, 1322667.5958633423, 1322667.5958633423, 1322667.5958633423, 1322667.5958633423, 1322667.5958633423, 1324295.553445816, 1333582.5514793396, 1333582.5514793396, 1333582.5514793396, 1333582.5514793396, 1335499.1924762726, 1335499.1924762726, 1335499.1924762726, 1339641.1666870117, 1340809.385061264, 1340809.385061264, 1340809.385061264, 1343992.8500652313, 1343992.8500652313, 1343992.8500652313, 1343992.8500652313, 1343992.8500652313, 1345084.9974155426, 1345084.9974155426, 1345084.9974155426, 1346170.3889369965, 1346170.3889369965, 1349603.7373542786, 1349603.7373542786, 1349603.7373542786, 1357106.3725948334, 1357106.3725948334, 1357106.3725948334, 1357106.3725948334, 1357106.3725948334, 1363531.2979221344, 1373428.970336914, 1373428.970336914, 1373428.970336914, 1373428.970336914, 1373428.970336914, 1373428.970336914, 1373428.970336914, 1379325.3328800201, 1379325.3328800201, 1379325.3328800201, 1379325.3328800201, 1379325.3328800201, 1379325.3328800201, 1379325.3328800201, 1383642.558813095, 1386440.8650398254, 1386440.8650398254, 1388147.5641727448, 1388147.5641727448, 1388147.5641727448, 1388147.5641727448, 1389763.6713981628, 1389763.6713981628, 1392318.4852600098, 1392318.4852600098, 1396304.4166564941, 1396304.4166564941, 1396304.4166564941, 1396304.4166564941, 1396304.4166564941, 1400073.6634731293, 1400073.6634731293, 1402088.8028144836, 1403345.482826233, 1403345.482826233, 1403345.482826233, 1403345.482826233, 1404584.5799446106, 1404584.5799446106, 1412656.7010879517, 1412656.7010879517, 1412656.7010879517, 1412656.7010879517, 1412656.7010879517, 1412656.7010879517, 1412656.7010879517, 1414048.60830307, 1415370.1663017273, 1415370.1663017273, 1415370.1663017273, 1415370.1663017273, 1417812.0386600494, 1417812.0386600494, 1417812.0386600494, 1420402.1198749542, 1420402.1198749542, 1420402.1198749542, 1423157.2360992432, 1423157.2360992432, 1431912.3249053955, 1431912.3249053955, 1431912.3249053955, 1431912.3249053955, 1434912.517786026, 1434912.517786026, 1434912.517786026, 1434912.517786026, 1439042.601108551, 1439042.601108551, 1442056.6022396088, 1442056.6022396088, 1442056.6022396088, 1442056.6022396088, 1446401.4825820923, 1446401.4825820923, 1450048.594236374, 1450048.594236374, 1450048.594236374, 1450048.594236374, 1450048.594236374, 1450897.122144699, 1454022.8762626648, 1454022.8762626648, 1454022.8762626648, 1454022.8762626648, 1455829.2083740234, 1455829.2083740234, 1455829.2083740234, 1455829.2083740234, 1456818.2883262634, 1456818.2883262634, 1456818.2883262634, 1457716.9847488403, 1457716.9847488403, 1458659.6102714539, 1459611.5190982819, 1460657.5589179993, 1460657.5589179993, 1461668.464899063, 1461668.464899063, 1461668.464899063, 1461668.464899063, 1470122.6184368134, 1471386.340379715, 1480237.2643947601, 1480237.2643947601, 1480237.2643947601, 1480237.2643947601, 1480237.2643947601, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1482930.0734996796, 1484270.9341049194, 1489085.5317115784, 1491805.1674365997, 1499214.026927948, 1499214.026927948, 1499214.026927948, 1499214.026927948, 1499214.026927948, 1499214.026927948, 1499214.026927948, 1500819.7379112244, 1500819.7379112244, 1500819.7379112244, 1500819.7379112244, 1500819.7379112244, 1500819.7379112244, 1500819.7379112244, 1503768.4631347656, 1503768.4631347656, 1503768.4631347656, 1503768.4631347656, 1503768.4631347656, 1509695.5511569977, 1509695.5511569977, 1513293.482542038, 1514158.3421230316, 1514158.3421230316, 1516764.671087265, 1517823.035478592, 1518809.3843460083, 1518809.3843460083, 1518809.3843460083, 1518809.3843460083, 1518809.3843460083, 1521563.3902549744, 1523421.552181244, 1529035.2058410645, 1529035.2058410645, 1529035.2058410645, 1529035.2058410645, 1532106.1162948608, 1532106.1162948608, 1532106.1162948608, 1534354.1610240936, 1534354.1610240936, 1534354.1610240936, 1534354.1610240936, 1534354.1610240936, 1535415.6341552734, 1535415.6341552734, 1536504.7047138214, 1539720.7930088043, 1539720.7930088043, 1539720.7930088043, 1539720.7930088043, 1543100.7907390594, 1543100.7907390594, 1543100.7907390594, 1546605.1218509674, 1546605.1218509674, 1546605.1218509674, 1547802.0038604736, 1555638.197183609, 1555638.197183609, 1560544.234752655, 1560544.234752655, 1560544.234752655, 1560544.234752655, 1565318.1688785553, 1565318.1688785553, 1565318.1688785553, 1565318.1688785553, 1567420.598268509, 1567420.598268509, 1571159.0385437012, 1571159.0385437012, 1571159.0385437012, 1571159.0385437012, 1571159.0385437012, 1576893.575668335, 1576893.575668335, 1576893.575668335, 1580097.4516868591, 1580097.4516868591, 1580097.4516868591, 1580097.4516868591, 1580097.4516868591, 1583217.2691822052, 1583217.2691822052, 1584342.805147171, 1586416.3701534271, 1587560.420513153, 1587560.420513153, 1588766.2062644958, 1588766.2062644958, 1589936.2106323242, 1592413.677930832, 1592413.677930832, 1594798.8080978394, 1594798.8080978394, 1596032.2806835175, 1601769.760131836, 1601769.760131836, 1601769.760131836, 1605202.0862102509, 1605202.0862102509, 1606507.973909378, 1606507.973909378, 1607843.3456420898, 1607843.3456420898, 1609191.7531490326, 1610573.6503601074, 1610573.6503601074, 1610573.6503601074, 1612087.7265930176, 1612087.7265930176, 1614326.03597641, 1614326.03597641, 1616670.8209514618, 1618397.8776931763, 1618397.8776931763, 1618397.8776931763, 1620484.478712082, 1620484.478712082, 1622404.8643112183, 1622404.8643112183, 1622404.8643112183, 1622404.8643112183, 1622404.8643112183, 1622404.8643112183, 1624311.91945076, 1625377.0396709442, 1625377.0396709442, 1628543.4744358063, 1628543.4744358063, 1628543.4744358063, 1628543.4744358063, 1628543.4744358063, 1628543.4744358063, 1628543.4744358063, 1630613.4884357452, 1630613.4884357452, 1637616.0740852356, 1637616.0740852356, 1639624.9542236328, 1641784.7383022308, 1648167.0122146606, 1648167.0122146606, 1648167.0122146606, 1648167.0122146606, 1652947.5409984589, 1658185.7163906097, 1658185.7163906097, 1662288.7890338898, 1662288.7890338898, 1662288.7890338898, 1662288.7890338898, 1662288.7890338898, 1662288.7890338898, 1662288.7890338898, 1663676.2297153473, 1663676.2297153473, 1665158.2460403442, 1668186.3696575165, 1668186.3696575165, 1670527.6794433594, 1670527.6794433594, 1670527.6794433594, 1676874.8471736908, 1676874.8471736908, 1676874.8471736908, 1676874.8471736908, 1676874.8471736908, 1676874.8471736908, 1676874.8471736908, 1681280.2317142487, 1681280.2317142487, 1681280.2317142487, 1681280.2317142487, 1681280.2317142487, 1682375.7643699646, 1684480.2267551422, 1684480.2267551422, 1685677.0758628845, 1685677.0758628845, 1687934.87906456, 1689073.329925537, 1689073.329925537, 1692555.5703639984, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1698900.9363651276, 1701325.5577087402, 1702664.5867824554, 1705287.367105484, 1705287.367105484, 1706651.1602401733, 1709440.871477127, 1709440.871477127, 1709440.871477127, 1709440.871477127, 1710739.9349212646, 1712147.1745967865, 1718337.3656272888, 1719816.8075084686, 1724191.9984817505, 1731355.0357818604, 1731355.0357818604, 1731355.0357818604, 1731355.0357818604, 1731355.0357818604, 1731355.0357818604, 1736269.4442272186, 1736269.4442272186, 1736269.4442272186, 1736269.4442272186, 1736269.4442272186, 1738665.5418872833, 1738665.5418872833, 1740276.0908603668, 1740276.0908603668, 1745080.657957077, 1745080.657957077, 1745080.657957077, 1745080.657957077, 1745080.657957077], "prediction_length": 1608, "reference": "Hallo, ich bin Jiawei Zhou von der Harvard University. Ich freue mich sehr, unsere Arbeit zum semantischenOnline-Parsing f\u00fcr die Latenzreduktion bei einem aufgabenorientierten Dialog vorstellen zu k\u00f6nnen. Dies ist eine gemeinsame Arbeit mit Jason, Michael, Anthony und Sam von Microsoft Semantic Machines. Beim aufgabenorientierten Dialog interagiert ein Benutzer mit dem System, das Anfragen von Benutzer\u00e4u\u00dferungen, in der Regel in gesprochener Form, bearbeitet. Vom Ende der Benutzer\u00e4u\u00dferung bis zur Antwort des Systems gibt es oft eine sp\u00fcrbare Verz\u00f6gerung. Im Detail wird die Benutzer\u00e4u\u00dferung in ein ausf\u00fchrbares Programm \u00fcbersetzt. Dieses wird dann ausgef\u00fchrt, damit das System richtig reagieren kann. Denn das Programm wird als semantischer Graph dargestellt, der die Berechnung skizziert, wobei ein Knoten einen Funktionsaufruf darstellt und seine Kindknoten die Argumente sind. Die gro\u00dfen Knoten markieren sofortige Operationen, aber die anderen sind langsam in der Ausf\u00fchrung. Das einfache Beispiel hier zeigt, dass diese Programme oft kompliziertere Graphen sein k\u00f6nnen als die Baumstrukturen. In diesem Vortrag stellen wir die Frage, ob wir mit der Generierung des Programms und seiner Ausf\u00fchrung beginnen k\u00f6nnen, bevor der Benutzer \u00fcberhaupt die \u00c4u\u00dferung beendet hat, sodass das System schneller reagieren kann. Dies ist das Problem der Online-Vorhersage und Entscheidung. Es gibt viele andere in diesem REALM. Beispiele sind Simultan\u00fcbersetzungen, bei denen ein Dolmetscher live eine Sprache in eine andere in Echtzeit \u00fcbersetzt; die intelligente automatische Vervollst\u00e4ndigung von Text, um die Absicht des Benutzers zu erraten; und der Uber Pool, wo Fahrer dorthin geschickt werden, wo sie basierend auf der prognostizierten Nachfrage m\u00f6glicherweise ben\u00f6tigt werden. All diese Szenarien haben eines gemeinsam. Es ist vorteilhaft, Entscheidungen zu treffen, bevor man alle Eingaben sieht. In unserem Fall werden wir uns mit dem semantischen Online-Parsing befassen, was eine Herausforderung sein k\u00f6nnte, da wir erraten m\u00fcssen, was der Benutzer sagen k\u00f6nnte. Dieses Gebiet ist auch wenig erforscht und hat keine formale Evaluationsmetrik. Schauen wir uns zun\u00e4chst an, wie ein gew\u00f6hnliches System funktioniert. Dieses funktioniert offline durch Parsing zum Programm erst am Ende der Benutzer\u00e4u\u00dferung. Hier wird das Zeichen Graph vorhergesagt, nachdem alle Information gesehen wurden. Im Gegensatz dazu schlagen wir ein Online-System vor, das bei jedem \u00c4u\u00dferung das Pr\u00e4fix vergleicht. Jedes Mal, wenn wir beispielsweise ein neues Token sehen, prognostizieren wir einen neuen Graphen. Beachten Sie, dass Fehler auftreten k\u00f6nnen. Bei der Position von der Poolparty mit Barack Obama haben wir einen Graphen mit den richtigen Knoten \u00fcber die Person und das Thema des Ereignisses, aber wahrscheinlich die falschen Informationen zu den Zeiten. Dieser Prozess geht weiter, bis wir die vollst\u00e4ndige Benutzer\u00e4u\u00dferung erhalten. Wie w\u00fcrde sich dies auf die Ausf\u00fchrungszeiten im Offline-System auswirken? Am Ende erhalten wir den Programm-Graphen, damit das System an dieser Stelle mit der Ausf\u00fchrung beginnen kann. Vergessen Sie nicht, dass die gro\u00dfen Knoten schnelle Operationen sind. Daher betrachten wir nur die Ausf\u00fchrungszeiten der farbigen langsamen Funktionen. Erstens k\u00f6nnen diese beiden Personensuchfunktionen parallel ausgef\u00fchrt werden, da sie keine Abh\u00e4ngigkeit von anderen Funktionen haben. Sie sind wei\u00df hinterlegt im rosa K\u00e4stchen. Als N\u00e4chstes kann der Knoten \u201eEreignis erstellen\u201c ausgef\u00fchrt werden, nachdem er Ergebnisse von Knoten bekommen hat. Mit der Top-Ertragsfunktion wird das gesamte Programm beendet. Der Ausf\u00fchrungsprozess ist streng und beschr\u00e4nkt sich auf die Abh\u00e4ngigkeitsstruktur des Programms, bei der einige Operationen nicht parallelisiert werden k\u00f6nnen. Das f\u00fchrt zu einer sp\u00fcrbaren Verz\u00f6gerung. In unserem Online-System, wo wir im Laufe der Zeit vorhersagen, kann die Programmausf\u00fchrung fr\u00fcher beginnen. Hier, beim Pr\u00e4fix nach Obama, sagen wir zuversichtlich voraus, dass die Funktion \u201ePerson finden\u201c im Programm sein muss, aber dass der Rest Fehler enthalten kann, da sie ausgegraut sind. Die Ausf\u00fchrung des Knotens kann sofort als Schritt gestartet werden. Dann, mit mehr Token, prognostizieren wir einen v\u00f6llig neuen Graphen, aber ein Teil davon wird bereits ausgef\u00fchrt. Wir m\u00fcssen also nur den Rest der Knoten betrachten, bei denen wir uns auch sicher sind. Hier kann wieder \u201eFinde Person\u201c parallel ausgef\u00fchrt werden. Auch hier k\u00f6nnten wir falsche Vorhersagen haben. Mit mehr Text steigt die Chance, dass wir richtig liegen. Zum Beispiel bei der Zeit des Ereignisses, bei der AM auch richtig vorhergesehen wird. Dann k\u00f6nnen wir mit der Ausf\u00fchrung des Rests beginnen, indem wir der Abh\u00e4ngigkeitsstruktur des Programms folgen. Indem wir die Ausf\u00fchrungszeiten mit den Zeiten der \u00c4u\u00dferungen \u00fcberlappen, sparen wir viel Zeit ein. Also haben wir die Aufgabe mit dem semantischen Online-Parsing vorgeschlagen. Eine zugrunde liegende Annahme ist, dass die Ausf\u00fchrungszeit die Vorhersagezeit des Modells dominiert. Also konnten wir nur Zeit gewinnen, indem wir fr\u00fcher voraussagten. Eine weitere Annahme ist, dass, wenn die Vorhersage und die Ausf\u00fchrung im Hintergrund stattfinden, sie f\u00fcr Benutzer nicht sichtbar sind. Es ist nicht notwendig, eine konsistente Parsing-Historie aufzubewahren. Also parsen wir nach jedem Token von Grund auf neu. Insbesondere wollen wir einen zweistufigen Ansatz vorschlagen. Wir schlagen einen Schritt vor, bei dem ein Graph mit vollst\u00e4ndiger Struktur vorhergesagt wird und einen Schritt, bei dem die Knoten ausgew\u00e4hlt werden, die es zu diesem Zeitpunkt wert sind, ausgef\u00fchrt zu werden. Wir hatten zwei Varianten der vorgeschlagenen Methode. Der erste Ansatz kombiniert eine Sprachmodell-Vervollst\u00e4ndigung mit einer vollst\u00e4ndigen \u00c4u\u00dferung zum Graph-Parsing. Insbesondere wird das Pr\u00e4fix nach Obama zun\u00e4chst durch ein fein abgestimmtes BART-Sprachmodell vervollst\u00e4ndigt und dann in ein Programm mit vollst\u00e4ndigem Offline-Parser \u00fcbersetzt. Der zweite Ansatz sagt das Programm direkt aus den Pr\u00e4fixen der Benutzer\u00e4u\u00dferung voraus. Dies wird durch Training von einem einzigen Online-Parser erreicht, der aus jedem Pr\u00e4fix in den Zielgraphen \u00fcbersetzen soll. Dies erleichtert dem Modell, die richtige Erwartung zu erlernen. Detaillierter gesagt: Wie k\u00f6nnen wir diese Graphen generieren? Wir formulieren das Problem, indem wir eine serielle Version des Graphen generieren. Jeder Knoten oder jede Kante wird durch eine Aktion dargestellt. Hier beginnen wir mit dem ersten Knoten. Die Reihe unten zeichnet den absoluten Index im Aktionsverlauf auf. Dann haben wir den zweiten Knoten. Als N\u00e4chstes kommt die Kante zwischen ihnen. Dort ist der Zeiger auf den Index des fr\u00fcheren Knotens und das Kantenlabel enthalten. Null bedeutet hier, dass der neueste Knoten mit dem Knoten verbunden wird, der durch die Null-Aktion und die Kante des n\u00e4chsten Knotens generiert wurde. Dieser Prozess geht weiter, bis wir den vollst\u00e4ndigen Graph generieren. Das zugrunde liegende Modell basiert auf dem Transformator mit Selbstausrichtungsmechanismus, \u00e4hnlich einem fr\u00fcheren \u00fcbergangsbasierten Parser. Nach Generierung eines vollst\u00e4ndigen Graphen haben wir die Wahrscheinlichkeiten der Aktionsebene erhalten, die verschiedenen Teilen des Graphen entsprechen. Wir w\u00e4hlen Konfidenzteilgraphen auf der Grundlage der auszuf\u00fchrenden Schwellwerte heuristisch aus. Sp\u00e4ter werden wir den Schwellenwert variieren, um unterschiedliche Kompromisse zwischen der Latenzreduzierung und den Ausf\u00fchrungskosten zu erzielen. F\u00fcr die formale Evaluation der Online-Methoden wollen wir eine endg\u00fcltige Latenzreduzierung oder FLR-Metrik vorschlagen. Hier ist eine Zusammenfassung, wie ein Offline-System die Ausf\u00fchrungszeiten beendet. In Online-Systemen \u00fcberschneidet sich die Ausf\u00fchrung mit den Zeiten der \u00c4u\u00dferung. Sie endet also fr\u00fcher. FLR ist als die Reduktionszeit im Vergleich zum Offline-System definiert und durch das Ende der Ausf\u00fchrung markiert. Wir f\u00fchren Experimente an zwei gro\u00dfen Datens\u00e4tzen von Konversationen mit semantischem Parsing durch: SMCalFlow und TreeDST. Unser auf dem Graphen basierte Parser erreicht, wenn er offline betrieben wird, beste Leistungen beim Parsing f\u00fcr beide Datens\u00e4tze. Das LM-Complete-Modell erzielt auch eine nicht triviale BLEU -Verst\u00e4rkung im Vergleich zur einfachen Basislinie der Knotenvervollst\u00e4ndigung. Schauen wir uns nun die Vorhersagegenauigkeit unseres Pr\u00e4fixes f\u00fcr den Graph-Parser an. Wir testen den F1-Score der Graph-Tupel zwischen der Generierung und dem Go-Graph in den Validierungsdaten auf der y-Achse f\u00fcr jede Pr\u00e4fixl\u00e4nge in der x-Achse, dargestellt durch Prozents\u00e4tze. Jede dieser Kurven stellt ein anderes Modell mit dem einzigen Unterschied in den Trainingsdaten dar. Die untere Kurve ist der Offline-Parser. Wir mischen Pr\u00e4fix- Daten in verschiedenen L\u00e4ngen hinein, um das Modell in einen Online-Parser zu \u00fcberf\u00fchren. Zum Beispiel bedeutet das Legendenpr\u00e4fix \u201e80 Prozent plus\u201c, dass das Modell mit Pr\u00e4fix-Daten trainiert wird, wobei die Pr\u00e4fixl\u00e4nge gr\u00f6\u00dfer als 80 Prozent der vollen \u00c4u\u00dferungsl\u00e4nge ist. Die obere linke Ecke ist der gew\u00fcnschte Bereich. Wie wir sehen k\u00f6nnen, funktioniert der Offline-Parser in der schwarzen Kurve der Pr\u00e4fix-Daten nicht gut. Da wir im Training mehr Pr\u00e4fixe mischen, hebt sich die Kurve nach oben und links und schneidet bei allen Pr\u00e4fixl\u00e4ngen besser ab. Die volle Leistung des \u00c4u\u00dferung-Parsings wird jedoch im oberen rechten Punkt nicht beeinflusst. Basierend auf diesen starken Ergebnissen, stellt sich die Frage, wie viel Latenz reduziert wird. Wir messen die Zeit anhand der Reihe von Quelltoken und simulieren verschiedene Funktionsausf\u00fchrungszeiten. Die Kurven zeigen den Kompromiss zwischen der FLR-Metrik und den Ausf\u00fchrungskosten, gemessen an der Reihe \u00fcberm\u00e4\u00dfiger Funktionskosten, die nicht korrekt sind. Dies wird durch Variation der Teilgraphenauswahlschwelle erreicht. Eine h\u00f6here Schwelle w\u00e4hlt weniger Fehlfunktionen aus, erh\u00e4lt aber eine kleinere FLR, w\u00e4hrend die niedrigere Schwelle aggressiver Programme ausw\u00e4hlt und ausf\u00fchrt. Wir vergleichen die beiden Ans\u00e4tze, die wir vorschlagen, mit einer Baseline, die nichts anderes tut, als den Offline-Parser f\u00fcr den Online-Einsatz anzuwenden. Der obere linke Bereich hat den besten FLR und Kostenvorteil. Wir sehen, dass beide unserer Methoden die Baseline um eine gro\u00dfe Differenz \u00fcbertreffen und sie bei TreeDST \u00e4hnlicher abschneiden. W\u00e4hrend die Ausf\u00fchrung einzelner Funktionen schneller ist, gibt es tendenziell mehr Ausf\u00fchrungsvorg\u00e4nge und weniger Raum f\u00fcr die Latenzreduzierung. Wenn die Ausf\u00fchrung einzelner Funktionen langsamer ist, gibt es mehr M\u00f6glichkeiten f\u00fcr FLR-Verbesserungen. Unsere beiden Ans\u00e4tze erreichen eine bessere Leistung in verschiedenen Kostenbereichen. Insgesamt erreichen wir je nach Ausf\u00fchrungszeit und zul\u00e4ssigen Kosten eine Reduzierung der relativen Latenz um 63 bis 60 Prozent. Schlie\u00dflich haben wir eine Aufschl\u00fcsselung der durchschnittlichen Latenzreduktion in Token f\u00fcr jeden Typ des Funktionsknotens, wenn die zul\u00e4ssigen Kosten drei Ausf\u00fchrungsvorg\u00e4nge betragen. Wie wir sehen k\u00f6nnen, gibt es in jedem Bereich positive Ergebnisse. Es gibt auch einige Funktionen, bei denen wir eine beeindruckende Latenzreduzierung erzielen und der rote Balken viel l\u00e4nger ist, wie z.\u00a0B. Find Manager und Empf\u00e4nger. Dies sind Low-Level-Funktionen, die keine gro\u00dfe Abh\u00e4ngigkeit von anderen haben. Abschlie\u00dfend haben wir das semantische Online-Parsing als neue Aufgabe vorgeschlagen, das wir mit der strengen Latenzreduktionsmetrik untersuchen. Mit einem starken graph-basierten, semantischen Parser erreichen wir eine relativ gute Latenzreduktion, entweder durch unseren Pipeline-Ansatz mit LM-Abschluss und einem vollst\u00e4ndigen Parser, oder direkt durch einen gelernten Parser mit Fokus auf die Pr\u00e4fixe. Dar\u00fcber hinaus kann unser Ansatz ein allgemeiner Rahmen sein und auf andere ausf\u00fchrbare semantische Darstellungen in verschiedenen Dom\u00e4nen angewendet werden. Zuk\u00fcnftige Arbeiten k\u00f6nnten eine intelligentere Vorhersage und die Methode der Ausf\u00fchrungsintegration erforschen. Danke f\u00fcrs Zuh\u00f6ren.", "source": ["2/acl_6060/dev/full_wavs/2022.acl-long.110.wav", "samplerate: 16000 Hz", "channels: 1", "duration: 1e+01:43.007 min", "format: WAV (Microsoft) [WAV]", "subtype: Signed 16 bit PCM [PCM_16]"], "source_length": 703007.375}
{"index": 1, "prediction": "Hallo, ich werde unsere Arbeit \u00fcber die Erzeugung von Retrieval-Augmented counterfactuals f\u00fcr Fragen-Antworten Aufgaben diskutieren. Diese Arbeit wurde w\u00e4hrend meiner Praktikum bei Google Research durchgef\u00fchrt, wo ich von Matthew Lamb und Ian Denney gehorcht wurde. Um die Aufgabe zu motivieren, lasse ich anfangen indem ich ein counterfactual definieren kann. In dieser Arbeit definieren wir eine counterfactual als eine St\u00f6rung des Eingangstexts, die sich in einigen sinnvollen kontrollierten Wegen von der Ausgabe unterscheidet. Originaltext, und erm\u00f6glicht es uns, \u00fcber die \u00c4nderungen des Ergebnisses oder der T\u00e4tigkeitszeichen zu reden. Zum Beispiel \u00e4ndern Sie die W\u00f6rter faszinierend zu faszinierend oder erwarten, um den Sinn zu nennen, \u00e4ndert das Gef\u00fchl f\u00fcr diese Filmbewertung. Ebenso \u00e4ndert die Einnahme der Qualifizierenden Frauen an die Frage die Antwort auf die Frage im Beispiel unten. Menschen sind typisch robust gegen solche St\u00f6rungen im Vergleich zu NLP-Modellen. auf die Aufgabe trainiert. Warum ist das so? Die Datens\u00e4tze k\u00f6nnen mit systematischen Biasen sammeln, die zu einer einfachen Beschlussgrenze f\u00fchren. Das wird durch das counterfactual verletzt. Wie in diesem 2D-Klassifizierungsproblem gezeigt wird. Fr\u00fchere Arbeiten haben festgestellt, dass das Hinzuf\u00fcgen von counterfactual Beispielen zu den Trainingsdaten das Modell bei solchen St\u00f6rungen robust machen kann. So, wenn counterfactuals wertvoll sind, wie k\u00f6nnen wir sie generieren? Diese Aufgabe ist besonders schwierig f\u00fcr die NLP, weil hier drei Beispiele aus drei verschiedenen NLP Aufgaben sind. Wie Sie sehen k\u00f6nnen, Beispiele, die die Beschlussgrenze zwischen den Ergebnissen verletzen m\u00fcssen sehr sorgf\u00e4ltig durch Verzerrung bestimmter Attribute des Textes erstellt werden, die hier unterstreicht werden. Dies kann durch menschliche Ank\u00fcndigung durchgef\u00fchrt werden, aber dies ist teuer und verr\u00fcckt. Einige fr\u00fchere Arbeiten konzentrieren sich auf die Verwendung von Syntaxb\u00e4umen oder die semantische Regel-Labelung, aber die Reihe von St\u00f6rungen die durch diese Techniken generiert werden, ist durch das semantische Rahmen begrenzt. Neuere Arbeiten haben Massensprachmodelle verwendet, um in massen Teile des Textes zu f\u00fcllen, um Label zu \u00e4ndern, aber zu finden, welche Teile des Textes zu st\u00f6ren, kann herausfordernd sein. Danke f\u00fcr das H\u00f6ren. Es gibt mehr Herausforderungen, um Gegenf\u00e4lle f\u00fcr die Frage, die spezifisch beantwortet wird, zu generieren. Diese Aufgabe erfordert Hintergrundkenntnisse. Zum Beispiel, um die urspr\u00fcngliche Frage zu st\u00f6ren, ist Indiana Jones Temple of Doom ein Vorbild, wir m\u00fcssen sich der anderen Filme in der Franchise bewusst sein. Um eine solche Frage zu bekommen, ist Indiana Jones Raiders of the Lost Ark ein Vorg\u00e4nger? Dies kann zu Fragen f\u00fchren, die nicht mit den verf\u00fcgbaren Beweisen beantwortet werden oder falsche Voraussetzungen haben. Dar\u00fcber hinaus k\u00f6nnen einige Frage St\u00f6rungen zu erheblichen semantischen Drift aus dem urspr\u00fcnglichen Eingang f\u00fchren. Zum Beispiel, diese Frage, ist Indiana Jones im Tempel of Doom Kinder Sklaverei praktizieren? Wir bieten eine sehr einfache, aber wirksame Technik namens Retrieve-Generate-Filter oder RGF zur Bew\u00e4ltigung von counterfactual St\u00f6rungen der Fragen und zielt auch darauf ab, alle anderen oben genannten Herausforderungen zu l\u00f6sen. Die grundlegende Intuition hinter RGF ist, dass die notwendige Hintergrundinformationen, die erforderlich sind, um St\u00f6rungen zu generieren, in den nahen Fehlern, die durch ein Fragestellungsmodell gemacht werden, vorhanden sein k\u00f6nnen. Zum Beispiel, die state-of-the-art Realm produziert die folgenden top-k Antworten auf die Frage, wer ist der Kapit\u00e4n des Richmond Schiffes? Der Fu\u00dfballclub? Nun, es erholt den urspr\u00fcnglichen Referenzpass und antwortet Trent Cotkin als die beste Wahl. Er erh\u00e4lt auch zus\u00e4tzliche Passagen und Antworten, die zur F\u00fchrung der Frage Motivation verwendet werden k\u00f6nnen. Zum Beispiel erholt es zwei weitere Antworten, die den Kapit\u00e4nen der Reserve-Team und das weibliche Team des gleichen Clubs entsprechen, und dies kann zu interessanten Ver\u00f6ffentlichungen f\u00fchren. TopK erh\u00e4lt zuerst die relevanten Antworten und Kontexte von TopK, die nicht den Referenzantrag im Kontext entsprechen. Nach diesem Schritt, die Frage Generation Modellbedingungen auf diesen alternativen Antworten zu generieren eine Frage, die ihnen entspricht. Und schlie\u00dflich k\u00f6nnen wir die generierten Fragen auf der Grundlage der Minimalit\u00e4t oder auf der Grundlage der Art der semantischen St\u00f6rung filtern, die wir einf\u00fchren m\u00f6chten. in gr\u00f6\u00dferen Details. Zur Wiederherstellung verwenden wir eine retrieve-then-read-Modell wie Realm, die die urspr\u00fcngliche Frage und einen gro\u00dfen Corpus wie Wikipedia als Eingang nimmt. Es besteht aus zwei Modulen. Der Retriever-Modul f\u00fchrt die \u00c4hnlichkeitsuche \u00fcber einen tiefen Index von Passagen durch, um die wichtigsten Passagen der Frage zu erlangen, und ein Leser-Modul extrahiert dann eine Spanne aus jedem Passage als potenzielle Antwort. Retriebe den goldenen Pass und antwortet in den meisten F\u00e4llen. In dieser Arbeit interessieren wir uns jedoch mehr f\u00fcr die Antworten und den Kontext, den sie weiter unter die Linie zieht. Im n\u00e4chsten Schritt, die Frage der Generation, verwenden wir diese alternativen Antworten und Kontext, um neue Fragen zu generieren, die diesen Alternativen entsprechen. Question Generation Modell ist ein vorbereitetes Text-to-Text-Transformer, das gut auf den NQ-Daten angepasst ist, um eine Frage f\u00fcr eine Antwort zu generieren, die im Kontext markiert wird. In der Folge stellen wir die Frage der Generation Modell die alternative Antwort im Kontext, die wir im vorherigen Schritt erhalten haben. Zum Beispiel f\u00fcr die Frage, wer der Kapit\u00e4n des Richmond Fu\u00dfballclubs ist, erh\u00e4lt Realm Abschnitte \u00fcber die weibliche Mannschaft des Clubs, Kapit\u00e4n von Jess Kennedy, und die Frage Generation Modell generiert die Frage, wer Kapit\u00e4n des Richmond Fu\u00dfballclubs erster Das Team? Was hat eine spezifische semantische St\u00f6rung? Auf \u00e4hnliche Weise erhalten wir auch Fragen wie wer das VFL-Reserve-Team von Richmond kapteniert hat oder wer Graham in der Grand-Finale im vergangenen Jahr verweigert hat? Schlie\u00dflich filtern wir ein Unterset der generierten Anfragen auf der Grundlage gew\u00fcnschter Merkmale. Wie zuvor motiviert, m\u00f6chten wir sicherstellen, dass die neue Frage immer noch semantisch nahe dem Original ist. F\u00fcr Filtertechniken erfordert es keine zus\u00e4tzliche Aufsicht. Wir behalten einfach neue Fragen, die eine kleine Distanz von der urspr\u00fcnglichen Frage auf Token-Level haben. Zum Beispiel entfernen wir die Frage, die Graham in der Grand-Finale im vergangenen Jahr verweigert hat, weil sie eine l\u00e4ngere Editingabstand von der urspr\u00fcnglichen Frage hat. In unseren Experimenten zeigen wir, dass diese einfache Heuristik verwendet werden kann, um NQ-Training-Daten zu erh\u00f6hen. Wir experimentieren auch mit einer Filterstrategie, die auf der Art der semantischen St\u00f6rung beruht. Zu diesem Zweck verwenden wir eine allgemeine Zweckfrage namens QED. QED identifiziert zwei Teile der Frage, eine Predigt und eine Referenz. Referenzen sind keine Ausdr\u00fccke in der Frage, die den Einheiten im Kontext entsprechen. Eine Predigt ist grunds\u00e4tzlich der Rest der Frage. Zum Beispiel k\u00f6nnen wir die Frage, die Richmonds erstes damals weibliches Team in zwei Referenzen aufgeteilt hat, Richmond Fu\u00dfballclub, weibliches Team und die Predikate, die X aufgeteilt hat. Ein Modell, das auf der Referenz vorhersehbare Ank\u00fcndigungen f\u00fcr NQ ausgebildet ist, gibt uns diese Frage. Entfernen Sie sowohl die urspr\u00fcngliche als auch die generierte Frage basierend auf Q und E erm\u00f6glicht es uns, unsere generierten counterfactuals f\u00fcr die Bewertung zu kategorisieren. Insbesondere erhalten wir zwei Gruppen von Fragen, diejenigen, die eine Referenzwechsel unterliegen, w\u00e4hrend die Predigt beibehalten werden, und diejenigen, die eine Predigtwechsel unterliegen und optionell Referenzen hinzuf\u00fcgen. Zum Beispiel, wer Kapit\u00e4n Richmond's VFL Reserve Team ist eine Referenzwechsel. Wer die Nummer 9 tr\u00e4gt, ist f\u00fcr den Club eine vorhersehbare \u00c4nderung. Wir bewerten jetzt die Wirksamkeit von RGF St\u00f6rungen, wenn sie auf Trainingsdaten erh\u00f6ht werden. Um die Wirksamkeit der Kontraschallvergr\u00f6\u00dferung insbesondere effektiv zu bewerten, experimentieren wir mit zwei starken Datenvergr\u00f6\u00dferungsbasellen. Die erste Grundlinie, genannt zuf\u00e4llige Antworten und Frage-Generation, f\u00fcgt Daten hinzu, die keine Beziehung zu der urspr\u00fcnglichen Frage haben. Das hei\u00dft, Passagen und Antworten sind einfach zuf\u00e4llig aus Wikipedia sammeln. Diese Baseline f\u00fcgt grunds\u00e4tzlich mehr Daten hinzu, die wie Enqueue aussehen. Mit der zweiten Baseline, der goldene Antwort und der Frage-Generation, aktualisieren wir speziell den Erholungsteil unserer Methode. Hier werden alternative Antworten nur aus dem gleichen Abschnitt ausgew\u00e4hlt, der die goldene Antwort enthielt. Wie die Baselinen und die RGF-Erh\u00f6hung auf Lesenverst\u00e4ndnis durchgef\u00fchrt werden, wo das Modell Zugang zu Frage und Kontext hat? Wir experimentieren mit sechs Out-of-Domain-Data-Sets und pr\u00e4sentieren hier Ergebnisse, wo die Daten sind Training-Data verdoppelt in Erh\u00f6hung. Wir finden, dass beide Datenvergr\u00f6\u00dferungsbasellen nicht in der Lage sind, die Out-of-Domain-Generalisierung zu verbessern, in der Tat, ein Ensemble von sechs Modellen, die auf der urspr\u00fcnglichen Daten trainiert, scheint die Wettbewerbsf\u00e4higste Baseline zu sein. Im Vergleich zu dieser Grundlinie finden wir, dass RGF counterfactuals die Out-of-Domain-Performance verbessern k\u00f6nnen und gleichzeitig die In-Domain-Performance aufrechterhalten. Dies weist darauf hin, dass die Erf\u00fcllung in den Vernunftsl\u00fccken des Modells durch counterfactual augmentation effektiver ist als mehr Daten aus der Trainingsverteilung hinzuzuf\u00fcgen. Dar\u00fcber hinaus finden wir, dass die Verwendung von retrieval-to-sample alternativen Ergebnissen oder Antworten wichtig f\u00fcr eine wirksame CDA ist. Wir experimentieren auch mit Open-Domain QA wo das Modell nur die Frage sieht, und wiederum bewerten wir auf vier Out-of-Domain-Data-Sets. Wir finden dass Baseline-Modelle nicht so effektiv f\u00fcr Out-of-Domain-Generalisierung sind. Die Datenerh\u00f6hung mit RGF zeigt jedoch erhebliche Verbesserungen. Wir verbessern sogar in der In-Domain NQ Datensatz. Wir gehen davon aus, dass die counterfactual data augmentation das model in besseren query-codings f\u00fcr sehr \u00e4hnliche queries hilft. Schlie\u00dflich bewerten wir auch die F\u00e4higkeit des Modells, die Konsistenz in der lokalen Nachbarschaft der urspr\u00fcnglichen Frage zu verbessern. Konsistenz beurteilt den Proportion von Fragen, die durch das Modell korrekt beantragt werden, in dem sowohl die urspr\u00fcngliche als auch die counterfactualfrage korrekt beantragt werden. Dies hilft uns ausdr\u00fccklich, die Robustheit des Modells bis zu kleinen St\u00f6rungen in der Nachbarschaft des urspr\u00fcnglichen Eingangs zu messen. Wir experimentieren mit f\u00fcnf Datens\u00e4tzen, die Paare von Fragen enthalten, die \u00e4hnlich sind. Semantisch nahe gegenseitig. Abgesehen von den drei Datens\u00e4tzen, AQA, AmbQA und CoreFContrastSet, die bereits verf\u00fcgbar sind, bewerten wir auch auf RGF counterfactuals, die mit urspr\u00fcnglichen Enqueue Fragen abgestimmt werden, basierend auf der Frage, ob sie eine voraussichtliche \u00c4nderung oder eine Referenz\u00e4nderung erlebt haben. Diese Untersetze wurden in-house angegeben, um L\u00e4rm zu beseitigen und werden als Ressource bereitgestellt. Alle Baselinen sind nicht in der Lage, die Konsistenz mit dem Ensemble-Modell signifikant zu verbessern und die Konsistenz durch kleine Margen zu verbessern. Allerdings hat die RGF counterfactual augmentation beeindruckende Gewinne in Konsistenz, sowohl auf vorherigen Datens\u00e4tzen als auch auf den beiden Untersetzen, die wir f\u00fcr die Referenz und Prognose St\u00f6rungen behandelt haben. Bitte beachten Sie, dass die erweiterten RGF-Daten nicht durch St\u00f6rungen verwechselt werden. Nur die Bewertungss\u00e4tze sind vorhanden. Tats\u00e4chlich zeigt eine qualitative Inspektion der Arten von Kontra-Faktualen, dass die generierten Fragen mehrere verschiedene St\u00f6rungen enthalten. Zum Beispiel wird diese urspr\u00fcngliche Frage \u00fcber die Bev\u00f6lkerung von Walnut Grove, Minnesota in verschiedenen Dimensionen wie Stadt, Staat, Land und in verschiedenen Prognosen wie Standort, Armut, Anzahl der Schulen gest\u00f6rt. RGF-St\u00f6rungen sind Kontextspezifisch. Zum Beispiel f\u00fcr diese andere Frage \u00fcber den Wimbledon-Singles-Turnier, die St\u00f6rung ist entlang der Art von Spiel, Art des Turniers, oder das Spielergebnis. Die letzten Takeaways. Wir bew\u00e4ltigen die Aufgabe der Kontra-faktuellen Datenvergr\u00f6\u00dferung und St\u00f6rung f\u00fcr Informationen, die nach Anfragen suchen, und bew\u00e4ltigen ihre einzigartigen Herausforderungen durch eine Umkehrung des Generationsansatzes. \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende \u00dcberschreitende Wir finden, dass diese Technik keine zus\u00e4tzliche \u00dcberwachung erfordert und die Beispiele f\u00fcr die Erh\u00f6hung gekennzeichnet sind. Die Erh\u00f6hung verbessert die Out-of-Domain-Generalisierung und die Nachbarkonsistenz. Und wir finden, dass RGF counterfactual semantisch vielf\u00e4ltig sind ohne Bias w\u00e4hrend der Erh\u00f6hung einf\u00fchren. Danke f\u00fcr Sie.", "delays": [2500.0, 2500.0, 2500.0, 4500.0, 4500.0, 4500.0, 4500.0, 5500.0, 5500.0, 5500.0, 6500.0, 9000.0, 9000.0, 9000.0, 9000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11000.0, 11500.0, 12000.0, 12500.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 16000.0, 16000.0, 16000.0, 16000.0, 16000.0, 16500.0, 16500.0, 16500.0, 20000.0, 20000.0, 20000.0, 20000.0, 20000.0, 20000.0, 21000.0, 21000.0, 21000.0, 21000.0, 21000.0, 21000.0, 22500.0, 22500.0, 22500.0, 22500.0, 22500.0, 23000.0, 23500.0, 23500.0, 23500.0, 23500.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28000.0, 28500.0, 28500.0, 28500.0, 36500.0, 36500.0, 36500.0, 36500.0, 36500.0, 36500.0, 36500.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 40000.0, 41500.0, 42000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43000.0, 43500.0, 46000.0, 46000.0, 46000.0, 47000.0, 47000.0, 47500.0, 47500.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 50500.0, 50500.0, 50500.0, 50500.0, 52500.0, 52500.0, 52500.0, 52500.0, 53500.0, 54000.0, 54000.0, 54000.0, 55500.0, 56000.0, 56500.0, 56500.0, 56500.0, 56500.0, 56500.0, 56500.0, 56500.0, 57500.0, 58000.0, 59500.0, 59500.0, 59500.0, 59500.0, 61000.0, 61000.0, 65000.0, 65000.0, 65000.0, 65000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 71000.0, 73000.0, 73000.0, 74500.0, 74500.0, 74500.0, 74500.0, 74500.0, 74500.0, 74500.0, 74500.0, 75500.0, 75500.0, 75500.0, 77000.0, 77000.0, 77000.0, 78000.0, 78000.0, 78000.0, 79500.0, 80000.0, 80000.0, 83500.0, 83500.0, 83500.0, 83500.0, 83500.0, 83500.0, 84500.0, 84500.0, 84500.0, 84500.0, 84500.0, 84500.0, 84500.0, 85000.0, 85500.0, 86000.0, 86000.0, 86000.0, 86500.0, 87000.0, 88500.0, 88500.0, 89500.0, 90500.0, 90500.0, 91000.0, 91000.0, 91000.0, 91000.0, 91000.0, 95000.0, 95000.0, 95000.0, 96000.0, 96000.0, 96000.0, 97500.0, 97500.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 101500.0, 101500.0, 101500.0, 101500.0, 101500.0, 101500.0, 102000.0, 102000.0, 102500.0, 103000.0, 103500.0, 105500.0, 105500.0, 106000.0, 106000.0, 106000.0, 106500.0, 106500.0, 106500.0, 107500.0, 107500.0, 107500.0, 108000.0, 111000.0, 111000.0, 111000.0, 111000.0, 111000.0, 111000.0, 111000.0, 111000.0, 112000.0, 112000.0, 113000.0, 113500.0, 114000.0, 114000.0, 115000.0, 116500.0, 116500.0, 116500.0, 116500.0, 116500.0, 118000.0, 118000.0, 118500.0, 118500.0, 118500.0, 118500.0, 118500.0, 119000.0, 119500.0, 119500.0, 119500.0, 119500.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 123500.0, 141000.0, 141000.0, 141000.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 141500.0, 142000.0, 142000.0, 142000.0, 142000.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 142500.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 145500.0, 145500.0, 145500.0, 146000.0, 146000.0, 146000.0, 147000.0, 147500.0, 148500.0, 148500.0, 153500.0, 153500.0, 153500.0, 153500.0, 153500.0, 153500.0, 153500.0, 154500.0, 154500.0, 154500.0, 154500.0, 154500.0, 154500.0, 154500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165000.0, 167500.0, 167500.0, 167500.0, 167500.0, 167500.0, 167500.0, 167500.0, 167500.0, 168000.0, 168000.0, 168000.0, 170000.0, 170000.0, 172000.0, 172000.0, 172500.0, 172500.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 174000.0, 178000.0, 178000.0, 178000.0, 178000.0, 178000.0, 179000.0, 179500.0, 179500.0, 179500.0, 180000.0, 180000.0, 180000.0, 180000.0, 180000.0, 182000.0, 182000.0, 182000.0, 182000.0, 182000.0, 183000.0, 183000.0, 183500.0, 185000.0, 186000.0, 186000.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 186500.0, 187500.0, 187500.0, 188000.0, 189000.0, 190500.0, 190500.0, 190500.0, 190500.0, 191000.0, 191000.0, 191500.0, 191500.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 196500.0, 196500.0, 197500.0, 197500.0, 197500.0, 198000.0, 198000.0, 198000.0, 198000.0, 199500.0, 199500.0, 200000.0, 200000.0, 201000.0, 201000.0, 201000.0, 202000.0, 202000.0, 202000.0, 202000.0, 202500.0, 203000.0, 203500.0, 204000.0, 204500.0, 205000.0, 206000.0, 206000.0, 206000.0, 206000.0, 206000.0, 206000.0, 207000.0, 207500.0, 207500.0, 208500.0, 208500.0, 208500.0, 209500.0, 209500.0, 210000.0, 210000.0, 210000.0, 211000.0, 211000.0, 213000.0, 213000.0, 213000.0, 213000.0, 213000.0, 213500.0, 213500.0, 213500.0, 213500.0, 213500.0, 213500.0, 216000.0, 216000.0, 216000.0, 220000.0, 220000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 221000.0, 222500.0, 222500.0, 227500.0, 227500.0, 227500.0, 228500.0, 228500.0, 228500.0, 228500.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 229000.0, 232500.0, 232500.0, 232500.0, 232500.0, 234000.0, 234000.0, 234000.0, 234000.0, 234000.0, 234000.0, 234000.0, 234000.0, 234000.0, 235000.0, 235500.0, 235500.0, 235500.0, 235500.0, 235500.0, 235500.0, 235500.0, 236500.0, 236500.0, 237500.0, 237500.0, 237500.0, 237500.0, 237500.0, 240000.0, 240000.0, 243000.0, 243000.0, 243000.0, 244000.0, 244000.0, 244000.0, 244000.0, 244000.0, 245500.0, 245500.0, 246000.0, 246000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 250000.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 254500.0, 254500.0, 255000.0, 256000.0, 256000.0, 256000.0, 256000.0, 256000.0, 256000.0, 256000.0, 256000.0, 256500.0, 256500.0, 260500.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 261000.0, 262000.0, 262000.0, 262500.0, 262500.0, 264000.0, 264000.0, 264000.0, 264000.0, 266500.0, 267000.0, 267000.0, 267000.0, 267500.0, 267500.0, 267500.0, 268500.0, 268500.0, 268500.0, 269500.0, 269500.0, 269500.0, 270000.0, 270000.0, 270000.0, 270000.0, 270000.0, 270000.0, 270500.0, 271000.0, 271000.0, 271500.0, 271500.0, 274000.0, 274000.0, 274000.0, 274000.0, 274000.0, 274000.0, 274000.0, 276000.0, 276000.0, 276000.0, 276000.0, 276000.0, 276500.0, 276500.0, 276500.0, 276500.0, 277000.0, 277500.0, 277500.0, 279000.0, 279500.0, 279500.0, 279500.0, 280000.0, 280000.0, 280000.0, 280500.0, 283500.0, 283500.0, 283500.0, 286000.0, 286000.0, 286000.0, 286000.0, 286000.0, 286500.0, 286500.0, 286500.0, 286500.0, 287500.0, 289000.0, 289000.0, 289000.0, 289000.0, 289000.0, 289000.0, 289000.0, 289500.0, 289500.0, 290000.0, 290000.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 293000.0, 293000.0, 293000.0, 293500.0, 293500.0, 293500.0, 293500.0, 294000.0, 297000.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 298500.0, 299500.0, 301000.0, 301000.0, 301000.0, 301500.0, 301500.0, 302000.0, 302000.0, 302500.0, 302500.0, 303500.0, 303500.0, 304500.0, 304500.0, 305000.0, 305000.0, 305500.0, 306500.0, 306500.0, 306500.0, 306500.0, 307500.0, 307500.0, 308000.0, 308000.0, 308500.0, 308500.0, 309000.0, 309500.0, 310000.0, 311000.0, 311000.0, 311000.0, 311500.0, 311500.0, 312000.0, 312000.0, 312000.0, 312000.0, 312500.0, 312500.0, 318500.0, 318500.0, 318500.0, 318500.0, 318500.0, 318500.0, 319500.0, 319500.0, 319500.0, 319500.0, 319500.0, 319500.0, 319500.0, 320000.0, 322500.0, 323000.0, 323500.0, 323500.0, 323500.0, 323500.0, 323500.0, 323500.0, 323500.0, 323500.0, 324000.0, 324000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 329000.0, 329000.0, 329000.0, 329000.0, 329500.0, 329500.0, 329500.0, 329500.0, 329500.0, 329500.0, 329500.0, 334000.0, 334000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335000.0, 335500.0, 336000.0, 336000.0, 336000.0, 336000.0, 336000.0, 336000.0, 336000.0, 338500.0, 341500.0, 341500.0, 341500.0, 341500.0, 341500.0, 341500.0, 344000.0, 344000.0, 344000.0, 344000.0, 345000.0, 345000.0, 345000.0, 345000.0, 350500.0, 350500.0, 350500.0, 350500.0, 350500.0, 350500.0, 350500.0, 350500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 351500.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 360000.0, 360000.0, 360000.0, 360000.0, 360000.0, 360000.0, 360000.0, 360000.0, 361500.0, 361500.0, 361500.0, 362000.0, 362000.0, 365500.0, 365500.0, 365500.0, 366500.0, 366500.0, 366500.0, 366500.0, 368000.0, 368000.0, 368000.0, 368500.0, 368500.0, 369000.0, 369000.0, 370500.0, 370500.0, 370500.0, 371500.0, 371500.0, 372000.0, 372500.0, 372500.0, 373500.0, 373500.0, 373500.0, 376500.0, 376500.0, 377500.0, 377500.0, 378000.0, 378000.0, 379000.0, 379500.0, 379500.0, 379500.0, 380500.0, 380500.0, 380500.0, 381500.0, 382000.0, 382500.0, 382500.0, 382500.0, 382500.0, 383000.0, 383500.0, 383500.0, 384000.0, 384000.0, 384000.0, 384000.0, 386500.0, 386500.0, 386500.0, 387000.0, 390000.0, 390000.0, 390000.0, 390000.0, 394500.0, 394500.0, 394500.0, 394500.0, 394500.0, 395500.0, 395500.0, 395500.0, 395500.0, 395500.0, 395500.0, 395500.0, 395500.0, 395500.0, 396000.0, 397500.0, 397500.0, 398500.0, 398500.0, 398500.0, 398500.0, 398500.0, 398500.0, 403500.0, 403500.0, 403500.0, 403500.0, 403500.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 408000.0, 410500.0, 410500.0, 411000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412500.0, 413000.0, 413000.0, 414000.0, 414000.0, 414000.0, 414000.0, 416000.0, 416000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 419000.0, 422500.0, 422500.0, 422500.0, 422500.0, 422500.0, 422500.0, 422500.0, 422500.0, 424500.0, 424500.0, 424500.0, 424500.0, 426000.0, 426000.0, 426000.0, 426000.0, 426000.0, 426000.0, 426000.0, 426000.0, 426000.0, 426500.0, 426500.0, 427500.0, 427500.0, 427500.0, 429000.0, 429000.0, 429500.0, 430000.0, 430500.0, 430500.0, 431000.0, 431000.0, 431500.0, 431500.0, 432000.0, 434000.0, 434000.0, 434000.0, 434500.0, 435000.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 439500.0, 439500.0, 439500.0, 439500.0, 440000.0, 441000.0, 442000.0, 442000.0, 442000.0, 448000.0, 448000.0, 448000.0, 448000.0, 448000.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 451000.0, 453500.0, 454500.0, 454500.0, 454500.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457000.0, 457500.0, 460000.0, 460000.0, 460000.0, 460000.0, 460000.0, 461500.0, 461500.0, 461500.0, 461500.0, 461500.0, 461500.0, 461500.0, 462000.0, 464000.0, 464000.0, 464000.0, 465000.0, 465000.0, 465500.0, 465500.0, 465500.0, 465500.0, 466000.0, 466000.0, 468000.0, 468000.0, 468000.0, 469000.0, 469000.0, 469000.0, 469000.0, 471000.0, 471000.0, 471000.0, 471000.0, 471500.0, 474500.0, 474500.0, 474500.0, 474500.0, 474500.0, 474500.0, 474500.0, 474500.0, 475500.0, 476500.0, 476500.0, 476500.0, 476500.0, 477500.0, 478000.0, 478000.0, 478000.0, 478000.0, 478000.0, 480000.0, 480000.0, 480000.0, 480000.0, 481500.0, 483500.0, 485000.0, 485000.0, 486000.0, 486000.0, 488500.0, 488500.0, 488500.0, 488500.0, 488500.0, 488500.0, 488500.0, 492500.0, 492500.0, 492500.0, 492500.0, 492500.0, 492500.0, 494000.0, 494000.0, 494000.0, 494000.0, 497000.0, 497000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 501000.0, 502000.0, 502000.0, 502000.0, 502000.0, 503000.0, 503000.0, 503000.0, 503000.0, 503000.0, 503000.0, 505000.0, 506500.0, 506500.0, 507000.0, 507000.0, 507000.0, 507000.0, 507000.0, 507500.0, 507500.0, 507500.0, 508000.0, 508000.0, 508000.0, 508000.0, 509000.0, 509500.0, 510000.0, 510000.0, 511000.0, 514000.0, 514000.0, 514000.0, 514000.0, 515000.0, 515000.0, 515000.0, 515000.0, 517000.0, 517000.0, 517000.0, 517000.0, 519000.0, 519000.0, 519500.0, 519500.0, 519500.0, 520500.0, 520500.0, 520500.0, 521500.0, 521500.0, 521500.0, 523500.0, 525000.0, 525000.0, 525000.0, 525000.0, 525000.0, 525000.0, 525000.0, 525000.0, 525000.0, 526000.0, 526000.0, 527500.0, 527500.0, 527500.0, 529000.0, 529000.0, 529000.0, 529500.0, 530000.0, 530500.0, 533000.0, 533000.0, 533000.0, 534000.0, 534000.0, 534000.0, 534000.0, 534000.0, 535500.0, 536000.0, 536000.0, 536000.0, 537000.0, 537000.0, 537500.0, 538000.0, 538500.0, 544000.0, 544000.0, 544000.0, 544000.0, 544000.0, 545000.0, 545000.0, 545000.0, 545000.0, 545000.0, 545000.0, 546000.0, 546000.0, 546000.0, 546000.0, 549000.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 550500.0, 551500.0, 551500.0, 553000.0, 553000.0, 553000.0, 553000.0, 553000.0, 555500.0, 555500.0, 555500.0, 557500.0, 557500.0, 558500.0, 559000.0, 559500.0, 559500.0, 561000.0, 561000.0, 562000.0, 562000.0, 563000.0, 563000.0, 563000.0, 564000.0, 564000.0, 564000.0, 565000.0, 565500.0, 565500.0, 565500.0, 565500.0, 565500.0, 568500.0, 568500.0, 568500.0, 569000.0, 569000.0, 569500.0, 574000.0, 574000.0, 574000.0, 574000.0, 574000.0, 574000.0, 574000.0, 575500.0, 576000.0, 576000.0, 576000.0, 577000.0, 577000.0, 577000.0, 577500.0, 578500.0, 578500.0, 579500.0, 579500.0, 579500.0, 579500.0, 582500.0, 582500.0, 582500.0, 582500.0, 582500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 585000.0, 585000.0, 585000.0, 586000.0, 586500.0, 586500.0, 586500.0, 586500.0, 586500.0, 590000.0, 590000.0, 590000.0, 590000.0, 591000.0, 591000.0, 591000.0, 593500.0, 593500.0, 593500.0, 593500.0, 593500.0, 593500.0, 593500.0, 593500.0, 593500.0, 594000.0, 594000.0, 594500.0, 597500.0, 597500.0, 597500.0, 597500.0, 597500.0, 598500.0, 598500.0, 598500.0, 598500.0, 599000.0, 599500.0, 600000.0, 600000.0, 600000.0, 600000.0, 600000.0, 600000.0, 600000.0, 606000.0, 606000.0, 606000.0, 608000.0, 608000.0, 608000.0, 608000.0, 608000.0, 608000.0, 608000.0, 608000.0, 608500.0, 608500.0, 608500.0, 609000.0, 609000.0, 610000.0, 610000.0, 610500.0, 611500.0, 612500.0, 612500.0, 612500.0, 612500.0, 614000.0, 614000.0, 614000.0, 615500.0, 615500.0, 615500.0, 615500.0, 615500.0, 616000.0, 616000.0, 616000.0, 616000.0, 616500.0, 620000.0, 620000.0, 620000.0, 620000.0, 620000.0, 620000.0, 621000.0, 621500.0, 621500.0, 621500.0, 621500.0, 621500.0, 621500.0, 621500.0, 621500.0, 621500.0, 622000.0, 624000.0, 624000.0, 624000.0, 626500.0, 627500.0, 630000.0, 630000.0, 630000.0, 630000.0, 630000.0, 630000.0, 630000.0, 630000.0, 630000.0, 631500.0, 631500.0, 631500.0, 631500.0, 631500.0, 631500.0, 631500.0, 631500.0, 636500.0, 636500.0, 636500.0, 636500.0, 639000.0, 639000.0, 639000.0, 639000.0, 639000.0, 639000.0, 639000.0, 639000.0, 639000.0, 639500.0, 639500.0, 639500.0, 639500.0, 640000.0, 640500.0, 640500.0, 641000.0, 641000.0, 641000.0, 643000.0, 643000.0, 643000.0, 643500.0, 643500.0, 643500.0, 644000.0, 647000.0, 647000.0, 647000.0, 647000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 648000.0, 650000.0, 650000.0, 654500.0, 654500.0, 654500.0, 659000.0, 659000.0, 659000.0, 659000.0, 659000.0, 659000.0, 659500.0, 659500.0, 659500.0, 659500.0, 659500.0, 660000.0, 660000.0, 661000.0, 666000.0, 666000.0, 666000.0, 667000.0, 667000.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 667500.0, 669000.0, 669000.0, 670000.0, 670000.0, 672000.0, 672000.0, 673500.0, 673500.0, 673500.0, 673500.0, 674500.0, 674500.0, 675500.0, 678000.0, 678000.0, 678000.0, 678000.0, 678000.0, 680000.0, 680000.0, 680000.0, 681000.0, 681000.0, 681000.0, 681000.0, 681000.0, 681000.0, 681000.0, 684000.0, 684000.0, 684500.0, 685000.0, 685500.0, 687000.0, 687000.0, 687000.0, 687000.0, 687000.0, 687500.0, 688000.0, 688000.0, 688000.0, 691500.0, 691500.0, 692500.0, 692500.0, 692500.0, 693500.0, 693500.0, 693500.0, 693500.0, 694000.0, 697500.0, 697500.0, 697500.0, 697500.0, 698000.0, 702000.0, 702000.0, 702000.0, 702000.0, 702000.0, 702000.0, 702000.0, 702000.0, 702500.0, 702500.0, 702500.0, 702500.0, 705500.0, 705500.0, 705500.0, 707500.0, 708500.0, 708500.0, 708500.0, 708500.0, 708500.0, 708500.0, 708500.0, 708500.0, 708500.0, 713000.0, 713000.0, 713000.0, 713000.0, 713000.0, 713000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 714000.0, 716500.0, 716500.0, 716500.0, 716500.0, 716500.0, 720000.0, 720000.0, 720000.0, 720000.0, 720000.0, 720000.0, 720000.0, 720000.0, 721500.0, 721500.0, 721500.0, 721500.0, 721500.0, 721500.0, 722500.0, 723000.0, 723000.0, 723000.0, 727500.0, 727500.0, 727500.0, 727500.0, 727500.0, 729013.6875, 729013.6875, 729013.6875], "elapsed": [3655.418872833252, 3655.418872833252, 3655.418872833252, 7205.908298492432, 7205.908298492432, 7205.908298492432, 7205.908298492432, 9110.907554626465, 9110.907554626465, 9110.907554626465, 10982.590675354004, 15735.280275344849, 15735.280275344849, 15735.280275344849, 15735.280275344849, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 20525.644779205322, 21727.96654701233, 22915.66300392151, 24111.584663391113, 29804.231882095337, 29804.231882095337, 29804.231882095337, 29804.231882095337, 29804.231882095337, 32556.09655380249, 32556.09655380249, 32556.09655380249, 32556.09655380249, 32556.09655380249, 36970.75414657593, 36970.75414657593, 36970.75414657593, 48335.25371551514, 48335.25371551514, 48335.25371551514, 48335.25371551514, 48335.25371551514, 48335.25371551514, 54660.70318222046, 54660.70318222046, 54660.70318222046, 54660.70318222046, 54660.70318222046, 54660.70318222046, 59829.31637763977, 59829.31637763977, 59829.31637763977, 59829.31637763977, 59829.31637763977, 64432.35468864441, 66301.03325843811, 66301.03325843811, 66301.03325843811, 66301.03325843811, 68178.15804481506, 68178.15804481506, 68178.15804481506, 68178.15804481506, 68178.15804481506, 68178.15804481506, 68178.15804481506, 74761.23189926147, 74761.23189926147, 74761.23189926147, 74761.23189926147, 74761.23189926147, 75782.3634147644, 75782.3634147644, 75782.3634147644, 91898.34809303284, 91898.34809303284, 91898.34809303284, 91898.34809303284, 91898.34809303284, 91898.34809303284, 91898.34809303284, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 101122.49565124512, 105369.76480484009, 106906.03184700012, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 109818.55297088623, 111227.77962684631, 120760.79535484314, 120760.79535484314, 120760.79535484314, 128005.21516799927, 128005.21516799927, 129818.57705116272, 129818.57705116272, 131536.81564331055, 131536.81564331055, 131536.81564331055, 131536.81564331055, 131536.81564331055, 131536.81564331055, 131536.81564331055, 135257.55548477173, 135257.55548477173, 135257.55548477173, 135257.55548477173, 138712.69369125366, 138712.69369125366, 138712.69369125366, 138712.69369125366, 140790.18354415894, 141830.35802841187, 141830.35802841187, 141830.35802841187, 144933.94780158997, 146084.94758605957, 147344.34986114502, 147344.34986114502, 147344.34986114502, 147344.34986114502, 147344.34986114502, 147344.34986114502, 147344.34986114502, 149532.81927108765, 150720.23391723633, 154214.74242210388, 154214.74242210388, 154214.74242210388, 154214.74242210388, 157916.97311401367, 157916.97311401367, 167618.50237846375, 167618.50237846375, 167618.50237846375, 167618.50237846375, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 190526.26156806946, 198271.20089530945, 198271.20089530945, 200679.4195175171, 200679.4195175171, 200679.4195175171, 200679.4195175171, 200679.4195175171, 200679.4195175171, 200679.4195175171, 200679.4195175171, 202634.26041603088, 202634.26041603088, 202634.26041603088, 205219.3193435669, 205219.3193435669, 205219.3193435669, 207142.96054840088, 207142.96054840088, 207142.96054840088, 209945.85275650024, 211101.75561904907, 211101.75561904907, 218334.31839942932, 218334.31839942932, 218334.31839942932, 218334.31839942932, 218334.31839942932, 218334.31839942932, 221180.85932731628, 221180.85932731628, 221180.85932731628, 221180.85932731628, 221180.85932731628, 221180.85932731628, 221180.85932731628, 222374.38201904297, 223622.19619750977, 224922.31965065002, 224922.31965065002, 224922.31965065002, 226136.8408203125, 227459.15961265564, 231441.18976593018, 231441.18976593018, 234389.71829414368, 237172.68681526184, 237172.68681526184, 238702.3482322693, 238702.3482322693, 238702.3482322693, 238702.3482322693, 238702.3482322693, 252420.95470428467, 252420.95470428467, 252420.95470428467, 255464.74432945251, 255464.74432945251, 255464.74432945251, 257763.3035182953, 257763.3035182953, 262855.27896881104, 262855.27896881104, 262855.27896881104, 262855.27896881104, 262855.27896881104, 262855.27896881104, 262855.27896881104, 262855.27896881104, 265024.64389801025, 265024.64389801025, 265024.64389801025, 265024.64389801025, 265024.64389801025, 265024.64389801025, 265998.162984848, 265998.162984848, 266961.6975784302, 268038.5935306549, 269102.0517349243, 273392.92645454407, 273392.92645454407, 274680.4850101471, 274680.4850101471, 274680.4850101471, 275927.9465675354, 275927.9465675354, 275927.9465675354, 278378.2260417938, 278378.2260417938, 278378.2260417938, 279803.8477897644, 287261.79242134094, 287261.79242134094, 287261.79242134094, 287261.79242134094, 287261.79242134094, 287261.79242134094, 287261.79242134094, 287261.79242134094, 289925.324678421, 289925.324678421, 292696.8958377838, 296585.0839614868, 299046.4725494385, 299046.4725494385, 301731.36830329895, 305860.27693748474, 305860.27693748474, 305860.27693748474, 305860.27693748474, 305860.27693748474, 309911.9567871094, 309911.9567871094, 311440.96302986145, 311440.96302986145, 311440.96302986145, 311440.96302986145, 311440.96302986145, 312966.74060821533, 314582.0698738098, 314582.0698738098, 314582.0698738098, 314582.0698738098, 316285.2654457092, 316285.2654457092, 316285.2654457092, 316285.2654457092, 316285.2654457092, 321226.7334461212, 350011.95096969604, 350011.95096969604, 350011.95096969604, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 351876.2249946594, 353410.18557548523, 353410.18557548523, 353410.18557548523, 353410.18557548523, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 355271.12007141113, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 360024.8053073883, 362305.9616088867, 362305.9616088867, 362305.9616088867, 365304.5892715454, 365304.5892715454, 365304.5892715454, 367861.9568347931, 368801.5146255493, 370697.06749916077, 370697.06749916077, 379674.8847961426, 379674.8847961426, 379674.8847961426, 379674.8847961426, 379674.8847961426, 379674.8847961426, 379674.8847961426, 382091.21966362, 382091.21966362, 382091.21966362, 382091.21966362, 382091.21966362, 382091.21966362, 382091.21966362, 395466.14146232605, 395466.14146232605, 395466.14146232605, 395466.14146232605, 395466.14146232605, 395466.14146232605, 395466.14146232605, 395466.14146232605, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 410742.4292564392, 421541.99481010437, 421541.99481010437, 421541.99481010437, 421541.99481010437, 421541.99481010437, 421541.99481010437, 421541.99481010437, 421541.99481010437, 423360.6677055359, 423360.6677055359, 423360.6677055359, 426535.27665138245, 426535.27665138245, 430166.0258769989, 430166.0258769989, 431310.6412887573, 431310.6412887573, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 434439.92376327515, 441778.08141708374, 441778.08141708374, 441778.08141708374, 441778.08141708374, 441778.08141708374, 444147.2623348236, 448886.2192630768, 448886.2192630768, 448886.2192630768, 451200.04200935364, 451200.04200935364, 451200.04200935364, 451200.04200935364, 451200.04200935364, 456246.924161911, 456246.924161911, 456246.924161911, 456246.924161911, 456246.924161911, 459132.1849822998, 459132.1849822998, 460596.16470336914, 464706.36081695557, 467818.40205192566, 467818.40205192566, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 469630.90682029724, 472443.3305263519, 472443.3305263519, 474186.21373176575, 477397.9187011719, 485600.10290145874, 485600.10290145874, 485600.10290145874, 485600.10290145874, 487231.59861564636, 487231.59861564636, 488876.37972831726, 488876.37972831726, 490581.8028450012, 490581.8028450012, 490581.8028450012, 490581.8028450012, 490581.8028450012, 490581.8028450012, 490581.8028450012, 490581.8028450012, 497084.51414108276, 497084.51414108276, 499157.6247215271, 499157.6247215271, 499157.6247215271, 500136.1060142517, 500136.1060142517, 500136.1060142517, 500136.1060142517, 502931.6248893738, 502931.6248893738, 503944.83757019043, 503944.83757019043, 505896.963596344, 505896.963596344, 505896.963596344, 508096.5943336487, 508096.5943336487, 508096.5943336487, 508096.5943336487, 509302.4456501007, 510560.2295398712, 511725.18515586853, 512874.51124191284, 514057.7108860016, 515285.6287956238, 517761.06691360474, 517761.06691360474, 517761.06691360474, 517761.06691360474, 517761.06691360474, 517761.06691360474, 520235.773563385, 521464.23053741455, 521464.23053741455, 526612.779378891, 526612.779378891, 526612.779378891, 530689.7149085999, 530689.7149085999, 532157.9051017761, 532157.9051017761, 532157.9051017761, 535254.1689872742, 535254.1689872742, 541233.080625534, 541233.080625534, 541233.080625534, 541233.080625534, 541233.080625534, 542853.2562255859, 542853.2562255859, 542853.2562255859, 542853.2562255859, 542853.2562255859, 542853.2562255859, 550524.3945121765, 550524.3945121765, 550524.3945121765, 557120.9127902985, 557120.9127902985, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 559501.3108253479, 566322.7472305298, 566322.7472305298, 576256.0725212097, 576256.0725212097, 576256.0725212097, 578895.1182365417, 578895.1182365417, 578895.1182365417, 578895.1182365417, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 580329.3113708496, 592286.4084243774, 592286.4084243774, 592286.4084243774, 592286.4084243774, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 596481.1828136444, 599269.5355415344, 600838.8676643372, 600838.8676643372, 600838.8676643372, 600838.8676643372, 600838.8676643372, 600838.8676643372, 600838.8676643372, 603861.8791103363, 603861.8791103363, 607176.9654750824, 607176.9654750824, 607176.9654750824, 607176.9654750824, 607176.9654750824, 614251.8904209137, 614251.8904209137, 622434.2749118805, 622434.2749118805, 622434.2749118805, 624471.1089134216, 624471.1089134216, 624471.1089134216, 624471.1089134216, 624471.1089134216, 627327.3174762726, 627327.3174762726, 628331.1877250671, 628331.1877250671, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 636972.0895290375, 643452.1028995514, 643452.1028995514, 643452.1028995514, 643452.1028995514, 643452.1028995514, 647347.7902412415, 647347.7902412415, 648637.65001297, 651449.5360851288, 651449.5360851288, 651449.5360851288, 651449.5360851288, 651449.5360851288, 651449.5360851288, 651449.5360851288, 651449.5360851288, 655519.1621780396, 655519.1621780396, 667943.5806274414, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 669734.9090576172, 672914.5202636719, 672914.5202636719, 674507.2720050812, 674507.2720050812, 679241.0273551941, 679241.0273551941, 679241.0273551941, 679241.0273551941, 682997.6191520691, 683932.3151111603, 683932.3151111603, 683932.3151111603, 684836.6003036499, 684836.6003036499, 684836.6003036499, 686562.1852874756, 686562.1852874756, 686562.1852874756, 688662.7469062805, 688662.7469062805, 688662.7469062805, 689807.7521324158, 689807.7521324158, 689807.7521324158, 689807.7521324158, 689807.7521324158, 689807.7521324158, 690862.1261119843, 692049.9129295349, 692049.9129295349, 695705.6686878204, 695705.6686878204, 702495.3961372375, 702495.3961372375, 702495.3961372375, 702495.3961372375, 702495.3961372375, 702495.3961372375, 702495.3961372375, 707486.0322475433, 707486.0322475433, 707486.0322475433, 707486.0322475433, 707486.0322475433, 708783.7526798248, 708783.7526798248, 708783.7526798248, 708783.7526798248, 710181.554555893, 711470.3328609467, 711470.3328609467, 715339.8866653442, 716834.6979618073, 716834.6979618073, 716834.6979618073, 718250.6513595581, 718250.6513595581, 718250.6513595581, 719626.2505054474, 727587.7394676208, 727587.7394676208, 727587.7394676208, 739694.1692829132, 739694.1692829132, 739694.1692829132, 739694.1692829132, 739694.1692829132, 741889.1532421112, 741889.1532421112, 741889.1532421112, 741889.1532421112, 745304.1794300079, 748966.3014411926, 748966.3014411926, 748966.3014411926, 748966.3014411926, 748966.3014411926, 748966.3014411926, 748966.3014411926, 749955.9729099274, 749955.9729099274, 750935.2719783783, 750935.2719783783, 753585.2882862091, 753585.2882862091, 753585.2882862091, 753585.2882862091, 753585.2882862091, 753585.2882862091, 756480.372428894, 756480.372428894, 756480.372428894, 757460.4868888855, 757460.4868888855, 757460.4868888855, 757460.4868888855, 758463.8137817383, 764830.3079605103, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 768219.0346717834, 770531.2957763672, 773908.5874557495, 773908.5874557495, 773908.5874557495, 775289.9701595306, 775289.9701595306, 776594.8812961578, 776594.8812961578, 777884.156703949, 777884.156703949, 780551.0003566742, 780551.0003566742, 783264.050245285, 783264.050245285, 784639.4481658936, 784639.4481658936, 786025.8736610413, 789073.4593868256, 789073.4593868256, 789073.4593868256, 789073.4593868256, 792031.989812851, 792031.989812851, 793530.2934646606, 793530.2934646606, 795102.9222011566, 795102.9222011566, 799257.7948570251, 801715.7926559448, 803320.7995891571, 809738.8789653778, 809738.8789653778, 809738.8789653778, 811527.378320694, 811527.378320694, 813333.0655097961, 813333.0655097961, 813333.0655097961, 813333.0655097961, 814109.3397140503, 814109.3397140503, 823542.5806045532, 823542.5806045532, 823542.5806045532, 823542.5806045532, 823542.5806045532, 823542.5806045532, 825634.8586082458, 825634.8586082458, 825634.8586082458, 825634.8586082458, 825634.8586082458, 825634.8586082458, 825634.8586082458, 826636.7647647858, 832631.12616539, 834101.3207435608, 835647.801399231, 835647.801399231, 835647.801399231, 835647.801399231, 835647.801399231, 835647.801399231, 835647.801399231, 835647.801399231, 836980.783700943, 836980.783700943, 849554.4185638428, 849554.4185638428, 849554.4185638428, 849554.4185638428, 849554.4185638428, 849554.4185638428, 852526.6938209534, 852526.6938209534, 852526.6938209534, 852526.6938209534, 853955.0452232361, 853955.0452232361, 853955.0452232361, 853955.0452232361, 853955.0452232361, 853955.0452232361, 853955.0452232361, 868587.7575874329, 868587.7575874329, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 871996.7923164368, 873515.6815052032, 875157.6058864594, 875157.6058864594, 875157.6058864594, 875157.6058864594, 875157.6058864594, 875157.6058864594, 875157.6058864594, 878988.5122776031, 884146.3232040405, 884146.3232040405, 884146.3232040405, 884146.3232040405, 884146.3232040405, 884146.3232040405, 888546.1308956146, 888546.1308956146, 888546.1308956146, 888546.1308956146, 893591.940164566, 893591.940164566, 893591.940164566, 893591.940164566, 906060.1360797882, 906060.1360797882, 906060.1360797882, 906060.1360797882, 906060.1360797882, 906060.1360797882, 906060.1360797882, 906060.1360797882, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 909086.2340927124, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 936951.6475200653, 940426.8777370453, 940426.8777370453, 940426.8777370453, 940426.8777370453, 940426.8777370453, 940426.8777370453, 940426.8777370453, 940426.8777370453, 942902.9819965363, 942902.9819965363, 942902.9819965363, 943852.9810905457, 943852.9810905457, 949581.8071365356, 949581.8071365356, 949581.8071365356, 951459.1019153595, 951459.1019153595, 951459.1019153595, 951459.1019153595, 954645.9028720856, 954645.9028720856, 954645.9028720856, 955708.9626789093, 955708.9626789093, 956795.4757213593, 956795.4757213593, 959885.3051662445, 959885.3051662445, 959885.3051662445, 962164.2713546753, 962164.2713546753, 963300.6653785706, 964538.0811691284, 964538.0811691284, 967023.0946540833, 967023.0946540833, 967023.0946540833, 974262.1493339539, 974262.1493339539, 976963.7882709503, 976963.7882709503, 978312.2618198395, 978312.2618198395, 981191.8878555298, 982815.1414394379, 982815.1414394379, 982815.1414394379, 989326.5538215637, 989326.5538215637, 989326.5538215637, 992350.7487773895, 993886.0592842102, 995470.6625938416, 995470.6625938416, 995470.6625938416, 995470.6625938416, 999874.0994930267, 1001453.2272815704, 1001453.2272815704, 1003050.3764152527, 1003050.3764152527, 1003050.3764152527, 1003050.3764152527, 1007000.8120536804, 1007000.8120536804, 1007000.8120536804, 1007858.7391376495, 1012968.2788848877, 1012968.2788848877, 1012968.2788848877, 1012968.2788848877, 1022561.9475841522, 1022561.9475841522, 1022561.9475841522, 1022561.9475841522, 1022561.9475841522, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1025177.6964664459, 1026569.6437358856, 1030623.6417293549, 1030623.6417293549, 1033303.925037384, 1033303.925037384, 1033303.925037384, 1033303.925037384, 1033303.925037384, 1033303.925037384, 1049836.267709732, 1049836.267709732, 1049836.267709732, 1049836.267709732, 1049836.267709732, 1054734.15017128, 1054734.15017128, 1054734.15017128, 1054734.15017128, 1054734.15017128, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1064133.38971138, 1068146.7323303223, 1068146.7323303223, 1069127.762556076, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1071205.6994438171, 1072103.5866737366, 1073085.0846767426, 1073085.0846767426, 1075057.5060844421, 1075057.5060844421, 1075057.5060844421, 1075057.5060844421, 1079053.2517433167, 1079053.2517433167, 1083344.185590744, 1083344.185590744, 1083344.185590744, 1083344.185590744, 1083344.185590744, 1083344.185590744, 1085592.4654006958, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1097604.3026447296, 1103128.6771297455, 1103128.6771297455, 1103128.6771297455, 1103128.6771297455, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1107902.5602340698, 1109390.2418613434, 1109390.2418613434, 1112325.3028392792, 1112325.3028392792, 1112325.3028392792, 1116543.518781662, 1116543.518781662, 1120339.6708965302, 1123152.2378921509, 1124707.011461258, 1124707.011461258, 1128916.2108898163, 1128916.2108898163, 1130487.5197410583, 1130487.5197410583, 1132046.7972755432, 1135163.735628128, 1135163.735628128, 1135163.735628128, 1135990.5219078064, 1136843.0988788605, 1142735.434770584, 1142735.434770584, 1142735.434770584, 1142735.434770584, 1142735.434770584, 1142735.434770584, 1142735.434770584, 1144751.0817050934, 1144751.0817050934, 1144751.0817050934, 1144751.0817050934, 1145791.0687923431, 1147780.5216312408, 1150160.2880954742, 1150160.2880954742, 1150160.2880954742, 1166015.8240795135, 1166015.8240795135, 1166015.8240795135, 1166015.8240795135, 1166015.8240795135, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1173419.6121692657, 1174744.794368744, 1181263.052225113, 1184528.4731388092, 1184528.4731388092, 1184528.4731388092, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1191195.732831955, 1192035.8154773712, 1196188.9071464539, 1196188.9071464539, 1196188.9071464539, 1196188.9071464539, 1196188.9071464539, 1202789.7562980652, 1202789.7562980652, 1202789.7562980652, 1202789.7562980652, 1202789.7562980652, 1202789.7562980652, 1202789.7562980652, 1205032.1502685547, 1208555.581331253, 1208555.581331253, 1208555.581331253, 1210832.5312137604, 1210832.5312137604, 1212017.3435211182, 1212017.3435211182, 1212017.3435211182, 1212017.3435211182, 1213098.026752472, 1213098.026752472, 1217161.712884903, 1217161.712884903, 1217161.712884903, 1219479.9892902374, 1219479.9892902374, 1219479.9892902374, 1219479.9892902374, 1224251.0731220245, 1224251.0731220245, 1224251.0731220245, 1224251.0731220245, 1225535.3519916534, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1233219.8688983917, 1235883.807182312, 1241008.8307857513, 1241008.8307857513, 1241008.8307857513, 1241008.8307857513, 1244580.7073116302, 1246168.6279773712, 1246168.6279773712, 1246168.6279773712, 1246168.6279773712, 1246168.6279773712, 1251707.447528839, 1251707.447528839, 1251707.447528839, 1251707.447528839, 1253943.4547424316, 1257161.5374088287, 1259653.4113883972, 1259653.4113883972, 1261561.5019798279, 1261561.5019798279, 1266521.4426517487, 1266521.4426517487, 1266521.4426517487, 1266521.4426517487, 1266521.4426517487, 1266521.4426517487, 1266521.4426517487, 1274480.0884723663, 1274480.0884723663, 1274480.0884723663, 1274480.0884723663, 1274480.0884723663, 1274480.0884723663, 1278177.6432991028, 1278177.6432991028, 1278177.6432991028, 1278177.6432991028, 1285381.2232017517, 1285381.2232017517, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1298167.2282218933, 1301055.8862686157, 1301055.8862686157, 1301055.8862686157, 1301055.8862686157, 1307223.5491275787, 1307223.5491275787, 1307223.5491275787, 1307223.5491275787, 1307223.5491275787, 1307223.5491275787, 1311937.6046657562, 1315124.8455047607, 1315124.8455047607, 1316134.4366073608, 1316134.4366073608, 1316134.4366073608, 1316134.4366073608, 1316134.4366073608, 1317091.1421775818, 1317091.1421775818, 1317091.1421775818, 1318071.3336467743, 1318071.3336467743, 1318071.3336467743, 1318071.3336467743, 1319924.3574142456, 1320965.840101242, 1322008.3129405975, 1322008.3129405975, 1323983.280658722, 1329630.4092407227, 1329630.4092407227, 1329630.4092407227, 1329630.4092407227, 1331970.0708389282, 1331970.0708389282, 1331970.0708389282, 1331970.0708389282, 1340434.0603351593, 1340434.0603351593, 1340434.0603351593, 1340434.0603351593, 1346367.7778244019, 1346367.7778244019, 1347851.145029068, 1347851.145029068, 1347851.145029068, 1350756.891489029, 1350756.891489029, 1350756.891489029, 1353462.1846675873, 1353462.1846675873, 1353462.1846675873, 1358666.054725647, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1363470.1585769653, 1366388.8652324677, 1366388.8652324677, 1371046.4935302734, 1371046.4935302734, 1371046.4935302734, 1374275.6991386414, 1374275.6991386414, 1374275.6991386414, 1375159.9559783936, 1375962.617635727, 1376903.9680957794, 1383802.817106247, 1383802.817106247, 1383802.817106247, 1385747.5912570953, 1385747.5912570953, 1385747.5912570953, 1385747.5912570953, 1385747.5912570953, 1388475.5156040192, 1389551.5406131744, 1389551.5406131744, 1389551.5406131744, 1391443.98355484, 1391443.98355484, 1392420.572757721, 1393438.874721527, 1394493.3342933655, 1405149.8856544495, 1405149.8856544495, 1405149.8856544495, 1405149.8856544495, 1405149.8856544495, 1407697.7143287659, 1407697.7143287659, 1407697.7143287659, 1407697.7143287659, 1407697.7143287659, 1407697.7143287659, 1410107.5761318207, 1410107.5761318207, 1410107.5761318207, 1410107.5761318207, 1417508.0299377441, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1421599.8616218567, 1427499.9797344208, 1427499.9797344208, 1430693.9690113068, 1430693.9690113068, 1430693.9690113068, 1430693.9690113068, 1430693.9690113068, 1434778.6419391632, 1434778.6419391632, 1434778.6419391632, 1438510.2965831757, 1438510.2965831757, 1440316.7543411255, 1441328.562259674, 1442326.905965805, 1442326.905965805, 1445279.663324356, 1445279.663324356, 1447439.965724945, 1447439.965724945, 1449766.8151855469, 1449766.8151855469, 1449766.8151855469, 1452080.578804016, 1452080.578804016, 1452080.578804016, 1454515.1088237762, 1457985.9080314636, 1457985.9080314636, 1457985.9080314636, 1457985.9080314636, 1457985.9080314636, 1466898.0247974396, 1466898.0247974396, 1466898.0247974396, 1468151.9856452942, 1468151.9856452942, 1469461.1694812775, 1481771.8360424042, 1481771.8360424042, 1481771.8360424042, 1481771.8360424042, 1481771.8360424042, 1481771.8360424042, 1481771.8360424042, 1486001.8467903137, 1487563.8213157654, 1487563.8213157654, 1487563.8213157654, 1489248.004436493, 1489248.004436493, 1489248.004436493, 1490127.4592876434, 1496095.3595638275, 1496095.3595638275, 1498235.2271080017, 1498235.2271080017, 1498235.2271080017, 1498235.2271080017, 1503589.3845558167, 1503589.3845558167, 1503589.3845558167, 1503589.3845558167, 1503589.3845558167, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1508814.5892620087, 1509884.0622901917, 1509884.0622901917, 1509884.0622901917, 1512333.9114189148, 1513632.729291916, 1513632.729291916, 1513632.729291916, 1513632.729291916, 1513632.729291916, 1521772.6254463196, 1521772.6254463196, 1521772.6254463196, 1521772.6254463196, 1524690.2146339417, 1524690.2146339417, 1524690.2146339417, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1531393.9981460571, 1532845.7193374634, 1532845.7193374634, 1534338.5055065155, 1542560.7643127441, 1542560.7643127441, 1542560.7643127441, 1542560.7643127441, 1542560.7643127441, 1545645.0703144073, 1545645.0703144073, 1545645.0703144073, 1545645.0703144073, 1547172.9168891907, 1548701.8430233002, 1553196.77734375, 1553196.77734375, 1553196.77734375, 1553196.77734375, 1553196.77734375, 1553196.77734375, 1553196.77734375, 1564272.9008197784, 1564272.9008197784, 1564272.9008197784, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1568680.7363033295, 1569897.0625400543, 1569897.0625400543, 1569897.0625400543, 1571128.756761551, 1571128.756761551, 1573481.1594486237, 1573481.1594486237, 1574658.884048462, 1577065.8059120178, 1582101.657152176, 1582101.657152176, 1582101.657152176, 1582101.657152176, 1587724.3237495422, 1587724.3237495422, 1587724.3237495422, 1592804.2402267456, 1592804.2402267456, 1592804.2402267456, 1592804.2402267456, 1592804.2402267456, 1594553.8167953491, 1594553.8167953491, 1594553.8167953491, 1594553.8167953491, 1596176.8295764923, 1609626.3780593872, 1609626.3780593872, 1609626.3780593872, 1609626.3780593872, 1609626.3780593872, 1609626.3780593872, 1613127.6853084564, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1614941.7295455933, 1616481.2977313995, 1625475.269317627, 1625475.269317627, 1625475.269317627, 1629294.2440509796, 1631284.6579551697, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1635907.2120189667, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1642193.4356689453, 1651218.4784412384, 1651218.4784412384, 1651218.4784412384, 1651218.4784412384, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1657075.8502483368, 1658399.3344306946, 1658399.3344306946, 1658399.3344306946, 1658399.3344306946, 1659594.4039821625, 1660820.7228183746, 1660820.7228183746, 1662143.1095600128, 1662143.1095600128, 1662143.1095600128, 1667153.8498401642, 1667153.8498401642, 1667153.8498401642, 1668662.8875732422, 1668662.8875732422, 1668662.8875732422, 1673170.1085567474, 1680776.1023044586, 1680776.1023044586, 1680776.1023044586, 1680776.1023044586, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1684141.107082367, 1687250.014781952, 1687250.014781952, 1694212.116241455, 1694212.116241455, 1694212.116241455, 1704350.599527359, 1704350.599527359, 1704350.599527359, 1704350.599527359, 1704350.599527359, 1704350.599527359, 1705713.576078415, 1705713.576078415, 1705713.576078415, 1705713.576078415, 1705713.576078415, 1706839.1950130463, 1706839.1950130463, 1708904.8886299133, 1723485.0521087646, 1723485.0521087646, 1723485.0521087646, 1726615.8292293549, 1726615.8292293549, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1728288.0642414093, 1732277.3110866547, 1732277.3110866547, 1734996.993780136, 1734996.993780136, 1740312.451839447, 1740312.451839447, 1742549.5371818542, 1742549.5371818542, 1742549.5371818542, 1742549.5371818542, 1744213.5605812073, 1744213.5605812073, 1746095.4864025116, 1750294.9655056, 1750294.9655056, 1750294.9655056, 1750294.9655056, 1750294.9655056, 1762748.4781742096, 1762748.4781742096, 1762748.4781742096, 1765211.7137908936, 1765211.7137908936, 1765211.7137908936, 1765211.7137908936, 1765211.7137908936, 1765211.7137908936, 1765211.7137908936, 1771800.910949707, 1771800.910949707, 1773007.9905986786, 1774260.192155838, 1775440.4559135437, 1779362.1196746826, 1779362.1196746826, 1779362.1196746826, 1779362.1196746826, 1779362.1196746826, 1780864.39037323, 1782262.9685401917, 1782262.9685401917, 1782262.9685401917, 1794243.6563968658, 1794243.6563968658, 1797244.1158294678, 1797244.1158294678, 1797244.1158294678, 1800284.13772583, 1800284.13772583, 1800284.13772583, 1800284.13772583, 1801800.8081912994, 1814248.6407756805, 1814248.6407756805, 1814248.6407756805, 1814248.6407756805, 1815027.7616977692, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1821836.3423347473, 1822928.2598495483, 1822928.2598495483, 1822928.2598495483, 1822928.2598495483, 1828576.7214298248, 1828576.7214298248, 1828576.7214298248, 1833299.6201515198, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1837216.296672821, 1848123.0962276459, 1848123.0962276459, 1848123.0962276459, 1848123.0962276459, 1848123.0962276459, 1848123.0962276459, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1851278.8116931915, 1861720.355272293, 1861720.355272293, 1861720.355272293, 1861720.355272293, 1861720.355272293, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1873460.6926441193, 1876689.0330314636, 1876689.0330314636, 1876689.0330314636, 1876689.0330314636, 1876689.0330314636, 1876689.0330314636, 1878435.3008270264, 1879383.0814361572, 1879383.0814361572, 1879383.0814361572, 1886543.792963028, 1886543.792963028, 1886543.792963028, 1886543.792963028, 1886543.792963028, 1889318.0812683105, 1889318.0812683105, 1889318.0812683105], "prediction_length": 1806, "reference": "Hi. Ich werde nun unsere Arbeit am generierenden Retrieval und den erweiterten Kontrafakten f\u00fcr Aufgaben der Fragenbeantwortung vorstellen. Dies ist die Arbeit, die ich w\u00e4hrend meines Praktikums bei Google Research gemacht habe, wo ich von Matthew Lamm und Ian Tenney betreut wurde. Um die Aufgabe vorzustellen, m\u00f6chte ich damit beginnen, das Wort kontrafaktisch zu definieren. In dieser Arbeit definieren wir kontrafaktisch als eine St\u00f6rung des eingegebenen Textes, der sich in irgendeiner bedeutungsvollen kontrollierten Weise vom urspr\u00fcnglichen Text unterscheidet. Damit k\u00f6nnen wir \u00fcber die \u00c4nderungen des Ergebnisses oder des Labels der Aufgabe schlussfolgern. Wenn man beispielsweise die W\u00f6rter \u201efaszinierend\u201c zu \u201efesselnd\u201c oder \u201eerwartet\u201c zu \u201etodlangweilig\u201c \u00e4ndert, \u00e4ndert das die Stimmung der Filmrezension. Wird die n\u00e4here Bestimmung \u201eDamen\u201c zur Frage hinzugef\u00fcgt, \u00e4ndert sich die Antwort auf die Frage wie im Beispiel unten dargestellt. Menschen sind in der Regel robust gegen\u00fcber solchen St\u00f6rungen im Vergleich zu NLP-Modellen, die f\u00fcr die Aufgabe trainiert wurden. Warum? Der Datensatz kann mit systematischen Bias gesampelt werden. Das f\u00fchrt zu einer einfachen Entscheidungsgrenze, die kontrafaktisch \u00fcbertreten wird. Das zeigt sich in diesem 2D-Klassifizierungsproblem. Mit meiner Arbeit habe ich herausgefunden, dass das Hinzuf\u00fcgen von kontrafaktischen Beispielen zu den Trainingsdaten das Modell robust gegen solche St\u00f6rungen machen kann. Wenn also Kontrafakten wertvoll sind, wie k\u00f6nnen wir sie dann generieren? Diese Aufgabe ist besonders schwierig f\u00fcr NLP, denn hier sind drei Beispiele aus drei verschiedenen NLP-Aufgaben. Wie Sie sehen k\u00f6nnen, m\u00fcssen Beispiele, die die Entscheidungsgrenze zwischen den Ergebnissen verletzen, sehr sorgf\u00e4ltig erstellt werden, indem einige Attribute des Textes, die hier unterstrichen werden, gest\u00f6rt werden. Dies k\u00f6nnte durch menschliche Annotation geschehen, aber dies ist teuer und voreingenommen. Einige fr\u00fchere Arbeiten konzentrierten sich auf die Verwendung von Syntax-B\u00e4umen oder einer semantischen Rollenbezeichnung. Aber die Reihe von St\u00f6rungen, die durch diese Techniken generiert werden, sind durch den semantischen Rahmen begrenzt. Neuere Arbeiten haben maskierte Sprachmodelle verwendet, um maskierte Teile des Textes auszuf\u00fcllen, um Labels zu \u00e4ndern. Aber herauszufinden, welche Teile des Textes zu st\u00f6ren sind, kann eine Herausforderung sein. Es gibt mehr Herausforderungen f\u00fcr die Generierung von Kontrafakten als f\u00fcr die spezifische Beantwortung der Frage. Diese Aufgabe erfordert Hintergrundwissen. Um beispielsweise die urspr\u00fcngliche Frage zu st\u00f6ren: \u201eIst Indiana Jones und der Tempel des Todes ein Prequel?\u201c Wir m\u00fcssen die anderen Filme im Franchise kennen, um die Frage stellen zu k\u00f6nnen: \u201eIst Indiana Jones \u2013 J\u00e4ger des verlorenen Schatzes ein Prequel?\u201c Dar\u00fcber hinaus k\u00f6nnen zuf\u00e4llige St\u00f6rungen zu Fragen f\u00fchren, die mit den verf\u00fcgbaren Beweisen nicht beantwortet werden k\u00f6nnen oder falsche Voraussetzungen haben. Dar\u00fcber hinaus k\u00f6nnen einige Frage-St\u00f6rungen zu einer signifikanten semantischen Abweichung von der urspr\u00fcnglichen Eingabe f\u00fchren. Zum Beispiel ist diese Frage hier: \u201ePraktiziert Indiana Jones Kindersklaverei im Tempel des Todes?\u201c Wir wollen eine sehr einfache, aber effektive Technik mit dem Namen \u201eRetrieve Generate Filter\u201c oder RGF vorschlagen, um kontrafaktische St\u00f6rungen von Fragen in Angriff zu nehmen. Sie zielt auch darauf ab, alle anderen oben genannten Herausforderungen zu bew\u00e4ltigen. Die Kernintuition hinter RGF ist, dass die notwendigen Hintergrundinformationen, die erforderlich sind, um St\u00f6rungen zu genieren, in den Near-Misses vorhanden sein k\u00f6nnen, die von einem Frage-Antwort-Modell erstellt werden. Zum Beispiel liefert das hochmoderne Modell REALM die folgenden Top-k-Antworten auf die Frage, wer der Kapit\u00e4n des Richmond Football Club ist. Es holt die urspr\u00fcngliche Referenzpassage und die Antwort \u201eTrent Cotchin\u201c als erste Wahl ein. Zus\u00e4tzlich werden auch zus\u00e4tzliche Passagen und Antworten abgerufen, die verwendet werden k\u00f6nnen, um St\u00f6rungen der Frage zu steuern. Zum Beispiel holt es zwei weitere Antworten ein, passend zu den Kapit\u00e4nen der Reservemannschaft und der Damenmannschaft des gleichen Vereins. Dies kann zu interessanten \u00c4nderungen f\u00fchren. Zusammenfassend l\u00e4sst sich sagen, dass RGF zuerst die wichtigsten Top-k-Antworten und Kontexte abruft, die nicht mit der Referenz Antwort in Kontext \u00fcbereinstimmen. Im Anschluss an diesen Schritt konditioniert dieses Fragengenerierungsmodell diese alternativen Antworten, um eine ihnen entsprechende Frage zu generieren. Schlie\u00dflich k\u00f6nnen wir die generierten Fragen nach Minimalit\u00e4t oder nach der Art der semantischen St\u00f6rung, die wir einf\u00fchren m\u00f6chten, filtern. Wenn wir beim Retrieval jeden Schritt genauer durchgehen, dann sehen wir, dass wir einen Abruf verwenden. Dann lesen wir ein Modell wie REALM, das als Eingabe die urspr\u00fcngliche Frage und einen gro\u00dfen Korpus wie etwa Wikipedia hernimmt. Es besteht aus zwei Modulen. Das Retrieval-Modul f\u00fchrt eine \u00c4hnlichkeitssuche \u00fcber einen dichten Index von Passagen durch, um die wichtigsten Top-k-Passagen zur Frage abzurufen. Das Lesemodul extrahiert dann aus jeder Passage einen Bereich als potenzielle Antwort. REALM ruft die Goldpassage und in den meisten F\u00e4llen die Antwort ab. In dieser Arbeit sind wir jedoch mehr an den Antworten und am Kontext interessiert, der sp\u00e4ter abgerufen wird. Im n\u00e4chsten Schritt der Fragengenerierung verwenden wir diese alternativen Antworten und Kontexte, um neue Fragen zu generieren, die diesen Alternativen entsprechen. Das Modell der Fragengenerierung ist ein vortrainierter Text-to-Text-Transformer, der auf die NQ-Daten abgestimmt ist, um eine Frage f\u00fcr eine Antwort zu generieren, die f\u00fcr den Kontext markiert ist. W\u00e4hrend der Interferenz liefern wir das Fragengenerierungsmodell, die alternative Antwort und den Kontext, die wir im fr\u00fcheren Schritt abgerufen haben. Zum Beispiel f\u00fcr die Anfrage: \u201eWer ist der Kapit\u00e4n des Richmond Football Clubs?\u201c REALM ruft Passagen \u00fcber die Damenmannschaft des Clubs ab, die von Jess Kennedy angef\u00fchrt wird. Das fragengenerierende Modell generiert die Anfrage: \u201eWer war Kapit\u00e4nin der ersten Damenmannschaft des Richmond Football Clubs?\u201c Hier gibt es eine spezifische semantische St\u00f6rung. In einer \u00e4hnlichen Art und Weise erhalten wir auch Abfragen, wie etwa: \u201eWer war Kapit\u00e4n der Richmond's VFL-Reservemannschaft?\u201c Oder: \u201eWen hat Graham letztes Jahr im gro\u00dfen Finale geschlagen?\u201c Schlie\u00dflich filtern wir eine Teilmenge der generierten Abfragen basierend auf gew\u00fcnschten Eigenschaften aus. Wie vorhin begr\u00fcndet, m\u00f6chten wir sicherstellen, dass die neue Frage immer noch semantisch nah am Original ist. Bei den Filtertechniken, die keine zus\u00e4tzliche \u00dcberwachung erfordern, speichern wir einfach neue Fragen, die einen kleinen Token-Label und einen Bearbeitungsabstand von der urspr\u00fcnglichen Frage haben. Wir entfernen zum Beispiel die Frage: \u201eWen hat Graham letztes Jahr im gro\u00dfen Finale geschlagen?\u201c Denn diese hat einen l\u00e4ngeren Bearbeitungsabstand zur urspr\u00fcnglichen Frage. In unseren Experimenten zeigen wir, dass diese einfache Heuristik verwendet werden kann, um Trainingsdaten zu erweitern und in die Warteschlange zu stellen. Wir experimentieren auch mit einer Filterstrategie, die auf der Art der semantischen St\u00f6rung basiert. Zu diesem Zweck verwenden wir einen allgemeinen Zerlegungsrahmen f\u00fcr die Anfrage mit dem Namen QED. QED identifiziert zwei Teile der Frage: ein Pr\u00e4dikat und eine Referenz. Referenzen sind Substantivgruppen in der Frage, die Entit\u00e4ten im Kontext entsprechen. Ein Pr\u00e4dikat ist im Grunde der verbleibende Teil der Frage. Zum Beispiel sind wir in der Lage, die Abfrage zu zerlegen: \u201eWer war Kapit\u00e4nin der ersten Damenmannschaft des Richmond Football Clubs?\u201c Wir k\u00f6nnen die Frage in zwei Referenzen zerlegen: das Damenteam vom Richmond Football Club und das Pr\u00e4dikat X (wer war Kapit\u00e4nin?). Ein Modell, das auf Referenzen der Pr\u00e4dikatannotationen f\u00fcr NQ trainiert wurde, erlaubt uns diese Zerlegung der Frage. Die Zerlegung sowohl des Originals als auch der generierten Frage basierend auf QED erm\u00f6glicht es uns, unsere generierten Kontrafakten f\u00fcr die Bewertung zu kategorisieren. Konkret erhalten wir zwei Gruppen von Fragen. Es gibt Fragen, bei denen sich die Referenz \u00e4ndert, aber die Pr\u00e4dikate gleichbleiben, und Fragen, bei denen sich die Pr\u00e4dikate \u00e4ndern und optional Referenzen hinzugef\u00fcgt werden. Hier ist ein Beispiel f\u00fcr eine \u00c4nderung der Referenz: \u201eWer war Kapit\u00e4n der Richmond's VFL-Reservemannschaft?\u201c Das ist eine Ver\u00e4nderung des Pr\u00e4dikats: \u201eWer tr\u00e4gt die Nummer neun beim Club?\u201c Wir bewerten nun die Effektivit\u00e4t von RGF-St\u00f6rungen, wenn diese um die Trainingsdaten erg\u00e4nzt werden. Um insbesondere die Effektivit\u00e4t des kontrafaktischen Aufbaus effektiv bewerten zu k\u00f6nnen, experimentieren wir mit zwei starken Baselines des Datenaufbaus. Die erste Baseline, die als zuf\u00e4llige Antwort- und Fragengenerierung bezeichnet wird, f\u00fcgt Daten hinzu, die keine Relation zur urspr\u00fcnglichen Frage haben. Das hei\u00dft, dass Passagen und Antworten einfach zuf\u00e4llig aus Wikipedia entnommen werden. Diese Baseline f\u00fcgt im Grunde mehr Daten hinzu, die wie NQ aussehen. Mit der zweiten Baseline, der Goldantwort und der Fragengeneration, aktualisieren wir speziell den Retrieval bei unserer Methode. Hier werden alternative Antworten nur aus der gleichen Passage ausgew\u00e4hlt, welche die Goldantwort enth\u00e4lt. Welche Leistung erbringen die Baselines, RGF und der Aufbau beim Leseverst\u00e4ndnis, wo das Modell Zugriff auf Frage und Kontext hat? Wir experimentieren mit sechs von den Datens\u00e4tzen der Dom\u00e4ne und pr\u00e4sentieren hier die Ergebnisse, wobei es bei den Daten um die Trainingsdaten geht und beim Aufbau verdoppelt werden. Wir stellten fest, dass beide Baselines des Datenaufbaus nicht in der Lage sind, unsere Verallgemeinerung der Dom\u00e4ne zu verbessern. Tats\u00e4chlich scheint ein Ensemble von sechs Modellen, die mit den urspr\u00fcnglichen Daten trainiert wurden, die wettbewerbsf\u00e4higste Baseline zu sein. Im Vergleich zu dieser Baseline stellten wir fest, dass RGF-Kontrafakten in der Lage sind, die Leistung au\u00dferhalb der Dom\u00e4ne zu verbessern, w\u00e4hrend die Leistung innerhalb der Dom\u00e4ne beibehalten wird. Dies deutet darauf hin, dass das F\u00fcllen der Argumentationsl\u00fccken beim Modell \u00fcber einen kontrafaktischen Aufbau effektiver ist als mehr Daten aus der Training-Verteilung hinzuzuf\u00fcgen. Dar\u00fcber hinaus fanden wir heraus, dass die Verwendung von Retrievals zur Erprobung alternativer Ergebnisse oder Antworten f\u00fcr effektive CDA wichtig ist. Wir experimentieren auch mit einer offenen Dom\u00e4ne-QA -Einstellung, bei der das Modell nur die Frage sieht. Wir bewerten wieder vier von den Datens\u00e4tzen der Dom\u00e4ne. Wir stellten fest, dass die Baseline-Modelle nicht so effektiv f\u00fcr die Verallgemeinerung der Dom\u00e4ne sind. Allerdings zeigt der Datenaufbau mit RGF signifikantere Verbesserungen. Wir verbessern uns sogar in der Dom\u00e4ne NQ-Datensatz. Wir haben angenommen, dass der kontrafaktische Datenaufbau das Modell beim Lernen von besseren Abfragekodierungen f\u00fcr sehr \u00e4hnliche Abfragen unterst\u00fctzt. Schlie\u00dflich bewerten wir auch die F\u00e4higkeit des Modells, die Einheitlichkeit in der lokalen Nachbarschaft der urspr\u00fcnglichen Frage zu verbessern. Die Einheitlichkeit misst den Anteil der vom Modell korrekt beantworteten Fragen, bei denen sowohl das Original als auch die kontrafaktische Abfrage korrekt beantwortet werden. Dies hilft uns explizit, die Robustheit des Modells bei kleinen St\u00f6rungen in der N\u00e4he der urspr\u00fcnglichen Eingabe zu messen. Wir experimentieren mit f\u00fcnf Datens\u00e4tzen, die Paare von Fragen enthalten, die semantisch nahe beieinander liegen. Abgesehen von den drei Datens\u00e4tzen AQA, AmbigQA und den QUOREF-Kontrasts\u00e4tzen, die bereits verf\u00fcgbar sind, bewerten wir auch RGF-Kontrafakten. Diese sind mit urspr\u00fcnglichen NQ-Fragen gepaart, basierend darauf, ob sie von einer Pr\u00e4dikat- oder Referenz\u00e4nderung betroffen waren. Diese Teilmengen wurden intern annotiert, um Qualit\u00e4tsm\u00e4ngel zu eliminieren. Sie werden als Ressource bereitgestellt. Alle Baselines k\u00f6nnen die Einheitlichkeit signifikant nicht verbessern. Das Ensemble der Modelle kann die Einheitlichkeit geringf\u00fcgig verbessern. Der kontrafaktische Aufbau der RGF kann jedoch eine beeindruckende Steigerung der Einheitlichkeit sowohl bei fr\u00fcheren Datens\u00e4tzen als auch bei den beiden Teilmengen, die wir f\u00fcr Referenz- und Pr\u00e4dikat-St\u00f6rungen ausgew\u00e4hlt haben, aufweisen. Beachten Sie, dass die erweiterten RGF-Daten nicht durch den St\u00f6rungstyp verf\u00e4lscht werden, sondern nur durch die Evaluationss\u00e4tze. Tats\u00e4chlich zeigt eine qualitative \u00dcberpr\u00fcfung der verschiedenen Arten von Kontrafaktoren, dass die generierten Fragen mehrere unterschiedliche St\u00f6rungen enthalten. Zum Beispiel ist diese urspr\u00fcngliche Frage \u00fcber die Bev\u00f6lkerung von Walnut Grove in Minnesota gest\u00f6rt. Diese St\u00f6rung betrifft verschiedene Dimensionen wie Stadt, Bundesland, Land und verschiedene Pr\u00e4dikate wie Lage, Armut, Anzahl von Schulen. Das Audio von St\u00f6rungen ist kontextspezifisch. Bei dieser anderen Frage \u00fcber das Einzelturnier in Wimbledon handelt die St\u00f6rung von der Art des Spiels, der Art des Turniers oder des Spielergebnisses. Abschlie\u00dfende Erkenntnisse: Wir befassen uns mit der Aufgabe des kontrafaktischen Datenaufbaus und St\u00f6rungen bei der Information. Wir suchen nach Abfragen und bew\u00e4ltigen deren einzigartige Herausforderungen durch eine Umkehrung des Generierungsansatzes. Wir \u00fcbergenerieren mithilfe von Near-Misses des Modells und filtern basierend auf dem St\u00f6rungstyp oder der Minimalit\u00e4t. Wir stellten fest, dass diese Technik keiner zus\u00e4tzlichen \u00dcberwachung bedarf und die Beispiele f\u00fcr den Aufbau markiert sind. Der Aufbau verbessert sich dank der Dom\u00e4neverallgemeinerung und der Konsistenz der Nachbarschaft. Zudem stellten wir fest, dass die RGF-Kontrafakten semantisch divers sind, ohne dass Verzerrungen w\u00e4hrend des Aufbaus eingef\u00fchrt wurden. Vielen Dank!", "source": ["2/acl_6060/dev/full_wavs/2022.acl-long.117.wav", "samplerate: 16000 Hz", "channels: 1", "duration: 1e+01:9.014 min", "format: WAV (Microsoft) [WAV]", "subtype: Signed 16 bit PCM [PCM_16]"], "source_length": 729013.6875}
{"index": 2, "prediction": "Hallo, das ist Elena und ich werde unsere Arbeit pr\u00e4sentieren, um Unassimilierte Darlehen in Spanisch und Annotated Corpus und Modellierungsans\u00e4tze zu erkennen. So werden wir abdecken, was lexikale Darlehen ist, die Aufgabe, die wir vorgeschlagen haben, die Daten, die wir ver\u00f6ffentlicht haben, und einige Modelle, die wir entwickelt haben. Lassen Sie uns also mal sehen, was wir erforscht haben. Aber zu Beginn, was ist lexisches Darlehen und warum ist es als NLP-Task wichtig? Nun, lexikale Darlehen ist grunds\u00e4tzlich die Einbeziehung von Worten aus einer Sprache in eine andere. Zum Beispiel in Spanisch verwenden wir W\u00f6rter, die aus Englisch kommen und hier haben Sie einige Beispiele, W\u00f6rter wie Podcast, App, Online, Crowdfunding, alle diese sind Englisch W\u00f6rter, die wir manchmal in Spanisch verwenden. Lassen Sie uns leihen, ist eine Art sprachlicher L\u00f6hne, die grunds\u00e4tzlich reproduziert, welche Sprachmuster anderer Sprachen und L\u00f6hne und Codewechsel manchmal verglichen und beschrieben wurden als eine kontinuierliche Codewechsel ist die Sache, dass bilinguale Gehen Sie dort, wo Sie zwei Sprachen gleichzeitig mischen. Es gibt jedoch einige Unterschiede zwischen lexischer Darlehen und Code Switching. Wir werden uns auf lexisches Darlehen konzentrieren. Code Switching ist etwas, das von Bilingualen und durch Definition der Code Switches nicht integriert in keiner der Sprachen in Gebrauch, w\u00e4hrend lexische Darlehen ist etwas, das auch von Monolingualen gemacht wird. Die Darlehen werden den Grammatik der Empf\u00e4ngersprache entsprechen und die Darlehen k\u00f6nnen schlie\u00dflich in die Empf\u00e4ngersprache integriert werden. Warum ist Darlehen ein interessantes Ph\u00e4nomen? Aus der Sicht der Linguistik ist das Darlehen eine Manifestation dessen, wie sich Sprachen \u00e4ndern und wie sie interagieren, und auch lexische Darlehen sind eine Quelle neuer W\u00f6rter. Hier sind einige Beispiele f\u00fcr logische Darlehen, die in die spanische Sprache als neue Worte eingebunden wurden. Lassen Sie uns ein paar Beispiele betrachten. In Bezug auf NLP sind Darlehen eine gemeinsame Quelle von W\u00f6rtern. Und in der Tat hat sich die automatische Entdeckung logischer Darlehen als n\u00fctzlich f\u00fcr NLP-Downstream-Task bewiesen, wie z. B. Parsing Text zu Sprachsynthese oder Maschinen\u00fcbersetzung. mit englischen lexikalen Darlehen verbunden, die manchmal als Englismen bezeichnet wurden. Und hier haben Sie einige Beispiele f\u00fcr die Arbeit auf der automatischen Entdeckung von Darlehen in einigen dieser Sprachen. Daher ist die Aufgabe, die wir vorschlagen, unassimilierte lexische Darlehen in der spanischen Newswire zu erkennen, was bedeutet, dass wir daran interessiert sind, W\u00f6rter aus anderen Sprachen zu extrahieren, die in den spanischen Zeitungen verwendet werden, aber die nicht verwendet wurden. integriert oder assimiliert in die Empf\u00e4ngersprache. So noch nicht in Spanisch integriert. Hier habt ihr ein Beispiel. Das ist ein Satz auf Spanisch. Und wie Sie sehen k\u00f6nnen, gibt es drei Spalten des Textes, die tats\u00e4chlich englische W\u00f6rter wie Bestseller, Tierdruck und Patchwork sind. Dies sind die Arten von Spannen, die wir daran interessiert sind, zu extrahieren und zu erkennen. Es gab fr\u00fchere Arbeiten \u00fcber die Anglikismus-Detektion, die aus einem CRF-Modell f\u00fcr die Anglikismus-Detektion auf der spanischen Newswire bestand. Dieses Modell erreichte ein F1-Score von 86, aber es gab einige Einschr\u00e4nkungen sowohl im Datensatz als auch im Modellierungsverfahren. So konzentriert sich die Datenbank ausschlie\u00dflich auf eine Quelle von Nachrichten, bestehend nur aus Titeln und es gab auch eine \u00dcberlappe in den Darlehen, die im Trainingsset und im Testsset erscheint. Dies verhindert die Beurteilung, ob die Modellierung tats\u00e4chlich auf vorher unsichtbare Darlehen generalisiert werden k\u00f6nnte. Das Ziel ist es, einige dieser Einschr\u00e4nkungen in der Aufgabe zu l\u00f6sen. Zun\u00e4chst haben wir eine neue Datensatz geschaffen. Das Ziel, ein neues Datensatz, das mit lexischen Darlehen gemeldet wurde und das Ziel war, ein Testsatz zu erstellen, das so schwierig wie m\u00f6glich war. So w\u00e4re es minimal \u00fcberlap in W\u00f6rtern und Themen zwischen dem Trainingssatz und dem Testssatz und als Ergebnis, gut, der Testssatz kommt aus Quellen und Datums, die nicht im Trainingssatz gesehen wurden. Hier k\u00f6nnen Sie sehen, dass es keine \u00dcberschwemmung in der Zeit gibt. Der Testsatz ist auch sehr verleihend dicht, nur um Ihnen einige Zahlen zu geben. Wenn der Trainingssatz sechs Darlehen pro Tausend Tokens enth\u00e4lt, enth\u00e4lt der Testssatz 20 Darlehen pro Tausend Tokens. Der Testsatz enth\u00e4lt so viele aus W\u00f6rtern wie m\u00f6glich. Tats\u00e4chlich sind 92% der Darlehen im Testset OOV, so dass sie nicht w\u00e4hrend des Trainings gesehen wurden. Und der Korpus bestand grunds\u00e4tzlich aus einer Sammlung von Texten, die aus verschiedenen Quellen spanischer Zeitungen stammen. Und es wurde von Hand geschrieben, mit zwei Tags. Eine f\u00fcr englische lexikale Darlehen, die die Mehrheit der lexikalen Darlehen in Spanisch ist, und dann die Etikette f\u00fcr Darlehen aus anderen Sprachen. Wir verwendeten GONEL-Formate und wir verwendeten BIO-Coding, damit wir einzelne Token Darlehen wie App oder Multi-Token Darlehen wie Machine Learning verschl\u00fcsseln konnten. Das sind die Zahlen des K\u00f6rpers. Wie Sie sehen k\u00f6nnen, betr\u00e4gt es etwa 370.000 Tokens und hier haben Sie die Anzahl der Spans, die als Englisch gekennzeichnet wurden und die Spans, die als andere Darlehen gekennzeichnet wurden und wie viele von ihnen waren einzigartig. Und hier haben Sie ein paar Beispiele f\u00fcr die Datenerfassung. Wie Sie sehen k\u00f6nnen, zum Beispiel, hier haben wir im ersten Beispiel haben wir die Kredit-Batch-K\u00fcche, die eine Multi-Kredit ist. Und wir haben es mit dem BIO-Code aufgezeichnet. So wurde das BIO f\u00fcr die Batch in Spanisch verwendet, also nicht f\u00fcr Worte, die nicht geliehen wurden. Und hier in diesem zweiten Beispiel haben Sie Benching und Crash, die auch als Darlehen aus Englisch gekennzeichnet werden. So wenn wir die Daten festgelegt haben, haben wir mehrere Modelle f\u00fcr die Aufgabe der Extraktion und Entdeckung dieser logischen Darlehen untersucht. Der erste, den wir versuchten, war das bedingungslose Randomfeldmodell. Dies war das Modell, das auf fr\u00fcheren Arbeiten verwendet wurde, und wir verwendeten die gleichen handgefertigten Funktionen aus diesem Arbeiten. Wie Sie sehen k\u00f6nnen, sind dies die Merkmale. Dies sind bin\u00e4re Funktionen wie das Wort oder der Token in der Spitze, ist es ein Titelfall, ist es ein Zitatzeichen, Dinge wie das, welche sind die Art von Funktionen, die man in einem Namen erwarten w\u00fcrde Aufgabe der Entit\u00e4terkennung. Das sind die Ergebnisse, die wir erhalten haben. Wir haben 55 F1-Score mit dem gleichen CRF-Modell mit handgefertigter Funktion erzielt, was einen gro\u00dfen Unterschied im Vergleich zu dem berichteten F1-Score von 86 darstellt, was das Ergebnis mit dem gleichen CRF-Modell, den gleichen Funktionen erzielt wurde. auf einem anderen Datensatz, auch f\u00fcr die spanische lexische Darlehensdetektion. Das zeigt also, dass die Daten, die wir erstellt haben, schwieriger ist und dass wir mehr sofistikierte Modelle f\u00fcr diese Aufgaben erforschen m\u00fcssen. So haben wir zwei Transformers-basierte Modelle getestet. Wir benutzten BETO, das ein monolinguales BERT-Modell ist, das f\u00fcr Spanisch und auch f\u00fcr mehrsprachige BERT trainiert wurde. Beide Modelle, die wir benutzt haben durch die Transformers-Bibliothek von Hug und Face. Das sind die Ergebnisse, die wir erhalten haben. Wie Sie sehen k\u00f6nnen, mehrsprachige BERT funktioniert besser als Beto, beide auf der Entwicklungs-Set und auf der Test-Set und \u00fcber alle Metriken. So haben wir eine Idee, das erhaltene CRF-Modell zu vergleichen und 82, das erhaltene CRF-Modell bei 55 um 55 von F1-Score zu erhalten, w\u00e4hrend die mehrsprachige BERT 82 erzielt, was ein gro\u00dfer Unterschied ist. So einmal, dass wir diese Ergebnisse haben, haben wir uns eine weitere Frage gestellt, die ist, k\u00f6nnen wir ein BIOS-STM CRF-Modell mit verschiedenen Arten von Einbindungen, Einbindungen finden, die verschiedene Arten von sprachlichen Informationen verschl\u00fcsseln und die erhaltenen Ergebnisse \u00fcberschreiten. mit Transformatorbasierten Modellen? Um dies zu tun, haben wir einige vorl\u00e4ufige Experimente durchgef\u00fchrt. Wir haben dies durch das LSTM CRF-Modell mit der Flare-Bibliothek durchgef\u00fchrt und wir haben verschiedene Arten von Einbindungen wie Transformer-basiert versucht und experimentiert, aber auch schnell Text, Charakter-Inbindungen und so weiter. Was wir herausfanden, war, dass Transformers-basierte Einbindungen besser als nicht kontextualisierte Einbindungen funktionieren, dass die Kombination von Englisch BERT und Spanisch BERO Einbindungen mehrfach bilingualisierte BERT Einbindungen \u00fcbersteigt, und dass BP Einbindungen bessere F1 und Charakter Einbindungen bessere produzieren. Denken Sie an. In diesem Sinne waren diese die besten Leistungsergebnisse, die wir erhalten haben, beide Modelle waren BiLSTM CRF-Modell mit Flare, eine wurde mit BETO und BERT-Inbeddings und BP ern\u00e4hrt, und die andere BETO BERT-Inbeddings BP, und Dar\u00fcber hinaus sind auch Charaktereinstellungen. Der letzte war der, der die h\u00f6chste F1-Score auf dem Testset erzeugt hat, obwohl die h\u00f6chste Score auf dem Entwicklungsset erreicht wurde durch den ohne Charaktereinstellungen. Nur daran zu denken, dass das beste Ergebnis, das wir mit mehrsprachigen BERT erzielt haben, eine F1 von 76 auf dem Entwicklungsset erzielt hat, und 82 auf dem Testsset. So ist dies eine Verbesserung im Vergleich zu diesen Ergebnissen. Schlie\u00dflich fragten wir uns eine weitere Frage, die war, kann die lexische Darlehen-Detektion als \u00dcbertragungslehrung von Sprachidentifikation in Code-Schalter eingerichtet werden? So f\u00fchren wir das gleiche BLSTM CRF-Modell, das wir mit FLIR ausgef\u00fchrt haben, aber anstatt diese nicht angepassten Transformer-basierten BETO und BERT-Inbeddings zu verwenden, verwenden wir Code Switching-Inbeddings. Was sind Code Switching Embeddings? Nun, diese sind die Einbindungen, die gut geschm\u00fcckte Transformer-basierte Einbindungen waren, die f\u00fcr die Sprachidentifikation im spanisch-englischen Abschnitt vorbereitet wurden. Linthe ist ein Datensatz auf Code das einen Abschnitt auf Spanisch-Englisch-Code-Schalter hat. So f\u00fcttern wir unser BiLSTM CRF mit Code Switch-Inbeddings und optional Character-Inbeddings, BPR-Inbeddings und so weiter. Das beste Ergebnis, das wir erhalten haben, war 84.22, das ist das h\u00f6chste unter allen Modellen, die wir versucht haben. Bei der Pr\u00fcfung, obwohl die besten Ergebnisse F1 Score, die wir auf der Entwicklungs-Set, die war 79, war niedriger als das beste Ergebnis erzielt durch die BLSTM CRF mit nicht angepassten Einbindungen gef\u00fcllt. Einige Schlussfolgerungen aus unserer Arbeit. Wir haben ein neues Datensatz von spanischen Nachrichtenleitungen produziert, die mit unassimilierten lexikalen Darlehen gemeldet wird. Dieser Datensatz ist mehr Darlehen Dichte und OOV reich als vorherige Ressourcen. Wir haben vier Arten von Modellen f\u00fcr die logische Darlehensdetektion im Hinblick auf Fehleranalyse untersucht. Erinnerung war ein Schwachpunkt f\u00fcr alle Modelle. Wie Sie hier sehen k\u00f6nnen, beinhalten einige h\u00e4ufige falsche Negativen hochgelassene Darlehen, W\u00f6rter, die sowohl in Englisch als auch Spanisch existieren, zum Beispiel. Interessanterweise auch BP-Inbeddings scheinen F1-Score zu verbessern und CharakterInbeddings scheinen die Erinnerung zu verbessern, was es eine interessante Entdeckung ist, die sicherlich wir auf zuk\u00fcnftigen Arbeiten untersuchen k\u00f6nnen. Nun, das ist alles, was ich habe. Danke so viel f\u00fcr das H\u00f6ren.", "delays": [5000.0, 5000.0, 5000.0, 5500.0, 5500.0, 5500.0, 6000.0, 8000.0, 8000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 18000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19000.0, 19500.0, 19500.0, 20500.0, 20500.0, 21500.0, 22000.0, 22000.0, 22000.0, 22500.0, 22500.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 24000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 30000.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 41500.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 42000.0, 46000.0, 46000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 48000.0, 55000.0, 55000.0, 55000.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 58500.0, 59000.0, 59000.0, 60500.0, 60500.0, 61000.0, 61000.0, 62500.0, 63000.0, 63000.0, 64500.0, 64500.0, 64500.0, 65500.0, 66500.0, 66500.0, 66500.0, 66500.0, 67000.0, 67000.0, 68000.0, 68000.0, 68000.0, 68000.0, 68000.0, 68500.0, 69000.0, 72000.0, 72000.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 73500.0, 74500.0, 75500.0, 75500.0, 75500.0, 75500.0, 75500.0, 75500.0, 77500.0, 80000.0, 80000.0, 80000.0, 80000.0, 80500.0, 80500.0, 80500.0, 81000.0, 81000.0, 81000.0, 81500.0, 82000.0, 82500.0, 83500.0, 85000.0, 85000.0, 85000.0, 87500.0, 87500.0, 87500.0, 88000.0, 88000.0, 88500.0, 88500.0, 89000.0, 89000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 93500.0, 93500.0, 93500.0, 95000.0, 95000.0, 96000.0, 96000.0, 96000.0, 97000.0, 97000.0, 97000.0, 97000.0, 98500.0, 98500.0, 102500.0, 102500.0, 102500.0, 102500.0, 105000.0, 105000.0, 105000.0, 105000.0, 105000.0, 105000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 112500.0, 112500.0, 112500.0, 112500.0, 112500.0, 112500.0, 112500.0, 112500.0, 113000.0, 113000.0, 113000.0, 114000.0, 114000.0, 114000.0, 114000.0, 114000.0, 115000.0, 115000.0, 116000.0, 117000.0, 117000.0, 117000.0, 117000.0, 117500.0, 118000.0, 118000.0, 119000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 122000.0, 122000.0, 122000.0, 132000.0, 132000.0, 132000.0, 132000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 133000.0, 134500.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 135000.0, 136000.0, 136000.0, 136000.0, 136000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 148000.0, 148500.0, 148500.0, 148500.0, 148500.0, 148500.0, 149000.0, 151500.0, 151500.0, 151500.0, 151500.0, 153000.0, 153000.0, 153000.0, 153000.0, 153000.0, 153000.0, 153500.0, 154000.0, 154000.0, 154000.0, 154500.0, 154500.0, 155000.0, 155000.0, 155000.0, 155500.0, 156000.0, 163500.0, 163500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 165500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 169500.0, 170500.0, 170500.0, 171500.0, 172000.0, 172000.0, 172000.0, 172000.0, 174000.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 175000.0, 175000.0, 175000.0, 176000.0, 176000.0, 176500.0, 177000.0, 178500.0, 178500.0, 179500.0, 181000.0, 181000.0, 181000.0, 181000.0, 181000.0, 183000.0, 183000.0, 183000.0, 183000.0, 183000.0, 189500.0, 189500.0, 189500.0, 189500.0, 189500.0, 189500.0, 191000.0, 191000.0, 191000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 193500.0, 193500.0, 193500.0, 193500.0, 193500.0, 195500.0, 195500.0, 195500.0, 196500.0, 199500.0, 199500.0, 199500.0, 199500.0, 200500.0, 200500.0, 200500.0, 200500.0, 200500.0, 200500.0, 200500.0, 200500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 207500.0, 209000.0, 209000.0, 209000.0, 209000.0, 209000.0, 209000.0, 209000.0, 209000.0, 209500.0, 210000.0, 210500.0, 211000.0, 211500.0, 211500.0, 213000.0, 213000.0, 213000.0, 213000.0, 213000.0, 214500.0, 214500.0, 216000.0, 216000.0, 216000.0, 217500.0, 217500.0, 217500.0, 217500.0, 219500.0, 219500.0, 220000.0, 220000.0, 220500.0, 220500.0, 220500.0, 220500.0, 220500.0, 221000.0, 222500.0, 223500.0, 223500.0, 223500.0, 224000.0, 224000.0, 224500.0, 225000.0, 225000.0, 227000.0, 227000.0, 227000.0, 227000.0, 227000.0, 228000.0, 228000.0, 229000.0, 229000.0, 229000.0, 229000.0, 231000.0, 231000.0, 231000.0, 231000.0, 231000.0, 232000.0, 232000.0, 233000.0, 233000.0, 233000.0, 233000.0, 234500.0, 234500.0, 234500.0, 235500.0, 235500.0, 236000.0, 236500.0, 237500.0, 239500.0, 239500.0, 239500.0, 239500.0, 239500.0, 239500.0, 239500.0, 244500.0, 244500.0, 244500.0, 244500.0, 244500.0, 244500.0, 245000.0, 248000.0, 248000.0, 248000.0, 248000.0, 248000.0, 249500.0, 249500.0, 249500.0, 249500.0, 249500.0, 253000.0, 253000.0, 254000.0, 254000.0, 254500.0, 254500.0, 254500.0, 254500.0, 255500.0, 256000.0, 256000.0, 257000.0, 257500.0, 257500.0, 257500.0, 257500.0, 257500.0, 257500.0, 258000.0, 258000.0, 258500.0, 258500.0, 259500.0, 259500.0, 260000.0, 260000.0, 260000.0, 261500.0, 263500.0, 263500.0, 263500.0, 264000.0, 264000.0, 264000.0, 264000.0, 264000.0, 265000.0, 265000.0, 265000.0, 265500.0, 267000.0, 268000.0, 268000.0, 268000.0, 268500.0, 268500.0, 268500.0, 268500.0, 268500.0, 268500.0, 269000.0, 269000.0, 272500.0, 272500.0, 272500.0, 272500.0, 272500.0, 272500.0, 272500.0, 274500.0, 274500.0, 274500.0, 274500.0, 274500.0, 274500.0, 274500.0, 278000.0, 278000.0, 278000.0, 278000.0, 278000.0, 280000.0, 280000.0, 280000.0, 280000.0, 280000.0, 282000.0, 282000.0, 282000.0, 282000.0, 282000.0, 282000.0, 282000.0, 282000.0, 282000.0, 283500.0, 283500.0, 283500.0, 284000.0, 284000.0, 284500.0, 286000.0, 286500.0, 286500.0, 288000.0, 288000.0, 288000.0, 288000.0, 289000.0, 289000.0, 291000.0, 291000.0, 292000.0, 292500.0, 292500.0, 296000.0, 296000.0, 296000.0, 296000.0, 296000.0, 296000.0, 299000.0, 299000.0, 299000.0, 299000.0, 299000.0, 299000.0, 299000.0, 299500.0, 299500.0, 299500.0, 299500.0, 300000.0, 301500.0, 301500.0, 301500.0, 301500.0, 301500.0, 302500.0, 303000.0, 303000.0, 303500.0, 304500.0, 304500.0, 304500.0, 304500.0, 304500.0, 305000.0, 305000.0, 305500.0, 306500.0, 307500.0, 307500.0, 307500.0, 307500.0, 310000.0, 310000.0, 311000.0, 311000.0, 312000.0, 312000.0, 312000.0, 312000.0, 312000.0, 315500.0, 315500.0, 315500.0, 315500.0, 315500.0, 315500.0, 316500.0, 316500.0, 316500.0, 317000.0, 317500.0, 319500.0, 320500.0, 320500.0, 320500.0, 320500.0, 324000.0, 324000.0, 324000.0, 324000.0, 324000.0, 324000.0, 324000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328500.0, 328500.0, 328500.0, 331000.0, 332500.0, 332500.0, 332500.0, 332500.0, 332500.0, 332500.0, 332500.0, 332500.0, 335500.0, 335500.0, 335500.0, 335500.0, 336000.0, 336000.0, 336000.0, 336000.0, 336000.0, 336000.0, 338000.0, 338000.0, 339000.0, 339000.0, 339000.0, 339500.0, 342500.0, 344000.0, 346000.0, 346000.0, 346000.0, 346000.0, 346000.0, 346000.0, 346000.0, 346000.0, 347000.0, 347000.0, 347000.0, 347500.0, 348000.0, 348000.0, 348000.0, 348000.0, 348000.0, 348000.0, 348500.0, 349500.0, 350000.0, 350000.0, 350000.0, 350000.0, 350000.0, 350500.0, 351000.0, 351000.0, 355000.0, 355000.0, 357500.0, 357500.0, 357500.0, 357500.0, 357500.0, 357500.0, 357500.0, 357500.0, 357500.0, 358000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359000.0, 359500.0, 360000.0, 360000.0, 361000.0, 361000.0, 361500.0, 363000.0, 363000.0, 364000.0, 364000.0, 364000.0, 364500.0, 366500.0, 366500.0, 371500.0, 371500.0, 371500.0, 371500.0, 371500.0, 371500.0, 371500.0, 371500.0, 373500.0, 373500.0, 373500.0, 373500.0, 373500.0, 373500.0, 373500.0, 374500.0, 375500.0, 375500.0, 375500.0, 375500.0, 375500.0, 376000.0, 376000.0, 377500.0, 377500.0, 377500.0, 378500.0, 378500.0, 378500.0, 378500.0, 379000.0, 379500.0, 379500.0, 379500.0, 380000.0, 381000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 385500.0, 386500.0, 386500.0, 387000.0, 387000.0, 388000.0, 388000.0, 388500.0, 388500.0, 389000.0, 389500.0, 390000.0, 390000.0, 391000.0, 391000.0, 391500.0, 392000.0, 392500.0, 393000.0, 396000.0, 396000.0, 396000.0, 400500.0, 400500.0, 400500.0, 400500.0, 400500.0, 400500.0, 400500.0, 400500.0, 400500.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405000.0, 405500.0, 405500.0, 405500.0, 406500.0, 408000.0, 408000.0, 408000.0, 410000.0, 410000.0, 410000.0, 410000.0, 410000.0, 410000.0, 410000.0, 410000.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 413500.0, 416500.0, 416500.0, 416500.0, 417000.0, 418500.0, 418500.0, 418500.0, 418500.0, 418500.0, 418500.0, 418500.0, 418500.0, 419000.0, 419500.0, 419500.0, 419500.0, 419500.0, 419500.0, 419500.0, 419500.0, 420000.0, 421000.0, 421000.0, 421000.0, 421000.0, 421000.0, 425000.0, 425000.0, 425000.0, 426500.0, 426500.0, 426500.0, 426500.0, 426500.0, 426500.0, 426500.0, 426500.0, 428500.0, 429500.0, 430500.0, 433500.0, 433500.0, 433500.0, 433500.0, 434500.0, 435000.0, 435500.0, 435500.0, 435500.0, 438000.0, 439000.0, 439000.0, 439000.0, 439000.0, 439000.0, 439000.0, 440500.0, 440500.0, 440500.0, 442500.0, 443500.0, 443500.0, 443500.0, 443500.0, 443500.0, 444000.0, 444000.0, 444000.0, 446000.0, 446000.0, 446000.0, 446000.0, 446000.0, 446000.0, 450000.0, 450500.0, 450500.0, 450500.0, 450500.0, 450500.0, 452000.0, 452000.0, 452000.0, 452000.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 454500.0, 455500.0, 456000.0, 456000.0, 456000.0, 459500.0, 460000.0, 461000.0, 461000.0, 465000.0, 465000.0, 465000.0, 465000.0, 465000.0, 468000.0, 468000.0, 468000.0, 468000.0, 468000.0, 468000.0, 468000.0, 469500.0, 469500.0, 470000.0, 470000.0, 470500.0, 470500.0, 470500.0, 471000.0, 471000.0, 471000.0, 471500.0, 472000.0, 473000.0, 473000.0, 473000.0, 473000.0, 473000.0, 473000.0, 474000.0, 474500.0, 474500.0, 474500.0, 474500.0, 475000.0, 476000.0, 476000.0, 477000.0, 477000.0, 477000.0, 477000.0, 477000.0, 478500.0, 478500.0, 478500.0, 478500.0, 478500.0, 478500.0, 478500.0, 478500.0, 479500.0, 480000.0, 480000.0, 480000.0, 482000.0, 482000.0, 483500.0, 483500.0, 483500.0, 484000.0, 484000.0, 484000.0, 484000.0, 485000.0, 485000.0, 486000.0, 490500.0, 490500.0, 490500.0, 490500.0, 490500.0, 490500.0, 491500.0, 491500.0, 491500.0, 491500.0, 493500.0, 494500.0, 495000.0, 495000.0, 495000.0, 495000.0, 497500.0, 498000.0, 498000.0, 498000.0, 498500.0, 500000.0, 500000.0, 501000.0, 501000.0, 501000.0, 501500.0, 501500.0, 502000.0, 502000.0, 502000.0, 502000.0, 502500.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 506000.0, 506500.0, 506500.0, 507000.0, 510000.0, 510000.0, 511000.0, 511000.0, 511000.0, 511000.0, 511000.0, 511000.0, 511000.0, 511000.0, 511500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 523500.0, 524500.0, 524500.0, 524500.0, 524500.0, 524500.0, 524500.0, 526500.0, 526500.0, 526500.0, 528000.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 529500.0, 531500.0, 532500.0, 533000.0, 533000.0, 533500.0, 533500.0, 535000.0, 535500.0, 535500.0, 535500.0, 536000.0, 536000.0, 536000.0, 536000.0, 539000.0, 539000.0, 539000.0, 539000.0, 539000.0, 539500.0, 539500.0, 539500.0, 540500.0, 540500.0, 540500.0, 541000.0, 541500.0, 543500.0, 543500.0, 543500.0, 543500.0, 544500.0, 544500.0, 545000.0, 545500.0, 545500.0, 546500.0, 547500.0, 547500.0, 549000.0, 549000.0, 549000.0, 549000.0, 549000.0, 549500.0, 549500.0, 549500.0, 550500.0, 551000.0, 551000.0, 551000.0, 551000.0, 551500.0, 552000.0, 553000.0, 555000.0, 555000.0, 556000.0, 556000.0, 556000.0, 556000.0, 557000.0, 557000.0, 559500.0, 559500.0, 559500.0, 559500.0, 560000.0, 561000.0, 561000.0, 561500.0, 561500.0, 561500.0, 562500.0, 562500.0, 562500.0, 562500.0, 562500.0, 563000.0, 564000.0, 566000.0, 566000.0, 566000.0, 566500.0, 566500.0, 567000.0, 567500.0, 570000.0, 572000.0, 572000.0, 572000.0, 572500.0, 572500.0, 572500.0, 573500.0, 574500.0, 574500.0, 575000.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 576000.0, 577500.0, 577500.0, 577500.0, 578500.0, 578500.0, 578500.0, 578500.0, 578500.0, 581000.0, 581000.0, 581000.0, 581000.0, 581000.0, 581500.0, 582000.0, 583000.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 584500.0, 585500.0, 586000.0, 586000.0, 586000.0, 586500.0, 587000.0, 587000.0, 587500.0, 592000.0, 592000.0, 592000.0, 593000.0, 593000.0, 593000.0, 593000.0, 593000.0, 594000.0, 594500.0, 594500.0, 594500.0, 594500.0, 595000.0, 595500.0, 596000.0, 596500.0, 596500.0, 597000.0, 597000.0, 598000.0, 598500.0, 598500.0, 599000.0, 599000.0, 599000.0, 599000.0, 599500.0, 600000.0, 600000.0, 600000.0, 600000.0, 601000.0, 601000.0, 602000.0, 602000.0, 603000.0, 603000.0, 603000.0, 603000.0, 604000.0, 604000.0, 605500.0, 606000.0, 606000.0, 606000.0, 607500.0, 607500.0, 607500.0, 608000.0, 609000.0, 609000.0, 609500.0, 609500.0, 609500.0, 610000.0, 614500.0, 614500.0, 614500.0, 614500.0, 614500.0, 614500.0, 614500.0, 616000.0, 616500.0, 616500.0, 616500.0, 616500.0, 616500.0, 618000.0, 618000.0, 618000.0, 620500.0, 620500.0, 620500.0, 620500.0, 620500.0, 620500.0, 621500.0, 623000.0, 627000.0, 627000.0, 627000.0, 627000.0, 627000.0, 627000.0, 627000.0, 627000.0, 627000.0, 628000.0, 628000.0, 629000.0, 629000.0, 629000.0, 629000.0, 629000.0, 631000.0, 631000.0, 631500.0, 632500.0, 632500.0, 634000.0, 635500.0, 637000.0, 637000.0, 637000.0, 637000.0, 637000.0, 638000.0, 639500.0, 639500.0, 639500.0, 639500.0, 639500.0, 639500.0, 642000.0, 643000.0, 643000.0, 643000.0, 643000.0, 643500.0, 643500.0, 646500.0, 646500.0, 646500.0, 646500.0, 647500.0, 647500.0, 650000.0, 651000.0, 651000.0, 651500.0, 651500.0, 652000.0, 653000.0, 654500.0, 654500.0, 655000.0, 658000.0, 658000.0, 658000.0, 658000.0, 658000.0, 658000.0, 659000.0, 659500.0, 659500.0, 659500.0, 659500.0, 659500.0, 661500.0, 661500.0, 662000.0, 662000.0, 662000.0, 662500.0, 662500.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 666000.0, 666000.0, 667000.0, 667000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670000.0, 670500.0, 670500.0, 670500.0, 671000.0, 673500.0, 673500.0, 673500.0, 673500.0, 673500.0, 673500.0, 675000.0, 675000.0, 675000.0, 676000.0, 677000.0, 680500.0, 680500.0, 680500.0, 680500.0, 680500.0, 683000.0, 683000.0, 683000.0, 683000.0, 683000.0, 684000.0, 685000.0, 685500.0, 685500.0, 686000.0, 686000.0, 687000.0, 688000.0, 688000.0, 688500.0, 688500.0, 691500.0, 691500.0, 691500.0, 691500.0, 691500.0, 693000.0, 693000.0, 693000.0, 693000.0, 694000.0, 694000.0, 694000.0, 694000.0, 695000.0, 695000.0, 695000.0, 695000.0, 696000.0, 696000.0, 698000.0, 698000.0, 698000.0, 699000.0, 699000.0, 704000.0, 704000.0, 704000.0, 704000.0, 704000.0, 704000.0, 704000.0, 704000.0, 709500.0, 709500.0, 709500.0, 709500.0, 709500.0, 709500.0, 709500.0, 711500.0, 711500.0, 711500.0, 711500.0, 711500.0, 712000.0, 712000.0, 712000.0, 712000.0, 712000.0, 712000.0, 712000.0, 713500.0, 713500.0, 713500.0, 713500.0, 713500.0, 713500.0, 713500.0, 715500.0, 715500.0, 715500.0, 715500.0, 718000.0, 718000.0, 719000.0, 719000.0, 721000.0, 722000.0, 722000.0, 722000.0, 722000.0, 722000.0, 723500.0, 723500.0, 723500.0, 723500.0, 724500.0, 724500.0, 726500.0, 726500.0, 727500.0, 727500.0, 728000.0, 730000.0, 730000.0, 730000.0, 730000.0, 730000.0, 730000.0, 730000.0, 731000.0, 731000.0, 731000.0, 731000.0, 731000.0, 731000.0, 731000.0, 732000.0, 732000.0, 737440.0, 737440.0, 737440.0, 737440.0], "elapsed": [7200.668096542358, 7200.668096542358, 7200.668096542358, 8111.8409633636475, 8111.8409633636475, 8111.8409633636475, 13469.668626785278, 18394.726276397705, 18394.726276397705, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 40684.6661567688, 43807.56139755249, 43807.56139755249, 43807.56139755249, 43807.56139755249, 43807.56139755249, 43807.56139755249, 43807.56139755249, 45263.20266723633, 45263.20266723633, 51205.26456832886, 51205.26456832886, 54002.16197967529, 55505.5718421936, 55505.5718421936, 55505.5718421936, 57063.13061714172, 57063.13061714172, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 64806.5881729126, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 75313.87829780579, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 104253.20744514465, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 106193.0685043335, 118448.71068000793, 118448.71068000793, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 128661.17119789124, 140699.41592216492, 140699.41592216492, 140699.41592216492, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 147951.81512832642, 149059.42225456238, 149059.42225456238, 152256.12235069275, 152256.12235069275, 153366.70422554016, 153366.70422554016, 156651.09419822693, 157785.2737903595, 157785.2737903595, 161046.0605621338, 161046.0605621338, 161046.0605621338, 163315.57297706604, 165899.0979194641, 165899.0979194641, 165899.0979194641, 165899.0979194641, 167127.72297859192, 167127.72297859192, 169777.47249603271, 169777.47249603271, 169777.47249603271, 169777.47249603271, 169777.47249603271, 171098.54245185852, 172455.03902435303, 183336.89832687378, 183336.89832687378, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 186077.72254943848, 188027.67038345337, 189910.05611419678, 189910.05611419678, 189910.05611419678, 189910.05611419678, 189910.05611419678, 189910.05611419678, 193853.2269001007, 198837.47792243958, 198837.47792243958, 198837.47792243958, 198837.47792243958, 202216.52007102966, 202216.52007102966, 202216.52007102966, 205450.75488090515, 205450.75488090515, 205450.75488090515, 206556.40363693237, 207690.0451183319, 208829.4837474823, 210990.0758266449, 214326.18141174316, 214326.18141174316, 214326.18141174316, 220496.66452407837, 220496.66452407837, 220496.66452407837, 221801.8560409546, 221801.8560409546, 223145.2820301056, 223145.2820301056, 224487.94150352478, 224487.94150352478, 232028.06067466736, 232028.06067466736, 232028.06067466736, 232028.06067466736, 232028.06067466736, 236214.54644203186, 236214.54644203186, 236214.54644203186, 240752.11906433105, 240752.11906433105, 244017.19284057617, 244017.19284057617, 244017.19284057617, 245614.70103263855, 245614.70103263855, 245614.70103263855, 245614.70103263855, 248330.1169872284, 248330.1169872284, 254625.63371658325, 254625.63371658325, 254625.63371658325, 254625.63371658325, 259320.2612400055, 259320.2612400055, 259320.2612400055, 259320.2612400055, 259320.2612400055, 259320.2612400055, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 269398.7431526184, 279040.2777194977, 279040.2777194977, 279040.2777194977, 279040.2777194977, 279040.2777194977, 279040.2777194977, 279040.2777194977, 279040.2777194977, 280294.10123825073, 280294.10123825073, 280294.10123825073, 282832.19051361084, 282832.19051361084, 282832.19051361084, 282832.19051361084, 282832.19051361084, 285230.18646240234, 285230.18646240234, 287784.36493873596, 290478.29842567444, 290478.29842567444, 290478.29842567444, 290478.29842567444, 291820.13177871704, 293199.09048080444, 293199.09048080444, 295935.6257915497, 298863.4281158447, 298863.4281158447, 298863.4281158447, 298863.4281158447, 298863.4281158447, 298863.4281158447, 298863.4281158447, 298863.4281158447, 304963.0982875824, 304963.0982875824, 304963.0982875824, 321620.1539039612, 321620.1539039612, 321620.1539039612, 321620.1539039612, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 324110.27002334595, 328186.99502944946, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 329820.14083862305, 332744.7040081024, 332744.7040081024, 332744.7040081024, 332744.7040081024, 358078.0165195465, 358078.0165195465, 358078.0165195465, 358078.0165195465, 358078.0165195465, 358078.0165195465, 364255.779504776, 365226.4566421509, 365226.4566421509, 365226.4566421509, 365226.4566421509, 365226.4566421509, 366101.455450058, 370447.49879837036, 370447.49879837036, 370447.49879837036, 370447.49879837036, 373343.3334827423, 373343.3334827423, 373343.3334827423, 373343.3334827423, 373343.3334827423, 373343.3334827423, 374372.04575538635, 375411.9474887848, 375411.9474887848, 375411.9474887848, 376502.74777412415, 376502.74777412415, 377572.34811782837, 377572.34811782837, 377572.34811782837, 378602.2415161133, 379650.28405189514, 398582.27801322937, 398582.27801322937, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 404373.45242500305, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 418201.1148929596, 419807.0156574249, 419807.0156574249, 421505.99360466003, 422564.92257118225, 422564.92257118225, 422564.92257118225, 422564.92257118225, 426047.3322868347, 427210.98613739014, 427210.98613739014, 427210.98613739014, 427210.98613739014, 427210.98613739014, 427210.98613739014, 427210.98613739014, 428274.48558807373, 428274.48558807373, 428274.48558807373, 433316.388130188, 433316.388130188, 434333.3818912506, 435361.4139556885, 438296.2498664856, 438296.2498664856, 440584.31100845337, 443828.0875682831, 443828.0875682831, 443828.0875682831, 443828.0875682831, 443828.0875682831, 448225.39138793945, 448225.39138793945, 448225.39138793945, 448225.39138793945, 448225.39138793945, 470396.609544754, 470396.609544754, 470396.609544754, 470396.609544754, 470396.609544754, 470396.609544754, 474412.0156764984, 474412.0156764984, 474412.0156764984, 477158.29157829285, 477158.29157829285, 477158.29157829285, 477158.29157829285, 477158.29157829285, 479373.9824295044, 479373.9824295044, 479373.9824295044, 479373.9824295044, 479373.9824295044, 482639.2149925232, 482639.2149925232, 482639.2149925232, 484532.9341888428, 489688.0648136139, 489688.0648136139, 489688.0648136139, 489688.0648136139, 491869.61555480957, 491869.61555480957, 491869.61555480957, 491869.61555480957, 491869.61555480957, 491869.61555480957, 491869.61555480957, 491869.61555480957, 505499.93658065796, 505499.93658065796, 505499.93658065796, 505499.93658065796, 505499.93658065796, 505499.93658065796, 505499.93658065796, 505499.93658065796, 509232.6364517212, 509232.6364517212, 509232.6364517212, 509232.6364517212, 509232.6364517212, 509232.6364517212, 509232.6364517212, 509232.6364517212, 510367.37728118896, 511582.6075077057, 515269.9522972107, 518125.301361084, 519820.5668926239, 519820.5668926239, 523415.49825668335, 523415.49825668335, 523415.49825668335, 523415.49825668335, 523415.49825668335, 527266.3803100586, 527266.3803100586, 531454.7109603882, 531454.7109603882, 531454.7109603882, 533594.0203666687, 533594.0203666687, 533594.0203666687, 533594.0203666687, 536621.8016147614, 536621.8016147614, 537594.9015617371, 537594.9015617371, 538511.593580246, 538511.593580246, 538511.593580246, 538511.593580246, 538511.593580246, 539398.4527587891, 541941.2562847137, 543993.7129020691, 543993.7129020691, 543993.7129020691, 544972.4361896515, 544972.4361896515, 545942.3677921295, 546923.451423645, 546923.451423645, 550967.4065113068, 550967.4065113068, 550967.4065113068, 550967.4065113068, 550967.4065113068, 553134.4723701477, 553134.4723701477, 555380.5487155914, 555380.5487155914, 555380.5487155914, 555380.5487155914, 560380.7377815247, 560380.7377815247, 560380.7377815247, 560380.7377815247, 560380.7377815247, 563006.0865879059, 563006.0865879059, 565562.869310379, 565562.869310379, 565562.869310379, 565562.869310379, 569638.7481689453, 569638.7481689453, 569638.7481689453, 576635.5969905853, 576635.5969905853, 578083.1224918365, 579529.7400951385, 582470.7291126251, 593433.6311817169, 593433.6311817169, 593433.6311817169, 593433.6311817169, 593433.6311817169, 593433.6311817169, 593433.6311817169, 602383.4781646729, 602383.4781646729, 602383.4781646729, 602383.4781646729, 602383.4781646729, 602383.4781646729, 603326.4453411102, 608700.4311084747, 608700.4311084747, 608700.4311084747, 608700.4311084747, 608700.4311084747, 611891.0195827484, 611891.0195827484, 611891.0195827484, 611891.0195827484, 611891.0195827484, 618814.0294551849, 618814.0294551849, 621489.6955490112, 621489.6955490112, 622874.4020462036, 622874.4020462036, 622874.4020462036, 622874.4020462036, 625407.7551364899, 626764.5890712738, 626764.5890712738, 629665.7426357269, 631190.1097297668, 631190.1097297668, 631190.1097297668, 631190.1097297668, 631190.1097297668, 631190.1097297668, 632589.371919632, 632589.371919632, 634030.3828716278, 634030.3828716278, 636957.1890830994, 636957.1890830994, 638385.0202560425, 638385.0202560425, 638385.0202560425, 642505.8674812317, 651324.545621872, 651324.545621872, 651324.545621872, 654243.8635826111, 654243.8635826111, 654243.8635826111, 654243.8635826111, 654243.8635826111, 655734.7319126129, 655734.7319126129, 655734.7319126129, 656573.3413696289, 658997.9317188263, 660877.9897689819, 660877.9897689819, 660877.9897689819, 662057.272195816, 662057.272195816, 662057.272195816, 662057.272195816, 662057.272195816, 662057.272195816, 663120.751619339, 663120.751619339, 674981.7922115326, 674981.7922115326, 674981.7922115326, 674981.7922115326, 674981.7922115326, 674981.7922115326, 674981.7922115326, 680078.2165527344, 680078.2165527344, 680078.2165527344, 680078.2165527344, 680078.2165527344, 680078.2165527344, 680078.2165527344, 688605.7722568512, 688605.7722568512, 688605.7722568512, 688605.7722568512, 688605.7722568512, 694207.5197696686, 694207.5197696686, 694207.5197696686, 694207.5197696686, 694207.5197696686, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 699799.5367050171, 704379.1110515594, 704379.1110515594, 704379.1110515594, 706011.6453170776, 706011.6453170776, 707532.0703983307, 712110.97240448, 713817.8267478943, 713817.8267478943, 718812.4661445618, 718812.4661445618, 718812.4661445618, 718812.4661445618, 720373.7668991089, 720373.7668991089, 726857.4051856995, 726857.4051856995, 728925.487279892, 729916.6493415833, 729916.6493415833, 736417.1049594879, 736417.1049594879, 736417.1049594879, 736417.1049594879, 736417.1049594879, 736417.1049594879, 743602.7913093567, 743602.7913093567, 743602.7913093567, 743602.7913093567, 743602.7913093567, 743602.7913093567, 743602.7913093567, 744900.4771709442, 744900.4771709442, 744900.4771709442, 744900.4771709442, 746097.5682735443, 749620.9535598755, 749620.9535598755, 749620.9535598755, 749620.9535598755, 749620.9535598755, 752158.9589118958, 753566.6418075562, 753566.6418075562, 755002.0642280579, 757947.6389884949, 757947.6389884949, 757947.6389884949, 757947.6389884949, 757947.6389884949, 759388.7341022491, 759388.7341022491, 760806.4892292023, 763595.7441329956, 770282.8054428101, 770282.8054428101, 770282.8054428101, 770282.8054428101, 780133.5582733154, 780133.5582733154, 787144.0889835358, 787144.0889835358, 790260.0643634796, 790260.0643634796, 790260.0643634796, 790260.0643634796, 790260.0643634796, 796163.7258529663, 796163.7258529663, 796163.7258529663, 796163.7258529663, 796163.7258529663, 796163.7258529663, 797989.2213344574, 797989.2213344574, 797989.2213344574, 799017.6801681519, 800096.9474315643, 804055.0503730774, 808736.0234260559, 808736.0234260559, 808736.0234260559, 808736.0234260559, 817670.8846092224, 817670.8846092224, 817670.8846092224, 817670.8846092224, 817670.8846092224, 817670.8846092224, 817670.8846092224, 829349.2848873138, 829349.2848873138, 829349.2848873138, 829349.2848873138, 829349.2848873138, 829349.2848873138, 831563.0354881287, 831563.0354881287, 831563.0354881287, 838538.2742881775, 843268.862247467, 843268.862247467, 843268.862247467, 843268.862247467, 843268.862247467, 843268.862247467, 843268.862247467, 843268.862247467, 855220.2968597412, 855220.2968597412, 855220.2968597412, 855220.2968597412, 856857.3606014252, 856857.3606014252, 856857.3606014252, 856857.3606014252, 856857.3606014252, 856857.3606014252, 859919.2068576813, 859919.2068576813, 861519.3619728088, 861519.3619728088, 861519.3619728088, 862376.2414455414, 867023.8814353943, 869662.2743606567, 873433.545589447, 873433.545589447, 873433.545589447, 873433.545589447, 873433.545589447, 873433.545589447, 873433.545589447, 873433.545589447, 877798.276424408, 877798.276424408, 877798.276424408, 880217.8838253021, 881529.5536518097, 881529.5536518097, 881529.5536518097, 881529.5536518097, 881529.5536518097, 881529.5536518097, 882743.8864707947, 885191.9238567352, 886535.6485843658, 886535.6485843658, 886535.6485843658, 886535.6485843658, 886535.6485843658, 887769.6716785431, 888989.0456199646, 888989.0456199646, 898583.6062431335, 898583.6062431335, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 905364.2287254333, 906770.2083587646, 912856.8103313446, 912856.8103313446, 912856.8103313446, 912856.8103313446, 912856.8103313446, 912856.8103313446, 914319.3414211273, 915851.1500358582, 915851.1500358582, 917296.5402603149, 917296.5402603149, 918080.676317215, 920423.5661029816, 920423.5661029816, 922474.8618602753, 922474.8618602753, 922474.8618602753, 923325.3688812256, 926683.3438873291, 926683.3438873291, 941060.9180927277, 941060.9180927277, 941060.9180927277, 941060.9180927277, 941060.9180927277, 941060.9180927277, 941060.9180927277, 941060.9180927277, 945463.0446434021, 945463.0446434021, 945463.0446434021, 945463.0446434021, 945463.0446434021, 945463.0446434021, 945463.0446434021, 947666.2878990173, 949963.9248847961, 949963.9248847961, 949963.9248847961, 949963.9248847961, 949963.9248847961, 951136.6050243378, 951136.6050243378, 954553.9162158966, 954553.9162158966, 954553.9162158966, 957026.2262821198, 957026.2262821198, 957026.2262821198, 957026.2262821198, 958282.4895381927, 959573.4696388245, 959573.4696388245, 959573.4696388245, 960858.9351177216, 963438.4567737579, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 974958.4391117096, 977218.5654640198, 978851.3202667236, 978851.3202667236, 979768.1992053986, 979768.1992053986, 981472.6822376251, 981472.6822376251, 982361.2849712372, 982361.2849712372, 983276.8313884735, 984163.8963222504, 985084.2530727386, 985084.2530727386, 986945.8429813385, 986945.8429813385, 987907.5000286102, 988985.3639602661, 990072.5283622742, 991134.1290473938, 1000505.1834583282, 1000505.1834583282, 1000505.1834583282, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1011112.0450496674, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1023335.49451828, 1024923.8021373749, 1024923.8021373749, 1024923.8021373749, 1027698.9853382111, 1031908.3006381989, 1031908.3006381989, 1031908.3006381989, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1039340.9855365753, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1045728.2152175903, 1051425.8046150208, 1051425.8046150208, 1051425.8046150208, 1052715.364933014, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1056455.6958675385, 1057878.0672550201, 1059339.3821716309, 1059339.3821716309, 1059339.3821716309, 1059339.3821716309, 1059339.3821716309, 1059339.3821716309, 1059339.3821716309, 1060624.6058940887, 1063397.6695537567, 1063397.6695537567, 1063397.6695537567, 1063397.6695537567, 1063397.6695537567, 1073609.5073223114, 1073609.5073223114, 1073609.5073223114, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1077951.122045517, 1083431.440114975, 1090002.6454925537, 1092876.59907341, 1102555.2277565002, 1102555.2277565002, 1102555.2277565002, 1102555.2277565002, 1104386.486530304, 1105392.1556472778, 1106417.1850681305, 1106417.1850681305, 1106417.1850681305, 1110547.2359657288, 1112376.902103424, 1112376.902103424, 1112376.902103424, 1112376.902103424, 1112376.902103424, 1112376.902103424, 1115526.926279068, 1115526.926279068, 1115526.926279068, 1119435.108423233, 1121808.0990314484, 1121808.0990314484, 1121808.0990314484, 1121808.0990314484, 1121808.0990314484, 1123012.277841568, 1123012.277841568, 1123012.277841568, 1128073.6739635468, 1128073.6739635468, 1128073.6739635468, 1128073.6739635468, 1128073.6739635468, 1128073.6739635468, 1137616.129398346, 1139033.0719947815, 1139033.0719947815, 1139033.0719947815, 1139033.0719947815, 1139033.0719947815, 1142839.8652076721, 1142839.8652076721, 1142839.8652076721, 1142839.8652076721, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1150502.1896362305, 1156426.4676570892, 1158842.066526413, 1158842.066526413, 1158842.066526413, 1166269.0484523773, 1167155.0846099854, 1169131.5398216248, 1169131.5398216248, 1175638.5045051575, 1175638.5045051575, 1175638.5045051575, 1175638.5045051575, 1175638.5045051575, 1181895.111322403, 1181895.111322403, 1181895.111322403, 1181895.111322403, 1181895.111322403, 1181895.111322403, 1181895.111322403, 1185421.966791153, 1185421.966791153, 1186780.1849842072, 1186780.1849842072, 1188216.0289287567, 1188216.0289287567, 1188216.0289287567, 1189566.9343471527, 1189566.9343471527, 1189566.9343471527, 1190822.3097324371, 1192179.7065734863, 1194742.1226501465, 1194742.1226501465, 1194742.1226501465, 1194742.1226501465, 1194742.1226501465, 1194742.1226501465, 1197435.4479312897, 1198820.870399475, 1198820.870399475, 1198820.870399475, 1198820.870399475, 1200154.934644699, 1203029.8030376434, 1203029.8030376434, 1209563.5323524475, 1209563.5323524475, 1209563.5323524475, 1209563.5323524475, 1209563.5323524475, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1214219.985485077, 1217391.8261528015, 1219086.9784355164, 1219086.9784355164, 1219086.9784355164, 1222059.30519104, 1222059.30519104, 1224528.1565189362, 1224528.1565189362, 1224528.1565189362, 1225521.327972412, 1225521.327972412, 1225521.327972412, 1225521.327972412, 1227442.5234794617, 1227442.5234794617, 1229238.8334274292, 1237853.6376953125, 1237853.6376953125, 1237853.6376953125, 1237853.6376953125, 1237853.6376953125, 1237853.6376953125, 1240071.5148448944, 1240071.5148448944, 1240071.5148448944, 1240071.5148448944, 1244329.9527168274, 1246734.1659069061, 1247983.5777282715, 1247983.5777282715, 1247983.5777282715, 1247983.5777282715, 1253504.3182373047, 1254924.289226532, 1254924.289226532, 1254924.289226532, 1256270.8456516266, 1263340.6262397766, 1263340.6262397766, 1266455.9230804443, 1266455.9230804443, 1266455.9230804443, 1268050.397157669, 1268050.397157669, 1269437.3002052307, 1269437.3002052307, 1269437.3002052307, 1269437.3002052307, 1270783.588886261, 1278238.2004261017, 1278238.2004261017, 1278238.2004261017, 1278238.2004261017, 1278238.2004261017, 1278238.2004261017, 1278238.2004261017, 1281121.928691864, 1281933.4316253662, 1281933.4316253662, 1282738.4934425354, 1288074.0857124329, 1288074.0857124329, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1289784.0836048126, 1290644.7002887726, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1321842.4053192139, 1324834.456205368, 1324834.456205368, 1324834.456205368, 1324834.456205368, 1324834.456205368, 1324834.456205368, 1330353.1465530396, 1330353.1465530396, 1330353.1465530396, 1334643.485069275, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1337061.8207454681, 1339966.1712646484, 1341744.5042133331, 1342748.0931282043, 1342748.0931282043, 1343752.8133392334, 1343752.8133392334, 1349265.1019096375, 1351743.4213161469, 1351743.4213161469, 1351743.4213161469, 1352805.4447174072, 1352805.4447174072, 1352805.4447174072, 1352805.4447174072, 1359254.9793720245, 1359254.9793720245, 1359254.9793720245, 1359254.9793720245, 1359254.9793720245, 1360459.6252441406, 1360459.6252441406, 1360459.6252441406, 1362594.68126297, 1362594.68126297, 1362594.68126297, 1363719.7005748749, 1364897.5019454956, 1369959.1629505157, 1369959.1629505157, 1369959.1629505157, 1369959.1629505157, 1381035.6619358063, 1381035.6619358063, 1382515.6123638153, 1384017.9829597473, 1384017.9829597473, 1386842.3318862915, 1389790.5070781708, 1389790.5070781708, 1394518.5594558716, 1394518.5594558716, 1394518.5594558716, 1394518.5594558716, 1394518.5594558716, 1396069.1390037537, 1396069.1390037537, 1396069.1390037537, 1399218.3151245117, 1404255.550146103, 1404255.550146103, 1404255.550146103, 1404255.550146103, 1405934.1683387756, 1407597.0017910004, 1409133.5191726685, 1412418.9875125885, 1412418.9875125885, 1414202.63838768, 1414202.63838768, 1414202.63838768, 1414202.63838768, 1415957.5197696686, 1415957.5197696686, 1420670.0339317322, 1420670.0339317322, 1420670.0339317322, 1420670.0339317322, 1421725.2352237701, 1423629.8773288727, 1423629.8773288727, 1424664.2870903015, 1424664.2870903015, 1424664.2870903015, 1426874.031305313, 1426874.031305313, 1426874.031305313, 1426874.031305313, 1426874.031305313, 1427946.837902069, 1430082.5972557068, 1434588.4156227112, 1434588.4156227112, 1434588.4156227112, 1435813.5175704956, 1435813.5175704956, 1436971.6746807098, 1438138.4217739105, 1444037.2731685638, 1449141.4301395416, 1449141.4301395416, 1449141.4301395416, 1450459.8157405853, 1450459.8157405853, 1450459.8157405853, 1453184.536933899, 1458271.6808319092, 1458271.6808319092, 1460940.3545856476, 1462602.1008491516, 1462602.1008491516, 1462602.1008491516, 1462602.1008491516, 1462602.1008491516, 1462602.1008491516, 1464129.7752857208, 1466364.5360469818, 1466364.5360469818, 1466364.5360469818, 1468085.0987434387, 1468085.0987434387, 1468085.0987434387, 1468085.0987434387, 1468085.0987434387, 1472471.6517925262, 1472471.6517925262, 1472471.6517925262, 1472471.6517925262, 1472471.6517925262, 1473454.996585846, 1474488.5168075562, 1476628.663301468, 1480185.8632564545, 1480185.8632564545, 1480185.8632564545, 1480185.8632564545, 1480185.8632564545, 1480185.8632564545, 1480185.8632564545, 1482482.3253154755, 1483773.476600647, 1483773.476600647, 1483773.476600647, 1485015.3908729553, 1486178.8408756256, 1486178.8408756256, 1487571.7332363129, 1502070.3189373016, 1502070.3189373016, 1502070.3189373016, 1508833.6553573608, 1508833.6553573608, 1508833.6553573608, 1508833.6553573608, 1508833.6553573608, 1511658.281326294, 1513034.5902442932, 1513034.5902442932, 1513034.5902442932, 1513034.5902442932, 1514424.399137497, 1515966.4089679718, 1520868.2861328125, 1525795.8118915558, 1525795.8118915558, 1535253.602027893, 1535253.602027893, 1539654.616355896, 1541299.1304397583, 1541299.1304397583, 1542901.2401103973, 1542901.2401103973, 1542901.2401103973, 1542901.2401103973, 1544545.5577373505, 1546234.2791557312, 1546234.2791557312, 1546234.2791557312, 1546234.2791557312, 1547781.7771434784, 1547781.7771434784, 1549355.7062149048, 1549355.7062149048, 1550960.0937366486, 1550960.0937366486, 1550960.0937366486, 1550960.0937366486, 1552572.9429721832, 1552572.9429721832, 1555365.0257587433, 1556345.61085701, 1556345.61085701, 1556345.61085701, 1559118.1094646454, 1559118.1094646454, 1559118.1094646454, 1560210.1123332977, 1562609.9429130554, 1562609.9429130554, 1563711.6267681122, 1563711.6267681122, 1563711.6267681122, 1564764.4164562225, 1575331.54129982, 1575331.54129982, 1575331.54129982, 1575331.54129982, 1575331.54129982, 1575331.54129982, 1575331.54129982, 1579106.950521469, 1580561.0136985779, 1580561.0136985779, 1580561.0136985779, 1580561.0136985779, 1580561.0136985779, 1584797.5294589996, 1584797.5294589996, 1584797.5294589996, 1591629.4434070587, 1591629.4434070587, 1591629.4434070587, 1591629.4434070587, 1591629.4434070587, 1591629.4434070587, 1594321.38133049, 1601321.563243866, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1609108.2887649536, 1610775.6745815277, 1610775.6745815277, 1612612.5359535217, 1612612.5359535217, 1612612.5359535217, 1612612.5359535217, 1612612.5359535217, 1616141.8988704681, 1616141.8988704681, 1617088.4964466095, 1619034.0547561646, 1619034.0547561646, 1621926.307439804, 1625015.738248825, 1628902.625799179, 1628902.625799179, 1628902.625799179, 1628902.625799179, 1628902.625799179, 1631378.3149719238, 1635975.161075592, 1635975.161075592, 1635975.161075592, 1635975.161075592, 1635975.161075592, 1635975.161075592, 1642699.0625858307, 1645549.4532585144, 1645549.4532585144, 1645549.4532585144, 1645549.4532585144, 1646980.6866645813, 1646980.6866645813, 1656128.6916732788, 1656128.6916732788, 1656128.6916732788, 1656128.6916732788, 1659493.8235282898, 1659493.8235282898, 1663856.53424263, 1665436.048746109, 1665436.048746109, 1666275.61545372, 1666275.61545372, 1667101.812839508, 1668706.5856456757, 1671500.335931778, 1671500.335931778, 1672411.6785526276, 1679009.0909004211, 1679009.0909004211, 1679009.0909004211, 1679009.0909004211, 1679009.0909004211, 1679009.0909004211, 1684585.4167938232, 1685790.9138202667, 1685790.9138202667, 1685790.9138202667, 1685790.9138202667, 1685790.9138202667, 1690050.520658493, 1690050.520658493, 1691270.107269287, 1691270.107269287, 1691270.107269287, 1692440.0072097778, 1692440.0072097778, 1696298.182964325, 1696298.182964325, 1696298.182964325, 1696298.182964325, 1696298.182964325, 1696298.182964325, 1696298.182964325, 1701492.0461177826, 1701492.0461177826, 1704085.5708122253, 1704085.5708122253, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1713503.502368927, 1715121.1607456207, 1715121.1607456207, 1715121.1607456207, 1716651.950597763, 1721997.0932006836, 1721997.0932006836, 1721997.0932006836, 1721997.0932006836, 1721997.0932006836, 1721997.0932006836, 1724456.1207294464, 1724456.1207294464, 1724456.1207294464, 1726083.8434696198, 1727776.4348983765, 1734003.6342144012, 1734003.6342144012, 1734003.6342144012, 1734003.6342144012, 1734003.6342144012, 1743427.4637699127, 1743427.4637699127, 1743427.4637699127, 1743427.4637699127, 1743427.4637699127, 1746292.649269104, 1748284.1787338257, 1749370.923757553, 1749370.923757553, 1750548.1975078583, 1750548.1975078583, 1752898.0040550232, 1755430.4993152618, 1755430.4993152618, 1756729.9087047577, 1756729.9087047577, 1764225.0289916992, 1764225.0289916992, 1764225.0289916992, 1764225.0289916992, 1764225.0289916992, 1768273.8771438599, 1768273.8771438599, 1768273.8771438599, 1768273.8771438599, 1770948.2851028442, 1770948.2851028442, 1770948.2851028442, 1770948.2851028442, 1773869.6031570435, 1773869.6031570435, 1773869.6031570435, 1773869.6031570435, 1776672.4154949188, 1776672.4154949188, 1779793.2000160217, 1779793.2000160217, 1779793.2000160217, 1781444.9479579926, 1781444.9479579926, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1789837.8448486328, 1800479.9690246582, 1800479.9690246582, 1800479.9690246582, 1800479.9690246582, 1800479.9690246582, 1800479.9690246582, 1800479.9690246582, 1805409.5096588135, 1805409.5096588135, 1805409.5096588135, 1805409.5096588135, 1805409.5096588135, 1806998.5270500183, 1806998.5270500183, 1806998.5270500183, 1806998.5270500183, 1806998.5270500183, 1806998.5270500183, 1806998.5270500183, 1810905.3361415863, 1810905.3361415863, 1810905.3361415863, 1810905.3361415863, 1810905.3361415863, 1810905.3361415863, 1810905.3361415863, 1815701.4226913452, 1815701.4226913452, 1815701.4226913452, 1815701.4226913452, 1822223.0439186096, 1822223.0439186096, 1825106.8964004517, 1825106.8964004517, 1829892.395734787, 1833597.6059436798, 1833597.6059436798, 1833597.6059436798, 1833597.6059436798, 1833597.6059436798, 1837479.3269634247, 1837479.3269634247, 1837479.3269634247, 1837479.3269634247, 1839212.7857208252, 1839212.7857208252, 1842844.7263240814, 1842844.7263240814, 1844813.260793686, 1844813.260793686, 1845853.2769680023, 1849683.3908557892, 1849683.3908557892, 1849683.3908557892, 1849683.3908557892, 1849683.3908557892, 1849683.3908557892, 1849683.3908557892, 1851993.0639266968, 1851993.0639266968, 1851993.0639266968, 1851993.0639266968, 1851993.0639266968, 1851993.0639266968, 1851993.0639266968, 1854197.9715824127, 1854197.9715824127, 1865312.281074524, 1865312.281074524, 1865312.281074524, 1865312.281074524], "prediction_length": 1661, "reference": "Hallo, hier ist Elena und ich stelle nun unsere Arbeit vor: Die Erkennung nicht-assimilierter Entlehnungen im Spanischen: Ein annotierter Korpus und Ans\u00e4tze zur Modellierung. Wir werden uns also damit besch\u00e4ftigen, was die lexikalische Entlehnung ist, die von uns vorgeschlagene Aufgabe, den ver\u00f6ffentlichten Datensatz und einige untersuchte Modelle. Doch zun\u00e4chst einmal: Was ist die lexikalische Entlehnung und warum ist sie als NLP-Aufgabe so wichtig? Die lexikalische Entlehnung ist im Grunde die \u00dcbernahme von W\u00f6rtern aus einer Sprache in eine andere Sprache. Zum Beispiel verwenden wir im Spanischen W\u00f6rter, die aus dem Englischen stammen. Und hier ein paar Beispiele: W\u00f6rter wie Podcast, App und Online-Crowdfunding sind englische W\u00f6rter, die wir manchmal im Spanischen verwenden. Die lexikalische Entlehnung ist eine Art der sprachlichen Entlehnung, die im Grunde genommen die Reproduktion von Mustern einer Sprache in einer anderen Sprache bedeutet. Manchmal wurde die Entlehnung mit dem Code-Switching verglichen und als ein Kontinuum beschrieben. Code-Switching wird von Zweisprachigen praktiziert, wenn sie zwei Sprachen gleichzeitig verwenden. Es gibt jedoch einige Unterschiede zwischen lexikalischer Entlehnung und Code-Switching. Wir werden uns auf die lexikalische Entlehnung konzentrieren. Zweisprachige Personen praktizieren das sogenannte Code-Switching. Per Definition sind die Code-Switches nicht Teil der verwendeten Sprachen, w\u00e4hrend die lexikalische Entlehnung auch von einsprachigen Personen verwendet wird. Die Entlehnungen werden der Grammatik der Empf\u00e4ngersprache angepasst. Entlehnungen k\u00f6nnen Schritt f\u00fcr Schritt in die Empf\u00e4ngersprache integriert werden. Warum ist Entlehnen so ein interessantes Ph\u00e4nomen? Aus Sicht der Linguistik ist die Entlehnung eine Manifestation dessen, wie sich Sprachen ver\u00e4ndern und wie sie interagieren. Auch lexikalische Entlehnungen sind eine Quelle f\u00fcr neue W\u00f6rter. Hier finden Sie einige Beispiele f\u00fcr lexikalische Entlehnungen, die als neue W\u00f6rter in die spanische Sprache aufgenommen wurden. Beim NLP sind Entlehnungen eine h\u00e4ufige Quelle von W\u00f6rtern, die nicht im Wortschatz enthalten sind. Die automatische Erkennung lexikalischer Entlehnungen erwies sich als n\u00fctzlich f\u00fcr NLP und nachgelagerte Aufgaben wie Parsing, Text-zu-Sprache-Synthesen oder die maschinelle \u00dcbersetzung. Der Einfluss des Englischen auf andere Sprachen erf\u00e4hrt immer st\u00e4rkeres Interesse, insbesondere bei englischen lexikalischen Entlehnungen. Diese werden manchmal auch als Anglizismen bezeichnet. Hier sind einige Beispiele von Arbeiten zur automatischen Erkennung von Entlehnungen in einigen dieser Sprachen. Die Aufgabe, die wir vorschlagen, besteht also darin, nicht-assimilierte lexikalische Entlehnungen in spanischen Nachrichten zu erkennen. Wir sind daran interessiert, aus anderen Sprachen entlehnte W\u00f6rter zu extrahieren, die in spanischen Zeitungen verwendet werden, aber nicht in die Empf\u00e4ngersprache integriert oder assimiliert wurden. Sie wurden also noch nicht ins Spanische integriert. Hier ist ein Beispiel. Dies ist ein Satz auf Spanisch: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork. Wie Sie sehen k\u00f6nnen, sind hier drei Textpassagen, die eigentlich englische W\u00f6rter sind: Bestseller, Animal Print und Patchwork. Bei diesen Passagen wollen wir extrahieren und erkennen. Es gab fr\u00fcher schon Arbeiten \u00fcber die Erkennung von Anglizismen. Diese besch\u00e4ftigten sich mit einem CRF-Modell f\u00fcr die Erkennung von Anglizismen in spanischen Nachrichten. Dieses Modell erreichte einen F1-Score von 86. Es gab jedoch einige Einschr\u00e4nkungen sowohl beim Datensatz als auch beim Modellierungsansatz. Der Datensatz konzentrierte sich also ausschlie\u00dflich auf eine Quelle von den Nachrichten und bestand nur aus Schlagzeilen. Au\u00dferdem gab es \u00dcberschneidungen bei den Entlehnungen, die im Trainingssatz und im Testsatz vorkommen. Dadurch konnte nicht beurteilt werden, ob der Modellierungsansatz tats\u00e4chlich auf zuvor unbekannte Entlehnungen verallgemeinert werden kann. Unser Ziel ist es also, einige dieser Einschr\u00e4nkungen in der Aufgabe zu \u00fcberwinden. Zu Beginn haben wir also einen neuen Datensatz erstellt. Das Ziel war ein neuer Datensatz, der mit lexikalischen Entlehnungen annotiert wurde, und einen m\u00f6glichst schwierigen Testsatz zu erstellen. Es g\u00e4be also minimale \u00dcberschneidungen bei W\u00f6rtern und Themen zwischen dem Trainingssatz und dem Testsatz. Das Ergebnis ist, dass der Testsatz aus Quellen und Daten stammt, die wir nicht im Trainingssatz sehen. Hier k\u00f6nnen Sie sehen, dass es keine \u00dcberschneidungen in der Zeit gibt. Au\u00dferdem enth\u00e4lt der Testsatz auch sehr viele Entlehnungen. Um Ihnen ein paar Zahlen zu nennen: Wenn der Trainingssatz sechs Entlehnungen pro 1000 Token enth\u00e4lt, enth\u00e4lt der Testsatz 20 Entlehnungen pro 1000 Token. Der Testsatz enthielt so viele Vokabelw\u00f6rter wie m\u00f6glich. Tats\u00e4chlich sind 92 Prozent der Entlehnungen im Testsatz OOV. Sie waren also w\u00e4hrend des Trainings nicht bekannt. Der Korpus bestand im Wesentlichen aus einer Sammlung von Texten, die aus verschiedenen Quellen spanischer Zeitungen stammten. Er wurde manuell mit zwei Tags annotiert. Einer f\u00fcr englische lexikalische Entlehnungen, die den Gro\u00dfteil der lexikalischen Entlehnungen im Spanischen ausmachen, und dann das andere Label f\u00fcr Entlehnungen aus anderen Sprachen. Wir verwenden CONLL-Formate und die BIO-Kodierung, sodass wir einfache Token-Entlehnungen wie \u201eApp\u201c oder mehrteilige Token-Entlehnungen wie \u201emaschinelles Lernen\u201c kodieren k\u00f6nnen. Das sind die Nummern des Korpus. Wie Sie sehen k\u00f6nnen, handelt es sich um etwa 370 000 Token. Hier sehen Sie die Reihe an Passagen, die als Englisch markiert wurden, und die Passagen, die als andere Entlehnungen markiert waren, und wie viele davon einzigartig waren. Hier sehen Sie einige Beispiele f\u00fcr den Datensatz. Wie Sie zum Beispiel hier sehen k\u00f6nnen, haben wir im ersten Beispiel die Entlehnung \u201ebatch cooking\u201c, die eine mehrteilige Wort-Entlehnung ist. Wir haben dieses Wort mit der BIO-Kodierung annotiert. BIO wurde also f\u00fcr W\u00f6rter im Spanischen verwendet, also nicht f\u00fcr W\u00f6rter, die nicht entlehnt wurden. Hier in diesem zweiten Beispiel sehen Sie \u201ebenching\u201c und \u201ecrash\u201c, die ebenfalls als Entlehnungen aus dem Englischen markiert sind. Nachdem wir also den Datensatz hatten, untersuchten wir verschiedene Modelle f\u00fcr die Aufgabe, bei der wir lexikalische Entlehnungen extrahieren und erkennen wollten. Zuerst haben wir das bedingte Zufallsfeld Modell getestet. Das war das Modell, das bei fr\u00fcheren Arbeiten verwendet worden war. Wir haben die gleichen manuell erstellten Funktionen wie bei dieser Arbeit verwendet. Wie Sie sehen k\u00f6nnen, sind dies die Funktionen. Dies sind bin\u00e4re Funktionen, wie das Wort oder das Token in Gro\u00dfbuchstaben. Handelt es sich um einen Titel? Ist es ein Anf\u00fchrungszeichen? Solche Dinge sind die Art von Funktionen, die man bei einer Named Entity Recognition-Aufgabe erwarten w\u00fcrde. Das sind die Ergebnisse, die wir erhalten haben. Wir erhalten 55 F1-Scores, wenn wir das CRF-Modell mit manuell erstellten Funktionen verwenden. Das ist ein gro\u00dfer Unterschied im Vergleich zum bereits berichteten F1-Score von 86, der ein Ergebnis desselben CRF-Modells mit derselben Funktionen war, aber auf einen anderen Datensatz angewendet wurde, auch f\u00fcr die Erkennung von spanischen lexikalischen Entlehnungen. Das beweist also, dass der Datensatz, den wir erstellt haben, schwieriger ist und dass wir anspruchsvollere Modelle f\u00fcr diese Aufgaben entwickeln m\u00fcssen. Wir haben also zwei Transformer-basierte Modelle getestet. Wir haben BETO verwendet, ein einsprachiges BERT-Modell, das auf Spanisch trainiert ist, und auch ein mehrsprachiges BERT-Modell. Beide Modelle verwenden wir \u00fcber die Transformer-Bibliothek von HuggingFace. Das sind die Ergebnisse, die wir erhalten haben. Wie Sie sehen k\u00f6nnen, schneidet das mehrsprachige BERT sowohl im Entwicklungssatz als auch im Testsatz und bei allen Metriken besser ab als BETO. Das CRF-Modell hat 82 erreicht, nur damit wir einen Vergleich ziehen k\u00f6nnen. Das CRF-Modell erreichte einen F1-Score von 55, w\u00e4hrend das mehrsprachige BERT 82 erreichte, was ein gro\u00dfer Unterschied ist. Nachdem wir also diese Ergebnisse hatten, stellten wir uns eine weitere Frage, n\u00e4mlich: K\u00f6nnen wir ein BiLSTM-CRF-Modell finden, verschiedene Arten von Einbettungen darin einspeisen, Einbettungen, die verschiedene Arten von sprachlichen Informationen kodieren, und die Ergebnisse von Transformer-basierten Modellen \u00fcbertreffen? Daf\u00fcr haben wir einige pr\u00e4limin\u00e4re Experimente durchgef\u00fchrt, und zwar mit dem BiLSTM-CRF-Modell unter Verwendung von Flare Library. Wir haben mit verschiedenen Arten von Einbettungen experimentiert, z.\u00a0B. mit Transformer-basierten, aber auch mit Schnell-Text-Einbettungen und Zeichen-Einbettungen. Wir haben herausgefunden, dass Transformer-basierte Einbettungen besser abschneiden als nicht kontextualisierte Einbettungen, dass die Kombination aus englischer BERT- und spanischer BETO-Einbettung besser ist als mehrsprachige BERT-Einbettungen. Auch ergeben die BPE-Einbettungen ein besseres F1 und die Zeicheneinbettungen ein besseres Recall. Vor diesem Hintergrund waren dies die besten Ergebnisse, die wir erzielen konnten. Beide Modelle waren BiLSTM-CRF-Modelle unter Verwendung von Flare. Bei einem wurden BETO- und BERT-Einbettungen und BPE eingespeist, beim anderen BETO- und BERT-Einbettungen und BPE sowie Zeichen-Einbettungen. Letzteres war dasjenige, das den h\u00f6chste F1-Score beim Testsatz erzielte, obwohl der h\u00f6chste Score beim Entwicklungssatz durch das Modell ohne Zeichen-Einbettungen erreicht wurde. Vergessen Sie nicht, dass das beste Ergebnis, das wir mit mehrsprachigem BERT erzielt haben, einen F1-Wert von 76 im Entwicklungssatz und 82 im Testsatz erreichte. Dies ist also eine Verbesserung im Vergleich zu diesen Ergebnissen. Schlie\u00dflich stellten wir uns noch eine weitere Frage: Kann die Erkennung von lexikalischen Entlehnungen als Transferlernen von Sprachidentifikation beim Code-Switching formuliert werden? Wir haben also dasselbe BiLSTM-CRF-Modell wie mit Flare verwendet, aber anstelle dieser nicht angepassten Transformer-basierten BETO- und BERT-Einbettungen haben wir Code-Switch-Einbettungen verwendet. Was sind Code-Switch-Einbettungen? Dies sind Einbettungen, die auf Transformer-basierte Einbettungen abgestimmt wurden. Diese wurden f\u00fcr die Sprachidentifikation im Spanisch-Englisch-Abschnitt des LinCE-Code-Switching-Datensatzes vortrainiert. LinCE ist ein Datensatz vom Code-Switching, der einen Abschnitt mit Code-Switching von Spanisch und Englisch enth\u00e4lt. Wir speisten also Code-Switch-Einbettungen in unser BiLSTM-CRF ein. Optional k\u00f6nnen Zeicheneinbettungen, BPE-Einbettungen und so weiter eingef\u00fcgt werden. Das beste Ergebnis, das wir erzielt haben, war 84,22. Das ist das beste Ergebnis aller Modelle, die wir mit dem Testsatz ausprobiert haben. Obwohl der beste F1-Score, den wir beim Entwicklungssatz erzielt haben, 97 war, war dieser niedriger als das beste Ergebnis vom BiLSTM-CRF, das mit unangepassten Einbettungen eingespeist war. Hier sind einige Schlussfolgerungen aus unserer Arbeit. Wir haben einen neuen Datensatz mit spanischen Nachrichten erstellt, der mit nicht assimilierten lexikalischen Entlehnungen annotiert ist. Dieser Datensatz ist dichter an Entlehnungen und OOV-reicher als fr\u00fchere Ressourcen. Wir haben vier Arten von Modellen f\u00fcr die Erkennung lexikalischer Entlehnungen erforscht. Also. Was die Fehleranalyse betrifft, war der Recall ein Schwachpunkt bei allen Modellen. Wie Sie hier sehen k\u00f6nnen, geh\u00f6ren zu den h\u00e4ufigen falsch-negativen Ergebnissen beispielsweise auch Entlehnungen in Gro\u00dfbuchstaben und W\u00f6rter, die es sowohl im Englischen als auch im Spanischen gibt. Interessant ist auch, dass BPE-Einbettungen den F1-Core zu verbessern scheinen. Und die Einbettung von Zeichen scheint den Recall zu verbessern. Das ist eine interessante Erkenntnis, die wir vielleicht in k\u00fcnftigen Arbeiten untersuchen k\u00f6nnen. Also. Das w\u00e4re alles, was ich zu sagen habe. Vielen Dank f\u00fcrs Zuh\u00f6ren.", "source": ["2/acl_6060/dev/full_wavs/2022.acl-long.268.wav", "samplerate: 16000 Hz", "channels: 1", "duration: 1e+01:17.440 min", "format: WAV (Microsoft) [WAV]", "subtype: Signed 16 bit PCM [PCM_16]"], "source_length": 737440.0}
{"index": 3, "prediction": "Mein Name ist Antoine. Ich bin ein PhD-Student an der Universit\u00e4t Massachusetts Amherst. Wir pr\u00e4sentieren unser Papier, Kinyarwanda Language Model. Heute werde ich \u00fcber die Motivation f\u00fcr diese Forschung sprechen, dann pr\u00e4sentieren Kinyarwanda Architektur im Detail. Dann werde ich \u00fcber unsere experimentellen Ergebnisse sprechen, dann mit einigen Schlussfolgerungen beenden. Wir alle wissen, dass die j\u00fcngste nat\u00fcrliche Sprachverarbeitung Fortschritte m\u00f6glich gemacht wurden durch die Verwendung von vorgeschulten Sprachmodellen wie BERT. Allerdings gibt es noch eine Reihe von Beschr\u00e4nkungen. Aufgrund der komplexen Morphologie, die durch die meisten morphologisch reichen Sprachen ausgedr\u00fcckt wird, verschl\u00fcsselt das ubiquitous byte-paar das die Tokenisierung-Algorithmen verwendet wird, kann nicht die genaue subword lexische Einheiten, d.h. die Morpheme, die f\u00fcr eine wirksame Darstellung erforderlich sind, extrahieren. Zum Beispiel haben wir hier drei Kinyarwanda W\u00f6rter, die mehrere Morpheme in ihnen haben, aber die BP-Algorithmen k\u00f6nnen sie nicht extrahieren. Dies liegt daran, dass einige morphologische Regeln verschiedene Oberfl\u00e4chenformen produzieren, die die genaue lexikale Information verbergen. Und BPE, die ausschlie\u00dflich auf den Oberfl\u00e4chenformen beruht, hat keinen Zugang zu diesen lexischen Modellen. Die zweite Herausforderung ist, dass auch wenn man Zugang zu einem Oracle morphologischen Analyzer hatte, BPE-Token mit Morphemen zu ersetzen, nicht genug ist, um die morphologische Zusammensetzung auszudr\u00fccken. Eine dritte L\u00fccke in der Forschung ist, dass neue vorbereitete Sprachmodelle am h\u00e4ufigsten bewertet werden auf hohen Ressourcen Sprachen und wir m\u00fcssen ihre Anwendbarkeit auf niedrigen Ressourcen und vielf\u00e4ltigen Sprachen auch bewerten. Deshalb pr\u00e4sentieren wir KeniaBERT, das eine einfache, aber wirksame Anpassung der BERT-Architektur ist, die dazu bestimmt ist, morphologisch reiche Sprachen effizienter zu behandeln. Wir bewerten KeniaBERT auf Kenia Rwanda, eine niedrige Ressourcen morphologisch reiche Sprache, die von mehr als 12 Millionen Menschen in Ost- und Nord-Afrika gesprochen wird. Der Eintritt in das Modell ist entweder ein Satz oder ein Dokument. Zum Beispiel hier, wir haben, was bedeutet, dass wir \u00fcberrascht waren, John dort zu finden. Wie Sie sehen k\u00f6nnen, Kenia Rwanda W\u00f6rter enthalten mehrere Morpheme, die verschiedene Informationen in ihnen enthalten. Daher, in unserem Modell, \u00fcbertragen wir diese Satz oder ein Dokument an einen morphologischen Analyzer, der dann Morpheme in jedem der W\u00f6rter erzeugt. Die Morph\u00e4en werden in der Regel aus einem Stamm und Null oder mehr Affixes hergestellt. Die Affiche k\u00f6nnen Spannung, Aspekt, Gegenstand oder Objekt anzeigen. Subjekte in Verben sind h\u00e4ufiger mit der Band zu Noun-Klasse f\u00fcr Subjekte und Objekte verbunden. Der morphologische Analyzer produziert auch eine Sprachmarke f\u00fcr jedes Wort. Nach diesem Schritt machen wir Einblicke f\u00fcr den Weg der Sprachtags. Wir machen auch Einblicke f\u00fcr die Affiche und Einblicke f\u00fcr den Stamm. Das sind die morphologischen Einbindungen. Wir \u00fcbernehmen diese Einbindungen dann \u00fcber einen morphologischen Encoder, der ein kleiner Transformer Encoder ist, der auf jedes Wort unabh\u00e4ngig angewendet wird. Die Ergebnisse sind die Vektoren, die mit den morphologischen Informationen in jedem Wort kontextualisiert sind. Jetzt f\u00fchren wir eine Zusammensetzung durch, in der die morphologischen Einbindungen die einem Teil der Rede und des Stammes entsprechen, zusammengefasst werden. Wir konzentrieren sie weiter mit einem anderen Stamm auf der Satzebene und so weiter. Dann bilden wir eine Einf\u00fchrung in die Hauptsatz- oder Dokumentencoder. Die Endergebnisse sind kontextualisierte Einbindungen, die f\u00fcr den Downstream-NLP-Task verwendet werden k\u00f6nnen. F\u00fcr einen morphologischen Analysator verwenden wir den endg\u00fcltigen Zustand von zwei Ebenen mit benutzerdefinierter Implementierung, die an die Kinyarwanda-Sprache angepasst ist. Wir modellieren effektiv die Morphologie aller Kinyarwanda W\u00f6rter, einschlie\u00dflich Verbose, Nouns, Demonstrative und possessive Pronouns, Numerals, und andere. Wir verwenden eine unkontrollierte Sprachmarke mit einem Algorithmus, ein erstklassiges Faktormodell wird verwendet, um die Wahrscheinlichkeit der Morphologie zu berechnen, und grunds\u00e4tzlich die Wahrscheinlichkeit, die vom Morphologischen Analyzer zugeordnet wird. Wir nehmen auch in Betracht den Weg der Sprachmarke Pr\u00e4zedenz sowie die syntaktischen Vereinbarungen, die in der Eingabe W\u00f6rter vorhanden sind. Der Sprachtagger verwendet eine bidirektionelle Inferenz, die sich am h\u00e4ufigsten verwendeten Viterbi-Algorithmus f\u00fcr die Dekodierung verbessert. Ein paar Bemerkungen hier f\u00fcr die positionelle Codierung. Der morphologische Encoder nutzt keine positionelle Codierung. Dies liegt daran, dass jeder der Morpheme einen bekannten Schloss in der morphologischen Modelle besitzt, daher ist die positionelle Information inherent, wenn die Morpheme gegeben werden. Zweitens verwendet der Satz Encoder die sogenannten untied relative positional embeddings, die k\u00fcrzlich auf der ICLR-Konferenz ver\u00f6ffentlicht wurden. Diese positionalen Einbindungen unterscheiden im Wesentlichen die positionalen Korrelationen von Token-to-Token \u00c4hnlich wie BERT verwenden wir ein maskiertes Sprachmodell f\u00fcr das Vor-Training-Objektiv. Im Wesentlichen m\u00fcssen wir sowohl den Stamm als auch die Affiche vorhersagen, die mit den Worten verbunden sind. W\u00e4hrend der Vor-Training, 15% aller Worte gelten f\u00fcr die Vorhersage, von denen 80% maskiert, 10% mit zuf\u00e4lligen W\u00f6rtern ausgetauscht, und 10% bleibt unver\u00e4ndert. F\u00fcr die Affix-Vorhersage haben wir ein Multi-Label-Klassifizierungsproblem. Daf\u00fcr werden wir beide zusammen affixes in eine feste Anzahl von sets zusammengefasst, und vorhersagen die set als klasse-label. Die andere Option ist, den Affix-Vektor zu vorhersagen. Wir bewerten beide Ans\u00e4tze in unseren Experimenten. Wir pre-train Kenya Barrett auf etwa 2.5 GB von Kinyarwanda Text und vergleichen es mit drei Baseline-Modellen. Eine ist ein mehrsprachiges Modell namens XLMR, das auf einer gro\u00dfen Textkorpora trainiert wird, die aus mehreren Sprachen hergestellt wird. Die anderen beiden Baselinen werden auf demselben Kinyarwanda Text mit einem Byte-Paar-Algorithmus oder mit morphologischer Analyse vorbereitet, ohne die beiden Transformer-Encoder-Architektur zu verwenden. Alle Modelle sind in der Bayes-Architektur konfiguriert, die zwischen 100 und 110 Millionen Parameter mit Kinyarwanda und Kinyaberg mit der geringsten Anzahl der Parameter liegt. Alle Modelle mit Ausnahme der mehrsprachigen, sind f\u00fcr 32.000 Gradient-Updates mit einer Bandgr\u00f6\u00dfe von 2.560 Sequenzen in jedem Band vorbereitet. Wir bewerten die vorbereiteten Modelle auf drei Aufgabens\u00e4tzen. Einer ist der glatte Benchmark, der oft verwendet wurde, um die Wirksamkeit der vorgeschulten Sprachmodelle zu bewerten, die wir unsere glatte Benchmark-Daten durch \u00dcbersetzung der urspr\u00fcnglichen Benchmark-Daten in Kinyarwanda mit Google Translate erhalten. Die zweite Aufgabe ist die Kinyarwanda Name Identity Recognition Benchmark, die eine qualitativ hochwertige Datensatz ist, die von ausgebildeten indigenen Sprechern angegeben wurde. Der dritte ist eine neue Kategorisierung, in der wir Nachrichtenartikel aus mehreren Websites ziehen und ihre Kategorisierungszeichen korrigieren, die von den Autoren zugeordnet wurden und dann im Wesentlichen versuchen, die gleichen Kategorien vorzustellen. Und jetzt gehen wir zu den Ergebnissen. F\u00fcr die glatte Benchmarke finden wir, dass Kinyarwanda konsequent Baseline-Modelle \u00fcberschreitet. Hier zeigen wir die durchschnittliche Leistung f\u00fcr 10 Fine-Tuning-Rennen. Wir f\u00fchren auch eine Benutzerbewertung der \u00dcbersetzungen durch Google Translate durch. In erster Linie sch\u00e4tzten Benutzer etwa 6.000 Beispiele, die Punkte auf einer Skala von 1 bis 4 z\u00e4hlen und die Qualit\u00e4t der \u00dcbersetzung beurteilen. Das Ergebnis ist, dass viele \u00dcbersetzungen ger\u00e4umig waren, aber alle Modelle mussten mit dem gleichen \u00dcbersetzungsger\u00e4umig und der relativen Leistung zwischen den Modellen ist immer noch wichtig zu beachten. F\u00fcr die Identit\u00e4tsidentifikationsarbeit finden wir auch, dass KineoBERT die beste Leistung mit der durchschnittlichen Grafikverteilung Regulierungsvariante bietet, die am besten funktioniert. Diese Ergebnisse sind auch durchschnittlich 10 Fine-Tuning-Rennen. F\u00fcr die News Kategorisierung Aufgabe finden wir gemischte Ergebnisse. Vorheriger Beitrag Text-Klassifizierung f\u00fcr Kinyarwanda hatte festgestellt, dass die einfache Keyword-Detektion meist genug ist, um diese spezifische Aufgabe zu l\u00f6sen. Daher gibt es weniger Gewinne aus der Verwendung von vorgeschulten Sprachmodellen auf dieser speziellen Aufgabe der Nachrichtenkategorisierung. Wir haben auch eine Ablationstudium durchgef\u00fchrt, um zu sehen, ob es alternative Strukturen gibt, die die Leistung verbessern. F\u00fcr die Gruppe benchmarking finden wir, dass die Verwendung von festgelegten sets konsequent besser funktioniert, w\u00e4hrend festgelegte Wahrscheinlichkeit-Regressionsziele die beste Leistung auf der benannten Entit\u00e4terkennung erzielen. Auch, indem wir die Verluste-Kurven f\u00fcr die Fine-Tuning betrachten, finden wir, dass KineBert in den meisten F\u00e4llen eine bessere Konvergenz hat. So zu schlie\u00dfen, diese Arbeit hat die Wirksamkeit der explizit verwendeten morphologischen Informationen in vorgeschulten Sprachmodellen gezeigt. Die vorgeschlagene zweistufige Transformer-Encoderarchitektur erm\u00f6glicht die Aufnahme der morphologischen Zusammensetzung, die ein wichtiger Aspekt der morphologisch reichen Sprachen ist. Diese Ergebnisse sollten weitere Forschungen in morphologisch bewusst vorbereitete Sprachmodelle motivieren.", "delays": [3500.0, 3500.0, 3500.0, 3500.0, 5000.0, 5000.0, 5000.0, 6500.0, 6500.0, 6500.0, 6500.0, 8000.0, 8000.0, 9500.0, 9500.0, 13000.0, 13000.0, 17000.0, 17000.0, 17000.0, 18500.0, 18500.0, 18500.0, 18500.0, 18500.0, 18500.0, 19500.0, 19500.0, 19500.0, 19500.0, 19500.0, 20500.0, 20500.0, 22000.0, 22000.0, 22000.0, 23000.0, 23000.0, 23000.0, 23000.0, 23500.0, 25000.0, 25000.0, 25000.0, 25500.0, 30000.0, 30000.0, 30000.0, 30000.0, 31000.0, 31000.0, 31500.0, 31500.0, 32500.0, 33000.0, 33000.0, 33500.0, 34000.0, 35500.0, 35500.0, 36000.0, 36000.0, 37000.0, 37000.0, 37000.0, 39000.0, 39000.0, 39000.0, 39000.0, 42500.0, 42500.0, 42500.0, 42500.0, 42500.0, 42500.0, 42500.0, 42500.0, 43500.0, 44000.0, 44500.0, 45000.0, 45500.0, 46500.0, 47500.0, 47500.0, 49500.0, 49500.0, 49500.0, 49500.0, 51000.0, 53000.0, 53000.0, 53000.0, 54500.0, 55500.0, 55500.0, 55500.0, 56000.0, 56000.0, 56000.0, 56000.0, 57500.0, 57500.0, 58500.0, 59500.0, 60000.0, 60000.0, 60000.0, 61500.0, 61500.0, 63500.0, 63500.0, 63500.0, 63500.0, 63500.0, 63500.0, 63500.0, 65500.0, 66000.0, 66000.0, 66000.0, 66000.0, 66500.0, 68000.0, 69000.0, 69000.0, 69000.0, 70000.0, 70000.0, 70500.0, 70500.0, 71000.0, 71000.0, 72000.0, 74000.0, 74000.0, 74000.0, 74000.0, 76500.0, 76500.0, 76500.0, 76500.0, 76500.0, 76500.0, 77500.0, 79000.0, 80000.0, 80000.0, 80000.0, 81000.0, 81000.0, 82000.0, 82000.0, 82000.0, 84000.0, 84000.0, 84000.0, 85000.0, 85000.0, 86000.0, 86000.0, 86500.0, 87000.0, 87500.0, 87500.0, 87500.0, 90500.0, 90500.0, 90500.0, 91500.0, 91500.0, 91500.0, 92500.0, 92500.0, 93000.0, 93000.0, 93500.0, 93500.0, 94500.0, 94500.0, 97500.0, 97500.0, 97500.0, 97500.0, 98000.0, 100000.0, 100000.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 100500.0, 107000.0, 107000.0, 107000.0, 107000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 108000.0, 109000.0, 109500.0, 109500.0, 109500.0, 112000.0, 112000.0, 112000.0, 112000.0, 112500.0, 112500.0, 113000.0, 113000.0, 114000.0, 114500.0, 114500.0, 115500.0, 115500.0, 116500.0, 118500.0, 118500.0, 118500.0, 118500.0, 120000.0, 120000.0, 120000.0, 121500.0, 122000.0, 122000.0, 122500.0, 122500.0, 123000.0, 124000.0, 126000.0, 126000.0, 127000.0, 127000.0, 130000.0, 130000.0, 130000.0, 131000.0, 131000.0, 131000.0, 131000.0, 131000.0, 131000.0, 132000.0, 132000.0, 132000.0, 133000.0, 133500.0, 134000.0, 135000.0, 135000.0, 136000.0, 136000.0, 136500.0, 137000.0, 137000.0, 137500.0, 138000.0, 138000.0, 139000.0, 139000.0, 139500.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 148000.0, 148000.0, 149000.0, 149000.0, 149000.0, 149500.0, 150500.0, 150500.0, 150500.0, 151500.0, 151500.0, 151500.0, 153000.0, 153000.0, 153500.0, 160000.0, 160000.0, 160000.0, 160000.0, 161000.0, 161000.0, 161000.0, 161000.0, 161000.0, 161000.0, 161000.0, 161000.0, 162000.0, 162000.0, 162500.0, 162500.0, 164000.0, 164000.0, 164000.0, 164000.0, 165000.0, 166000.0, 166000.0, 166500.0, 167000.0, 168000.0, 168000.0, 168000.0, 170500.0, 171000.0, 171500.0, 172000.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 174500.0, 179000.0, 179000.0, 179000.0, 180000.0, 180000.0, 180000.0, 181000.0, 181500.0, 181500.0, 183500.0, 183500.0, 183500.0, 184500.0, 184500.0, 185000.0, 185500.0, 185500.0, 185500.0, 185500.0, 186000.0, 186000.0, 186000.0, 186500.0, 186500.0, 188000.0, 188000.0, 188000.0, 189000.0, 189000.0, 189500.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 196000.0, 196000.0, 198500.0, 198500.0, 198500.0, 198500.0, 198500.0, 198500.0, 198500.0, 198500.0, 205000.0, 205000.0, 205000.0, 205000.0, 205000.0, 206000.0, 206000.0, 206500.0, 206500.0, 206500.0, 206500.0, 206500.0, 206500.0, 210000.0, 210000.0, 211000.0, 211000.0, 211000.0, 211000.0, 211000.0, 212500.0, 212500.0, 213000.0, 216000.0, 216000.0, 216000.0, 221500.0, 221500.0, 221500.0, 221500.0, 221500.0, 221500.0, 223500.0, 223500.0, 223500.0, 223500.0, 223500.0, 223500.0, 233000.0, 233000.0, 233000.0, 233000.0, 233000.0, 234500.0, 235000.0, 235000.0, 235000.0, 235000.0, 235500.0, 235500.0, 237000.0, 237000.0, 237000.0, 237000.0, 237000.0, 237500.0, 237500.0, 238500.0, 238500.0, 238500.0, 240000.0, 240000.0, 240000.0, 240000.0, 240000.0, 243000.0, 245500.0, 245500.0, 245500.0, 245500.0, 245500.0, 246000.0, 247500.0, 247500.0, 247500.0, 250500.0, 250500.0, 250500.0, 250500.0, 250500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 254500.0, 255500.0, 255500.0, 255500.0, 256500.0, 256500.0, 256500.0, 256500.0, 256500.0, 257500.0, 259000.0, 259000.0, 261000.0, 262000.0, 262500.0, 262500.0, 263000.0, 263000.0, 263000.0, 265500.0, 265500.0, 265500.0, 265500.0, 269000.0, 269000.0, 269000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 273000.0, 274000.0, 275000.0, 275000.0, 275500.0, 276000.0, 276000.0, 277500.0, 277500.0, 281000.0, 281000.0, 281000.0, 281000.0, 282000.0, 282500.0, 282500.0, 283000.0, 283000.0, 283000.0, 284500.0, 284500.0, 284500.0, 286500.0, 286500.0, 289000.0, 289000.0, 289000.0, 289500.0, 289500.0, 291000.0, 291000.0, 291000.0, 291000.0, 291000.0, 292500.0, 295500.0, 295500.0, 295500.0, 295500.0, 295500.0, 295500.0, 296000.0, 297000.0, 297500.0, 298500.0, 299500.0, 299500.0, 301000.0, 301000.0, 301500.0, 302500.0, 302500.0, 303500.0, 304000.0, 304000.0, 305000.0, 307500.0, 308500.0, 309500.0, 309500.0, 309500.0, 309500.0, 309500.0, 310500.0, 310500.0, 310500.0, 313000.0, 313000.0, 313000.0, 313000.0, 313000.0, 313000.0, 313500.0, 314500.0, 314500.0, 315000.0, 316000.0, 317500.0, 317500.0, 317500.0, 317500.0, 317500.0, 318500.0, 318500.0, 318500.0, 322000.0, 322000.0, 322000.0, 322000.0, 322000.0, 322000.0, 323000.0, 323000.0, 323000.0, 324500.0, 324500.0, 325000.0, 326000.0, 327000.0, 330500.0, 330500.0, 330500.0, 330500.0, 331500.0, 334500.0, 334500.0, 335000.0, 335000.0, 335000.0, 335000.0, 336000.0, 337000.0, 337000.0, 338000.0, 342000.0, 342000.0, 342000.0, 342000.0, 342000.0, 343000.0, 343000.0, 343500.0, 343500.0, 344000.0, 345500.0, 345500.0, 345500.0, 348500.0, 348500.0, 348500.0, 349000.0, 349000.0, 350000.0, 350000.0, 352000.0, 352000.0, 352000.0, 352000.0, 352000.0, 352000.0, 353500.0, 353500.0, 353500.0, 354500.0, 354500.0, 354500.0, 356000.0, 356500.0, 357000.0, 358000.0, 358000.0, 358000.0, 358000.0, 358000.0, 359000.0, 359000.0, 359000.0, 360000.0, 360000.0, 360000.0, 364000.0, 364500.0, 364500.0, 366000.0, 366000.0, 370000.0, 370000.0, 370000.0, 370000.0, 371500.0, 371500.0, 371500.0, 373500.0, 373500.0, 373500.0, 373500.0, 373500.0, 373500.0, 375000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378000.0, 378500.0, 384000.0, 384000.0, 385500.0, 385500.0, 385500.0, 385500.0, 385500.0, 385500.0, 386000.0, 387500.0, 387500.0, 389000.0, 390500.0, 390500.0, 390500.0, 390500.0, 390500.0, 390500.0, 390500.0, 390500.0, 390500.0, 391000.0, 391500.0, 391500.0, 392000.0, 393500.0, 393500.0, 393500.0, 393500.0, 396000.0, 397000.0, 397000.0, 397000.0, 397500.0, 397500.0, 399500.0, 399500.0, 400000.0, 400000.0, 400500.0, 400500.0, 402000.0, 402500.0, 402500.0, 404000.0, 404000.0, 404500.0, 404500.0, 404500.0, 404500.0, 406500.0, 406500.0, 407500.0, 409000.0, 409000.0, 412500.0, 412500.0, 412500.0, 412500.0, 413500.0, 415000.0, 415000.0, 415000.0, 415000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418000.0, 418500.0, 419000.0, 419500.0, 421000.0, 421000.0, 422500.0, 422500.0, 422500.0, 424000.0, 426000.0, 426000.0, 426000.0, 426000.0, 427000.0, 427000.0, 427500.0, 432000.0, 432000.0, 432000.0, 432000.0, 436000.0, 437000.0, 437000.0, 437000.0, 437000.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 438500.0, 439000.0, 439000.0, 439000.0, 439500.0, 441500.0, 442500.0, 442500.0, 446000.0, 446000.0, 446500.0, 446500.0, 446500.0, 446500.0, 447500.0, 447500.0, 448000.0, 450000.0, 450000.0, 450000.0, 450000.0, 450500.0, 453000.0, 453000.0, 453000.0, 453000.0, 454000.0, 458000.0, 459000.0, 459000.0, 459000.0, 459000.0, 459000.0, 460000.0, 460000.0, 460000.0, 460000.0, 462000.0, 462000.0, 462000.0, 462000.0, 462000.0, 464000.0, 464000.0, 464000.0, 464000.0, 468500.0, 468500.0, 468500.0, 469500.0, 469500.0, 469500.0, 469500.0, 470500.0, 470500.0, 471000.0, 471500.0, 472500.0, 473500.0, 473500.0, 474500.0, 474500.0, 475000.0, 477000.0, 477000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 481500.0, 481500.0, 484500.0, 484500.0, 484500.0, 484500.0, 484500.0, 485500.0, 485500.0, 487500.0, 487500.0, 487500.0, 490000.0, 491000.0, 494500.0, 494500.0, 494500.0, 494500.0, 494500.0, 494500.0, 495500.0, 495500.0, 495500.0, 497000.0, 497000.0, 497000.0, 497000.0, 498000.0, 499500.0, 499500.0, 500000.0, 500500.0, 500500.0, 500500.0, 502000.0, 503000.0, 503000.0, 503000.0, 503000.0, 503000.0, 503500.0, 507500.0, 507500.0, 507500.0, 508000.0, 508000.0, 508000.0, 509000.0, 509000.0, 509000.0, 510000.0, 510500.0, 511000.0, 511500.0, 512000.0, 513500.0, 514000.0, 517000.0, 517000.0, 517000.0, 517000.0, 518000.0, 518000.0, 518000.0, 518000.0, 519500.0, 521000.0, 521000.0, 521000.0, 521000.0, 521500.0, 521500.0, 521500.0, 523000.0, 523000.0, 523000.0, 523500.0, 523500.0, 524500.0, 527000.0, 527000.0, 527000.0, 527000.0, 527000.0, 528000.0, 528000.0, 530000.0, 531000.0, 531000.0, 531500.0, 531500.0, 531500.0, 532000.0, 533000.0, 534500.0, 534500.0, 535500.0, 536500.0, 536500.0, 536500.0, 537500.0, 537500.0, 537500.0, 538500.0, 539000.0, 539000.0, 539500.0, 540000.0, 540000.0, 540000.0, 540000.0, 540000.0, 540500.0, 547000.0, 547000.0, 547000.0, 547000.0, 548000.0, 548000.0, 548000.0, 548000.0, 549000.0, 549000.0, 549000.0, 550000.0, 550500.0, 552000.0, 554500.0, 554500.0, 554500.0, 554500.0, 554500.0, 554500.0, 556000.0, 556000.0, 557000.0, 557000.0, 557000.0, 558000.0, 558000.0, 558000.0, 558500.0, 559000.0, 561000.0, 562000.0, 562500.0, 562500.0, 563000.0, 563000.0, 564000.0, 564500.0, 565000.0, 566500.0, 566500.0, 566500.0, 569000.0, 569000.0, 569000.0, 573000.0, 573000.0, 573000.0, 573000.0, 573000.0, 573000.0, 573500.0, 573500.0, 573500.0, 573500.0, 573500.0, 574500.0, 574500.0, 575000.0, 576000.0, 576000.0, 576000.0, 576000.0, 576000.0, 576000.0, 576000.0, 579500.0, 579500.0, 580000.0, 580000.0, 580000.0, 586000.0, 586500.0, 586500.0, 586500.0, 586500.0, 586500.0, 587000.0, 587000.0, 589500.0, 589500.0, 590000.0, 590000.0, 590000.0, 590000.0, 590000.0, 590000.0, 590500.0, 591000.0, 591500.0, 591500.0, 591500.0, 596000.0, 596000.0, 596000.0, 597000.0, 598500.0, 599000.0, 599000.0, 599000.0, 599000.0, 599000.0, 599000.0, 601000.0, 601000.0, 601000.0, 601000.0, 602000.0, 602000.0, 603000.0, 603500.0, 604500.0, 604500.0, 604500.0, 604500.0, 604500.0, 605500.0, 605500.0, 606500.0, 606500.0, 607000.0, 608500.0, 611500.0, 612500.0, 612500.0, 612500.0, 612500.0, 612500.0, 613500.0, 613500.0, 614000.0, 614000.0, 615000.0, 615500.0, 616000.0, 616500.0, 617000.0, 618500.0, 619000.0, 619000.0, 619000.0, 619000.0, 620000.0, 621000.0, 621000.0, 621000.0, 621000.0, 622500.0, 622500.0, 622500.0, 622500.0, 622500.0, 624000.0, 624000.0, 624000.0, 625000.0, 625000.0, 628000.0, 628000.0, 628000.0, 628000.0, 629500.0, 629500.0, 629500.0, 630000.0, 630000.0, 630000.0, 634500.0, 634500.0, 635500.0, 635500.0, 635500.0, 635500.0, 636000.0, 636000.0, 636000.0, 636000.0, 636500.0, 636500.0, 637500.0, 638500.0, 638500.0, 638500.0, 638500.0, 640500.0, 640500.0, 640500.0, 641500.0, 641500.0, 641500.0, 642000.0, 642000.0, 644500.0, 644500.0, 644500.0, 645000.0, 645000.0, 645000.0, 645000.0, 645000.0, 646000.0, 646500.0, 646500.0, 648000.0, 650000.0, 652000.0, 652000.0, 652000.0, 655000.0, 655000.0, 655000.0, 655000.0, 655000.0, 658500.0, 658500.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 660000.0, 661000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 664000.0, 665000.0, 665000.0, 666000.0, 666500.0, 666500.0, 666500.0, 667500.0, 668000.0, 669000.0, 670000.0, 671000.0, 671000.0, 671000.0, 672000.0, 672000.0, 672000.0, 672000.0, 677000.0, 677000.0, 683500.0, 683500.0, 683500.0, 684000.0, 684000.0, 684000.0, 684000.0, 684000.0, 685500.0, 686500.0, 686500.0, 686500.0, 686500.0, 688500.0, 688500.0, 688500.0, 688500.0, 689500.0, 689500.0, 689500.0, 691000.0, 691000.0, 695019.6875, 695019.6875, 695019.6875, 695019.6875, 695019.6875, 695019.6875], "elapsed": [4963.549852371216, 4963.549852371216, 4963.549852371216, 4963.549852371216, 7521.96478843689, 7521.96478843689, 7521.96478843689, 10089.873313903809, 10089.873313903809, 10089.873313903809, 10089.873313903809, 12704.97727394104, 12704.97727394104, 15594.531297683716, 15594.531297683716, 22800.93765258789, 22800.93765258789, 32761.80601119995, 32761.80601119995, 32761.80601119995, 36915.56787490845, 36915.56787490845, 36915.56787490845, 36915.56787490845, 36915.56787490845, 36915.56787490845, 39565.56820869446, 39565.56820869446, 39565.56820869446, 39565.56820869446, 39565.56820869446, 42552.87718772888, 42552.87718772888, 46374.568700790405, 46374.568700790405, 46374.568700790405, 49554.07738685608, 49554.07738685608, 49554.07738685608, 49554.07738685608, 50896.01182937622, 53868.34907531738, 53868.34907531738, 53868.34907531738, 54696.88677787781, 61478.142976760864, 61478.142976760864, 61478.142976760864, 61478.142976760864, 63183.167695999146, 63183.167695999146, 64087.26787567139, 64087.26787567139, 65866.27054214478, 66869.28749084473, 66869.28749084473, 67832.40151405334, 68769.19937133789, 71608.47330093384, 71608.47330093384, 72666.64600372314, 72666.64600372314, 74700.75559616089, 74700.75559616089, 74700.75559616089, 79139.19186592102, 79139.19186592102, 79139.19186592102, 79139.19186592102, 87048.45356941223, 87048.45356941223, 87048.45356941223, 87048.45356941223, 87048.45356941223, 87048.45356941223, 87048.45356941223, 87048.45356941223, 89467.87452697754, 90786.62109375, 92176.55992507935, 93611.29140853882, 94984.30347442627, 97641.46280288696, 100540.68374633789, 100540.68374633789, 104674.56293106079, 104674.56293106079, 104674.56293106079, 104674.56293106079, 107238.68608474731, 110803.70354652405, 110803.70354652405, 110803.70354652405, 113851.26638412476, 116381.06656074524, 116381.06656074524, 116381.06656074524, 117484.20023918152, 117484.20023918152, 117484.20023918152, 117484.20023918152, 120389.57095146179, 120389.57095146179, 122773.61369132996, 125112.62273788452, 126591.55368804932, 126591.55368804932, 126591.55368804932, 130404.66332435608, 130404.66332435608, 135026.19218826294, 135026.19218826294, 135026.19218826294, 135026.19218826294, 135026.19218826294, 135026.19218826294, 135026.19218826294, 139576.52878761292, 140778.8369655609, 140778.8369655609, 140778.8369655609, 140778.8369655609, 141996.19007110596, 145608.20841789246, 148283.44321250916, 148283.44321250916, 148283.44321250916, 150847.09215164185, 150847.09215164185, 152182.54590034485, 152182.54590034485, 153528.1744003296, 153528.1744003296, 156385.50734519958, 159177.96111106873, 159177.96111106873, 159177.96111106873, 159177.96111106873, 165848.64783287048, 165848.64783287048, 165848.64783287048, 165848.64783287048, 165848.64783287048, 165848.64783287048, 167577.2738456726, 170398.2493877411, 172359.62772369385, 172359.62772369385, 172359.62772369385, 174350.92425346375, 174350.92425346375, 176312.0882511139, 176312.0882511139, 176312.0882511139, 180561.47956848145, 180561.47956848145, 180561.47956848145, 182917.63877868652, 182917.63877868652, 185361.42230033875, 185361.42230033875, 186518.65530014038, 187670.8369255066, 188873.18778038025, 188873.18778038025, 188873.18778038025, 205895.73764801025, 205895.73764801025, 205895.73764801025, 208604.50863838196, 208604.50863838196, 208604.50863838196, 211337.9189968109, 211337.9189968109, 212724.04885292053, 212724.04885292053, 214162.93287277222, 214162.93287277222, 216977.47778892517, 216977.47778892517, 223354.83074188232, 223354.83074188232, 223354.83074188232, 223354.83074188232, 224231.72044754028, 227912.96315193176, 227912.96315193176, 228977.72598266602, 228977.72598266602, 228977.72598266602, 228977.72598266602, 228977.72598266602, 228977.72598266602, 241313.90404701233, 241313.90404701233, 241313.90404701233, 241313.90404701233, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 244245.42236328125, 246644.58060264587, 247911.55219078064, 247911.55219078064, 247911.55219078064, 253667.69123077393, 253667.69123077393, 253667.69123077393, 253667.69123077393, 254935.00781059265, 254935.00781059265, 256190.4218196869, 256190.4218196869, 258696.33293151855, 260106.60672187805, 260106.60672187805, 262839.7743701935, 262839.7743701935, 265482.1083545685, 270910.8910560608, 270910.8910560608, 270910.8910560608, 270910.8910560608, 275446.6459751129, 275446.6459751129, 275446.6459751129, 277655.7080745697, 278525.9711742401, 278525.9711742401, 282069.70286369324, 282069.70286369324, 284330.3961753845, 285998.4917640686, 289779.6161174774, 289779.6161174774, 291755.2263736725, 291755.2263736725, 297645.17092704773, 297645.17092704773, 297645.17092704773, 299881.6485404968, 299881.6485404968, 299881.6485404968, 299881.6485404968, 299881.6485404968, 299881.6485404968, 302218.47558021545, 302218.47558021545, 302218.47558021545, 304307.24334716797, 305463.623046875, 306763.9331817627, 309109.34376716614, 309109.34376716614, 311632.5533390045, 311632.5533390045, 312905.4615497589, 314133.30602645874, 314133.30602645874, 315425.8723258972, 316800.1880645752, 316800.1880645752, 319328.0231952667, 319328.0231952667, 320705.8708667755, 332134.80162620544, 332134.80162620544, 332134.80162620544, 332134.80162620544, 332134.80162620544, 332134.80162620544, 337905.23195266724, 337905.23195266724, 339544.4800853729, 339544.4800853729, 339544.4800853729, 340414.90840911865, 342335.2196216583, 342335.2196216583, 342335.2196216583, 344073.73118400574, 344073.73118400574, 344073.73118400574, 346837.02397346497, 346837.02397346497, 347774.30725097656, 360738.1353378296, 360738.1353378296, 360738.1353378296, 360738.1353378296, 362920.2456474304, 362920.2456474304, 362920.2456474304, 362920.2456474304, 362920.2456474304, 362920.2456474304, 362920.2456474304, 362920.2456474304, 367849.3731021881, 367849.3731021881, 369011.83128356934, 369011.83128356934, 372530.0393104553, 372530.0393104553, 372530.0393104553, 372530.0393104553, 374938.6785030365, 377593.3094024658, 377593.3094024658, 378881.15310668945, 380174.1991043091, 382673.2873916626, 382673.2873916626, 382673.2873916626, 386238.760471344, 387000.896692276, 387815.6943321228, 388638.4918689728, 393192.4202442169, 393192.4202442169, 393192.4202442169, 393192.4202442169, 393192.4202442169, 393192.4202442169, 393192.4202442169, 401348.50311279297, 401348.50311279297, 401348.50311279297, 403406.26335144043, 403406.26335144043, 403406.26335144043, 405588.76156806946, 406703.22251319885, 406703.22251319885, 410986.89246177673, 410986.89246177673, 410986.89246177673, 413523.5433578491, 413523.5433578491, 414807.7600002289, 416110.43548583984, 416110.43548583984, 416110.43548583984, 416110.43548583984, 417518.8419818878, 417518.8419818878, 417518.8419818878, 418799.907207489, 418799.907207489, 422306.00023269653, 422306.00023269653, 422306.00023269653, 425006.2186717987, 425006.2186717987, 426247.92194366455, 432977.8702259064, 432977.8702259064, 432977.8702259064, 432977.8702259064, 432977.8702259064, 432977.8702259064, 439416.4845943451, 439416.4845943451, 444703.777551651, 444703.777551651, 444703.777551651, 444703.777551651, 444703.777551651, 444703.777551651, 444703.777551651, 444703.777551651, 457006.206035614, 457006.206035614, 457006.206035614, 457006.206035614, 457006.206035614, 461217.0715332031, 461217.0715332031, 462483.8306903839, 462483.8306903839, 462483.8306903839, 462483.8306903839, 462483.8306903839, 462483.8306903839, 470006.276845932, 470006.276845932, 472564.23830986023, 472564.23830986023, 472564.23830986023, 472564.23830986023, 472564.23830986023, 476259.4873905182, 476259.4873905182, 477484.21573638916, 484684.29493904114, 484684.29493904114, 484684.29493904114, 493717.4754142761, 493717.4754142761, 493717.4754142761, 493717.4754142761, 493717.4754142761, 493717.4754142761, 497342.2648906708, 497342.2648906708, 497342.2648906708, 497342.2648906708, 497342.2648906708, 497342.2648906708, 517180.8569431305, 517180.8569431305, 517180.8569431305, 517180.8569431305, 517180.8569431305, 520809.13376808167, 522115.9794330597, 522115.9794330597, 522115.9794330597, 522115.9794330597, 523399.39975738525, 523399.39975738525, 526987.6065254211, 526987.6065254211, 526987.6065254211, 526987.6065254211, 526987.6065254211, 528310.2061748505, 528310.2061748505, 530880.9244632721, 530880.9244632721, 530880.9244632721, 534608.8728904724, 534608.8728904724, 534608.8728904724, 534608.8728904724, 534608.8728904724, 539062.5712871552, 543647.0053195953, 543647.0053195953, 543647.0053195953, 543647.0053195953, 543647.0053195953, 544631.5100193024, 547492.1238422394, 547492.1238422394, 547492.1238422394, 552894.0436840057, 552894.0436840057, 552894.0436840057, 552894.0436840057, 552894.0436840057, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 561819.8885917664, 564209.5625400543, 564209.5625400543, 564209.5625400543, 567823.0056762695, 567823.0056762695, 567823.0056762695, 567823.0056762695, 567823.0056762695, 571200.2341747284, 574655.0929546356, 574655.0929546356, 579357.2034835815, 581754.1611194611, 583008.7206363678, 583008.7206363678, 584303.1234741211, 584303.1234741211, 584303.1234741211, 590151.0381698608, 590151.0381698608, 590151.0381698608, 590151.0381698608, 595516.1733627319, 595516.1733627319, 595516.1733627319, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 603046.9119548798, 605235.6226444244, 607399.8908996582, 607399.8908996582, 608504.5828819275, 609693.3491230011, 609693.3491230011, 613142.6849365234, 613142.6849365234, 621309.9920749664, 621309.9920749664, 621309.9920749664, 621309.9920749664, 624479.3386459351, 625799.1988658905, 625799.1988658905, 627085.9708786011, 627085.9708786011, 627085.9708786011, 630929.4710159302, 630929.4710159302, 630929.4710159302, 636280.2631855011, 636280.2631855011, 642209.6972465515, 642209.6972465515, 642209.6972465515, 643333.860874176, 643333.860874176, 645912.2314453125, 645912.2314453125, 645912.2314453125, 645912.2314453125, 645912.2314453125, 648566.4713382721, 654444.8661804199, 654444.8661804199, 654444.8661804199, 654444.8661804199, 654444.8661804199, 654444.8661804199, 655499.2537498474, 657536.1521244049, 658644.5782184601, 662137.2358798981, 665475.72016716, 665475.72016716, 668991.5838241577, 668991.5838241577, 670171.1385250092, 672389.6362781525, 672389.6362781525, 674791.0847663879, 676035.0689888, 676035.0689888, 678489.091873169, 685001.482963562, 687823.7533569336, 690993.3404922485, 690993.3404922485, 690993.3404922485, 690993.3404922485, 690993.3404922485, 693866.3575649261, 693866.3575649261, 693866.3575649261, 700766.8130397797, 700766.8130397797, 700766.8130397797, 700766.8130397797, 700766.8130397797, 700766.8130397797, 701613.4192943573, 703323.41837883, 703323.41837883, 704393.2402133942, 706253.1459331512, 708997.3382949829, 708997.3382949829, 708997.3382949829, 708997.3382949829, 708997.3382949829, 710879.6391487122, 710879.6391487122, 710879.6391487122, 718204.5826911926, 718204.5826911926, 718204.5826911926, 718204.5826911926, 718204.5826911926, 718204.5826911926, 720479.0570735931, 720479.0570735931, 720479.0570735931, 723946.9578266144, 723946.9578266144, 725128.057718277, 727421.9679832458, 729971.8527793884, 739809.5850944519, 739809.5850944519, 739809.5850944519, 739809.5850944519, 742602.457523346, 750545.9883213043, 750545.9883213043, 752049.0131378174, 752049.0131378174, 752049.0131378174, 752049.0131378174, 754803.7741184235, 756324.4752883911, 756324.4752883911, 757871.607542038, 764154.5112133026, 764154.5112133026, 764154.5112133026, 764154.5112133026, 764154.5112133026, 768450.5071640015, 768450.5071640015, 771246.9527721405, 771246.9527721405, 772136.8730068207, 774709.157705307, 774709.157705307, 774709.157705307, 780818.653345108, 780818.653345108, 780818.653345108, 781899.0302085876, 781899.0302085876, 783945.9753036499, 783945.9753036499, 788658.0457687378, 788658.0457687378, 788658.0457687378, 788658.0457687378, 788658.0457687378, 788658.0457687378, 792519.8497772217, 792519.8497772217, 792519.8497772217, 795250.4997253418, 795250.4997253418, 795250.4997253418, 799179.5382499695, 800539.7439002991, 802008.1815719604, 805775.9110927582, 805775.9110927582, 805775.9110927582, 805775.9110927582, 805775.9110927582, 809688.9593601227, 809688.9593601227, 809688.9593601227, 812611.1767292023, 812611.1767292023, 812611.1767292023, 818439.4745826721, 819274.3830680847, 819274.3830680847, 821923.4311580658, 821923.4311580658, 829813.5459423065, 829813.5459423065, 829813.5459423065, 829813.5459423065, 833072.3216533661, 833072.3216533661, 833072.3216533661, 837114.3779754639, 837114.3779754639, 837114.3779754639, 837114.3779754639, 837114.3779754639, 837114.3779754639, 840754.7361850739, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 848316.1473274231, 849743.3485984802, 863731.3759326935, 863731.3759326935, 866243.9472675323, 866243.9472675323, 866243.9472675323, 866243.9472675323, 866243.9472675323, 866243.9472675323, 867075.6530761719, 869540.0958061218, 869540.0958061218, 872298.6831665039, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 875402.7450084686, 876538.0711555481, 877595.7300662994, 877595.7300662994, 878605.7605743408, 881641.0181522369, 881641.0181522369, 881641.0181522369, 881641.0181522369, 886777.1100997925, 889285.507440567, 889285.507440567, 889285.507440567, 890484.7235679626, 890484.7235679626, 895252.5646686554, 895252.5646686554, 896549.7059822083, 896549.7059822083, 897841.3853645325, 897841.3853645325, 901518.7795162201, 902905.1938056946, 902905.1938056946, 907181.4203262329, 907181.4203262329, 909180.2968978882, 909180.2968978882, 909180.2968978882, 909180.2968978882, 915001.9071102142, 915001.9071102142, 917741.7895793915, 920711.9104862213, 920711.9104862213, 926569.5652961731, 926569.5652961731, 926569.5652961731, 926569.5652961731, 928405.4501056671, 931212.1744155884, 931212.1744155884, 931212.1744155884, 931212.1744155884, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 937979.3243408203, 939002.7403831482, 940042.6552295685, 941135.2009773254, 944249.4523525238, 944249.4523525238, 947801.7871379852, 947801.7871379852, 947801.7871379852, 951609.6811294556, 960128.6420822144, 960128.6420822144, 960128.6420822144, 960128.6420822144, 962663.791179657, 962663.791179657, 963949.634552002, 975336.2019062042, 975336.2019062042, 975336.2019062042, 975336.2019062042, 982223.5960960388, 984247.4884986877, 984247.4884986877, 984247.4884986877, 984247.4884986877, 987357.4254512787, 987357.4254512787, 987357.4254512787, 987357.4254512787, 987357.4254512787, 987357.4254512787, 987357.4254512787, 988432.4860572815, 988432.4860572815, 988432.4860572815, 989509.5450878143, 993493.2284355164, 995859.1313362122, 995859.1313362122, 1003239.0439510345, 1003239.0439510345, 1004515.5546665192, 1004515.5546665192, 1004515.5546665192, 1004515.5546665192, 1006849.1010665894, 1006849.1010665894, 1008132.4665546417, 1013125.8416175842, 1013125.8416175842, 1013125.8416175842, 1013125.8416175842, 1014550.2181053162, 1020929.1763305664, 1020929.1763305664, 1020929.1763305664, 1020929.1763305664, 1024054.2230606079, 1036255.4981708527, 1038186.0885620117, 1038186.0885620117, 1038186.0885620117, 1038186.0885620117, 1038186.0885620117, 1040754.0545463562, 1040754.0545463562, 1040754.0545463562, 1040754.0545463562, 1044728.8508415222, 1044728.8508415222, 1044728.8508415222, 1044728.8508415222, 1044728.8508415222, 1048538.099527359, 1048538.099527359, 1048538.099527359, 1048538.099527359, 1057088.77658844, 1057088.77658844, 1057088.77658844, 1059317.708015442, 1059317.708015442, 1059317.708015442, 1059317.708015442, 1061678.5943508148, 1061678.5943508148, 1062788.7752056122, 1063881.381034851, 1066040.3122901917, 1068390.1879787445, 1068390.1879787445, 1070782.5374603271, 1070782.5374603271, 1072028.8062095642, 1077197.4613666534, 1077197.4613666534, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1087208.5285186768, 1089513.5958194733, 1089513.5958194733, 1094939.998626709, 1094939.998626709, 1094939.998626709, 1094939.998626709, 1094939.998626709, 1096781.714439392, 1096781.714439392, 1100741.7628765106, 1100741.7628765106, 1100741.7628765106, 1105328.0301094055, 1107406.0184955597, 1114185.152053833, 1114185.152053833, 1114185.152053833, 1114185.152053833, 1114185.152053833, 1114185.152053833, 1116545.827627182, 1116545.827627182, 1116545.827627182, 1120269.1984176636, 1120269.1984176636, 1120269.1984176636, 1120269.1984176636, 1124275.4933834076, 1128430.4237365723, 1128430.4237365723, 1129797.928094864, 1131159.7011089325, 1131159.7011089325, 1131159.7011089325, 1135177.5977611542, 1138491.4281368256, 1138491.4281368256, 1138491.4281368256, 1138491.4281368256, 1138491.4281368256, 1139905.373096466, 1146532.8829288483, 1146532.8829288483, 1146532.8829288483, 1147481.0826778412, 1147481.0826778412, 1147481.0826778412, 1149412.612438202, 1149412.612438202, 1149412.612438202, 1151060.8084201813, 1151973.6332893372, 1152924.0629673004, 1153977.3738384247, 1154988.576412201, 1157707.7717781067, 1158825.7632255554, 1164389.209985733, 1164389.209985733, 1164389.209985733, 1164389.209985733, 1166692.3024654388, 1166692.3024654388, 1166692.3024654388, 1166692.3024654388, 1170333.6317539215, 1174021.3644504547, 1174021.3644504547, 1174021.3644504547, 1174021.3644504547, 1175329.2067050934, 1175329.2067050934, 1175329.2067050934, 1179355.8826446533, 1179355.8826446533, 1179355.8826446533, 1180739.658355713, 1180739.658355713, 1183343.6748981476, 1191351.8614768982, 1191351.8614768982, 1191351.8614768982, 1191351.8614768982, 1191351.8614768982, 1194136.432170868, 1194136.432170868, 1197789.3552780151, 1199364.96925354, 1199364.96925354, 1200263.2308006287, 1200263.2308006287, 1200263.2308006287, 1201113.0576133728, 1202949.589729309, 1205437.0687007904, 1205437.0687007904, 1207230.5521965027, 1209174.0326881409, 1209174.0326881409, 1209174.0326881409, 1211463.9768600464, 1211463.9768600464, 1211463.9768600464, 1213559.0105056763, 1214720.8456993103, 1214720.8456993103, 1215898.199081421, 1217068.8073635101, 1217068.8073635101, 1217068.8073635101, 1217068.8073635101, 1217068.8073635101, 1218292.7277088165, 1232793.1141853333, 1232793.1141853333, 1232793.1141853333, 1232793.1141853333, 1235590.3759002686, 1235590.3759002686, 1235590.3759002686, 1235590.3759002686, 1241510.2643966675, 1241510.2643966675, 1241510.2643966675, 1243987.2567653656, 1245271.8093395233, 1249690.4764175415, 1253705.5804729462, 1253705.5804729462, 1253705.5804729462, 1253705.5804729462, 1253705.5804729462, 1253705.5804729462, 1256246.5455532074, 1256246.5455532074, 1258018.2387828827, 1258018.2387828827, 1258018.2387828827, 1259904.2315483093, 1259904.2315483093, 1259904.2315483093, 1260809.2501163483, 1261775.726556778, 1265343.5435295105, 1267481.7543029785, 1268674.09157753, 1268674.09157753, 1269682.5878620148, 1269682.5878620148, 1271762.9070281982, 1272941.456079483, 1274124.492406845, 1277404.0458202362, 1277404.0458202362, 1277404.0458202362, 1283364.7060394287, 1283364.7060394287, 1283364.7060394287, 1295874.8111724854, 1295874.8111724854, 1295874.8111724854, 1295874.8111724854, 1295874.8111724854, 1295874.8111724854, 1297314.7320747375, 1297314.7320747375, 1297314.7320747375, 1297314.7320747375, 1297314.7320747375, 1299881.713628769, 1299881.713628769, 1301290.898323059, 1304274.9736309052, 1304274.9736309052, 1304274.9736309052, 1304274.9736309052, 1304274.9736309052, 1304274.9736309052, 1304274.9736309052, 1309582.7949047089, 1309582.7949047089, 1310473.984003067, 1310473.984003067, 1310473.984003067, 1320186.5196228027, 1321262.2861862183, 1321262.2861862183, 1321262.2861862183, 1321262.2861862183, 1321262.2861862183, 1322331.6087722778, 1322331.6087722778, 1331790.498971939, 1331790.498971939, 1333081.4988613129, 1333081.4988613129, 1333081.4988613129, 1333081.4988613129, 1333081.4988613129, 1333081.4988613129, 1334241.8594360352, 1335356.9042682648, 1336484.887123108, 1336484.887123108, 1336484.887123108, 1346421.2403297424, 1346421.2403297424, 1346421.2403297424, 1349315.0799274445, 1353257.1952342987, 1354826.5612125397, 1354826.5612125397, 1354826.5612125397, 1354826.5612125397, 1354826.5612125397, 1354826.5612125397, 1359206.1693668365, 1359206.1693668365, 1359206.1693668365, 1359206.1693668365, 1360781.3558578491, 1360781.3558578491, 1362401.0231494904, 1363281.3272476196, 1364878.0119419098, 1364878.0119419098, 1364878.0119419098, 1364878.0119419098, 1364878.0119419098, 1366576.7214298248, 1366576.7214298248, 1368254.0814876556, 1368254.0814876556, 1369176.0172843933, 1371727.2868156433, 1379494.559288025, 1381706.9973945618, 1381706.9973945618, 1381706.9973945618, 1381706.9973945618, 1381706.9973945618, 1383724.9886989594, 1383724.9886989594, 1384765.6800746918, 1384765.6800746918, 1386915.635585785, 1388052.7942180634, 1389203.9594650269, 1390349.6651649475, 1391504.1017532349, 1395148.1473445892, 1396450.1292705536, 1396450.1292705536, 1396450.1292705536, 1396450.1292705536, 1398921.0720062256, 1401900.9988307953, 1401900.9988307953, 1401900.9988307953, 1401900.9988307953, 1406054.5411109924, 1406054.5411109924, 1406054.5411109924, 1406054.5411109924, 1406054.5411109924, 1410250.53024292, 1410250.53024292, 1410250.53024292, 1411783.130645752, 1411783.130645752, 1416612.0209693909, 1416612.0209693909, 1416612.0209693909, 1416612.0209693909, 1419432.106256485, 1419432.106256485, 1419432.106256485, 1420430.5665493011, 1420430.5665493011, 1420430.5665493011, 1428208.4622383118, 1428208.4622383118, 1430515.9544944763, 1430515.9544944763, 1430515.9544944763, 1430515.9544944763, 1431706.7563533783, 1431706.7563533783, 1431706.7563533783, 1431706.7563533783, 1432783.7133407593, 1432783.7133407593, 1434892.6401138306, 1437242.445230484, 1437242.445230484, 1437242.445230484, 1437242.445230484, 1441527.7976989746, 1441527.7976989746, 1441527.7976989746, 1444020.4010009766, 1444020.4010009766, 1444020.4010009766, 1445306.3879013062, 1445306.3879013062, 1451280.1659107208, 1451280.1659107208, 1451280.1659107208, 1452663.2750034332, 1452663.2750034332, 1452663.2750034332, 1452663.2750034332, 1452663.2750034332, 1455147.9074954987, 1456507.6577663422, 1456507.6577663422, 1460627.073764801, 1463635.2849006653, 1467020.0719833374, 1467020.0719833374, 1467020.0719833374, 1471859.783411026, 1471859.783411026, 1471859.783411026, 1471859.783411026, 1471859.783411026, 1483002.5298595428, 1483002.5298595428, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1486432.8055381775, 1488707.0879936218, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1495288.991689682, 1497601.1753082275, 1497601.1753082275, 1499878.8738250732, 1501122.7571964264, 1501122.7571964264, 1501122.7571964264, 1503560.4991912842, 1504771.3363170624, 1507341.7086601257, 1509901.4520645142, 1512670.6511974335, 1512670.6511974335, 1512670.6511974335, 1516553.5447597504, 1516553.5447597504, 1516553.5447597504, 1516553.5447597504, 1524695.5914497375, 1524695.5914497375, 1536861.9103431702, 1536861.9103431702, 1536861.9103431702, 1538060.3234767914, 1538060.3234767914, 1538060.3234767914, 1538060.3234767914, 1538060.3234767914, 1540902.6045799255, 1542995.4810142517, 1542995.4810142517, 1542995.4810142517, 1542995.4810142517, 1547071.0899829865, 1547071.0899829865, 1547071.0899829865, 1547071.0899829865, 1549320.5347061157, 1549320.5347061157, 1549320.5347061157, 1552612.7107143402, 1552612.7107143402, 1562746.1396121979, 1562746.1396121979, 1562746.1396121979, 1562746.1396121979, 1562746.1396121979, 1562746.1396121979], "prediction_length": 1263, "reference": "Mein Name ist Antoine. Ich bin Doktorand an der University of Massachusetts Amherst. Ich stelle unser Paper KinyaBERT vor: ein Morphologie-bewusstes Kinyarwanda-Sprachmodell. Heute werde ich \u00fcber den Grund f\u00fcr diese Forschung sprechen. Dann werde ich die KinyaBERT-Modell-Architektur im Detail vorstellen. Ich werde dann \u00fcber unsere experimentellen Ergebnisse sprechen und zum Schluss einige Schlussfolgerungen darstellen. Wir alle wissen, dass die j\u00fcngsten Fortschritte bei der NLP durch die Verwendung von vortrainierten Sprachmodellen wie BERT erm\u00f6glicht wurden. Allerdings gibt es immer noch eine Reihe von Einschr\u00e4nkungen. Aufgrund der komplexen Morphologie, die von den meisten morphologisch reichen Sprachen ausgedr\u00fcckt wird, kann der allgegenw\u00e4rtige byte pair encoding-Tokenisierungsalgorithmus, den ich verwendet habe, nicht die genauen lexikalischen Unterwort-Einheiten extrahieren, d.\u00a0h. die Morpheme, die f\u00fcr eine effektive Repr\u00e4sentation ben\u00f6tigt werden. Hier haben wir zum Beispiel drei Kinyarwanda-W\u00f6rter, die mehrere Morpheme enthalten, aber die BPE-Algorithmen k\u00f6nnen sie nicht extrahieren. Das liegt daran, dass einige morphologische Regeln verschiedene Oberfl\u00e4chenformen erzeugen, die die genauen lexikalischen Informationen verbergen. BPE st\u00fctzt sich aber nur auf die Oberfl\u00e4chenformen und hat so keinen Zugang zu diesem lexikalischen Modell. Die zweite Herausforderung besteht darin, dass, selbst wenn man Zugang zu einem morphologischen Analysator von Oracle h\u00e4tte, w\u00fcrde es nicht ausreichen, das BPE-Token durch Morpheme zu ersetzen, um die morphologische Kompositionalit\u00e4t auszudr\u00fccken. Eine dritte L\u00fccke in der Forschung besteht darin, dass neue vortrainierte Sprachmodelle meist bei ressourcenintensiven Sprachen evaluiert werden. Wir m\u00fcssen ihre Anwendbarkeit auch bei geringen Ressourcen und in verschiedenen Sprachen bewerten. Daher pr\u00e4sentieren wir KinyaBERT. Das ist eine einfache, aber effektive Anpassung der BERT-Architektur, die f\u00fcr den effektiveren Umgang mit morphologisch reichen Sprachen gedacht ist. Wir evaluieren KinyaBERT mit Kinyarwanda, einer ressourcenarmen und morphologisch reichen Sprache, die von mehr als zw\u00f6lf Millionen Menschen in Ost- und Zentralafrika gesprochen wird. Die Eingabe f\u00fcr das Modell ist entweder ein Satz oder ein Dokument. Zum Beispiel haben wir hier den Satz: \u201eJohn twarahamubonye biradutangaza\u201c. Das bedeutet: \u201eWir waren \u00fcberrascht, John dort anzutreffen.\u201c Wie Sie sehen k\u00f6nnen, enthaltenW\u00f6rter im Kinyarwanda mehrere Morpheme, die unterschiedliche Informationen enthalten. Daher \u00fcbergeben wir in unserem Modell diesen Satz oder ein Dokument an einen morphologischen Analysator. Dieser erzeugt dann Morpheme, die in allen W\u00f6rtern enthalten sind. Die Morpheme setzen sich in der Regel aus dem Stamm und null oder mehr Affixen zusammen. Die Affixe k\u00f6nnen Zeitform, Aspekt, Subjekt oder Objekt in den Verben anzeigen und beziehen sich h\u00e4ufiger auf die Substantivklasse der Bantu f\u00fcr Subjekte und Objekte. Der morphologische Analysator erzeugt auch einen Teil eines Sprach-Tags f\u00fcr jedes der W\u00f6rter. Nach diesem Schritt erstellen wir Einbettungen f\u00fcr den Teil der Sprach-Tags. Einbettungen f\u00fcr die Affixe. Einbettungen f\u00fcr den Stamm. Dies sind die Einbettungen auf Morphologie-Ebene. Anschlie\u00dfend durchlaufen diese Einbettungen einen Morphologie-Encoder, der ein kleiner Transformer-Encoder ist, der auf jedes Wort unabh\u00e4ngig angewendet wird. Ausgegeben werden die Vektoren, die mit den morphologischen Informationen bei jedem Wort kontextualisiert werden. Nun f\u00fchren wir eine Komposition durch, bei der die morphologischen Einbettungen, die der Sprache und dem Wortstamm entsprechen, miteinander verkettet werden. Wir verketten sie mit einer weiteren Einbettung des Stammes auf der Ebene des Satzes. Dann erstellen wir eine Eingabe f\u00fcr den Hauptsatz oder den Dokument-Encoder. Die Endausgabe sind kontextualisierte Einbettungen, die f\u00fcr nachgelagerte NLP-Aufgaben verwendet werden k\u00f6nnen. F\u00fcr einen morphologischen Analysator verwenden wir Morphologie-Prinzipien mit endlichen Automaten auf zwei Ebenen und mit einer ma\u00dfgeschneiderten Implementierung, die auf die Sprache Kinyarwanda zugeschnitten ist. Wir modellieren die Morphologie aller W\u00f6rter auf Kinyarwanda, einschlie\u00dflich der Verben, Substantive, Demonstrativ- und Possessivpronomen, Numerale und andere. Wir verwenden einen nicht \u00fcberwachten Teil eines Sprach-Tagging-Algorithmus. Ein faktorisiertes Modell erster Ordnung wird verwendet, um die Morphologie-Wahrscheinlichkeit zu ber\u00fccksichtigen, d.\u00a0h. die Wahrscheinlichkeit, die vom morphologischen Analysator zugewiesen wird. Wir ber\u00fccksichtigen auch den Vorrang der Sprach-Tags sowie die syntaktischen Vereinbarungen, die in den eingegebenen W\u00f6rtern vorhanden sind. Der Teil des Sprach-Taggers verwendet eine bidirektionale Inferenz, die den h\u00e4ufiger verwendeten Viterbi-Algorithmus f\u00fcr die Dekodierung verbessert. Hier noch ein paar Anmerkungen zur Positionskodierung. Erstens verwendet der Morphologie-Encoder keine Positionskodierung. Das liegt daran, dass jedes der Morpheme einen bekannten Platz im morphologischen Modell einnimmt. Daher ist die positionelle Information inh\u00e4rent, wenn die Morpheme gegeben sind. Zweitens verwendet der Satz-Encoder die so genannten ungebundenen, relativ positionellen Einbettungen, die k\u00fcrzlich auf der ICLR-Konferenz ver\u00f6ffentlicht wurden. Diese positionellen Einbettungen entkoppeln im Wesentlichen positionelle Korrelationen von Token hin zu einer Token-Aufmerksamkeitsberechnung. \u00c4hnlich wie BERT verwenden wir ein maskiertes Sprachmodell als Vortrainingsziel. Im Wesentlichen m\u00fcssen wir sowohl den Stamm als auch die Affixe, aus denen die W\u00f6rter bestehen, vorhersagen. W\u00e4hrend des Vortrainings werden f\u00fcnfzehn Prozent aller W\u00f6rter f\u00fcr die Vorhersage ber\u00fccksichtigt, von denen achtzig Prozent maskiert, zehn Prozent mit zuf\u00e4lligen W\u00f6rtern ausgetauscht und zehn Prozent unver\u00e4ndert gelassen werden. F\u00fcr die Vorhersage der Affixe stehen wir vor einem Multi-Label-Klassifikationsproblem. Daher fassen wir entweder die Affixe zu einer festen Reihe von Gruppen zusammen und sagen die Gruppe als Klassenlabel voraus. Die andere M\u00f6glichkeit ist die Vorhersage der Affixe durch einen Wahrscheinlichkeitsvektor. Wir bewerten beide Ans\u00e4tze in unseren Experimenten. Wir trainieren KinyaBERT mit etwa zweieinhalb Gigabyte an Text in Kinyarwanda vor und vergleichen es mit drei Modellen der Baseline. Eines davon ist ein mehrsprachiges Modell namens XLM-R, das mit gro\u00dfen Textkorpora trainiert wird, die aus mehreren Sprachen bestehen. Die beiden anderen Baselines werden mit demselben Text auf Kinyarwanda vortrainiert, wobei entweder der byte pair encoding-Algorithmus oder die morphologische Analyse ohne die zweistufige Transformer-Encoder-Architektur verwendet wird. Alle Modelle sind in der Basisarchitektur konfiguriert, die etwa hundert bis hundertzehn Millionen Parameter umfasst, wobei Kinyarwanda mit KinyaBERT die geringste Anzahl von Parametern verwendet. Alle au\u00dfer dem mehrsprachigen Modell werden f\u00fcr zweiunddrei\u00dfigtausend Gradient-Updates vortrainiert, mit einer Batchgr\u00f6\u00dfe von zweitausendf\u00fcnfhundertsechzig Sequenzen in jedem Batch. Wir bewerten die vortrainierten Modelle anhand von drei Gruppen von Aufgaben. Eine davon ist die GLUE-Benchmark, die h\u00e4ufig zur Bewertung der Effektivit\u00e4t von vortrainierten Sprachmodellen verwendet wird. Wir erhalten unsere GLUE-Benchmark-Daten, indem wir die originalen Benchmark-Daten mit Google Translate ins Kinyarwanda \u00fcbersetzen. Die zweite Aufgabe ist die named entity recognition-Benchmark von Kinyarwanda, ein qualitativ hochwertiger Datensatz, der von trainierten Muttersprachlern annotiert wurde. Bei der dritten Aufgabe handelt es sich um eine Kategorisierung von Nachrichten. Hier rufen wir Nachrichtenartikel von verschiedenen Websites ab und sammeln ihre Kategorisierungstags, die von den Autoren zugewiesen wurden. Im Wesentlichen versuchen wir, die gleichen Kategorien vorherzusagen. Sehen wir uns jetzt die Ergebnisse an. Bei der GLUE-Benchmark haben wir festgestellt, dass KinyaBERT durchweg besser abschneidet als die Baseline-Modelle. Hier zeigen wir die durchschnittliche Leistung von zehn Durchl\u00e4ufen zur Feinabstimmung. Wir f\u00fchren auch eine Benutzerevaluation der \u00dcbersetzungen durch, die von Google Translate erstellt werden. Im Wesentlichen bewerteten die Benutzer etwa sechstausend Beispiele und vergaben Noten auf einer Skala von eins bis vier, um die Qualit\u00e4t der \u00dcbersetzungen zu bewerten. Das Ergebnis war, dass viele \u00dcbersetzungen qualitativ schlecht waren. Aber alle Modelle mussten mit der gleichen schlechten Qualit\u00e4t der \u00dcbersetzung umgehen, und die relative Leistung zwischen den Modellen kann immer noch bedeutend festgestellt werden. Bei der Aufgabe Named Entity Recognition stellten wir au\u00dferdem fest, dass KinyaBERT die beste Leistung erbringt, wobei die Variante mit der Verteilungsregression der Affixe am besten abschneidet. Diese Ergebnisse sind auch Mittelwerte von zehn Durchl\u00e4ufen zur Feinabstimmung. Bei der Aufgabe zur Kategorisierung der Nachrichten bekamen wir gemischte Ergebnisse. Fr\u00fchere Arbeiten zur Textklassifizierung f\u00fcr Kinyarwanda hatten herausgefunden, dass eine einfache Schl\u00fcsselworterkennung meistens ausreicht, um diese spezifische Aufgabe zu l\u00f6sen. Daher ist die Verwendung von vortrainierten Sprachmodellen weniger erfolgsversprechend. Jetzt zu dieser besonderen Aufgabe der Kategorisierung von Nachrichten. Wir haben auch eine Ablationsstudie durchgef\u00fchrt, um zu sehen, ob es alternative Strukturen gibt, die die Leistung verbessern. Bei der GLUE-Benchmark haben wir festgestellt, dass die Verwendung von Affix-S\u00e4tzen durchweg besser abschneidet, w\u00e4hrend das Ziel der Affix-Wahrscheinlichkeitsregression die beste Leistung bei der Named Entity Recognition erbringt. Auch wenn man die niedrigen Werte bei der Feinabstimmung betrachtet, stellt man fest, dass KinyaBERT in den meisten F\u00e4llen eine bessere Konvergenz aufweist. Abschlie\u00dfend l\u00e4sst sich sagen, dass diese Arbeit die Effektivit\u00e4t der expliziten Verwendung von morphologischen Informationen in vortrainierten Sprachmodellen bewiesen hat. Die vorgeschlagene zweistufige Transformer-Encoder-Architektur erm\u00f6glicht die Erfassung von morphologischer Komplexit\u00e4t und morphologischer Kompositionalit\u00e4t, die ein wichtiger Aspekt von morphologisch reichen Sprachen ist. Diese Ergebnisse sollten zu weiteren Forschungen \u00fcber morphologie-bewusste, vortrainierte Sprachmodelle motivieren.", "source": ["2/acl_6060/dev/full_wavs/2022.acl-long.367.wav", "samplerate: 16000 Hz", "channels: 1", "duration: 1e+01:35.020 min", "format: WAV (Microsoft) [WAV]", "subtype: Signed 16 bit PCM [PCM_16]"], "source_length": 695019.6875}
{"index": 4, "prediction": "Hallo, mein Name ist Micha\u0142 Pietruszka und das ist mein Vergn\u00fcgen, euch das Papier mit dem Titel Sparsierende Transformer Modelle mit Trainable Representation Pooling vorzustellen, eine Arbeit, die bei Aplica.ai in Zusammenarbeit mit \u0141ukasz Porchman und \u0141ukasz Goncalek durchgef\u00fchrt wurde. Lassen Sie uns mit den Problemen beginnen, die unsere Arbeitsziele haben. Unsere Methode funktioniert gut f\u00fcr die F\u00e4lle, in denen lange Eintr\u00e4ge ber\u00fccksichtigt werden. Rougly sprechen, es ist f\u00fcr die Aufgabenordnungen und Eintr\u00e4ge von \u00fcber 2000 Tokens und die Ziele sind k\u00fcrzer als die angegebenen Eintr\u00e4ge. Dies hat einige spezifische Anwendungen in NLP. Zum Beispiel kann man sich vorstellen, dass mit einem langen Dokument es notwendig ist, es zusammenzustellen, zu klassifizieren, die Frage dar\u00fcber zu beantworten, Informationen zu extrahieren, einige Schl\u00fcsselfrasen zu organisieren. Erinnern wir uns an den Vanilla Transformer und seine Frage seiner Aufmerksamkeit Komplexit\u00e4t, die vom Quadrat der Eingangl\u00e4nge abh\u00e4ngt. In einem Vanilla Transformer mit voller Aufmerksamkeit verbindung, Beziehungen von jedem token zu jedem anderen token m\u00fcssen berechnet werden. Sie sind berufliche Komplexit\u00e4t der Aufmerksamkeit so h\u00e4ngt von der Anzahl der Schicht L, Sequenzl\u00e4nge N, eine andere Sequenzl\u00e4nge, und der Dimensionalit\u00e4t der Repr\u00e4sentationen. Ebenso erzielt der Decoder die Aufmerksamkeit, die auf der rechten Seite gezeichnet wird, die einzige Unterschied hier ist, dass die Ziel-Token an den Eingabe-Token anwesend sind, die in diesem Fall auch in dieser Formel zu sehen sind. Der blaue Punkt stellt die Beziehungen dar, die im Falle der vollst\u00e4ndigen Aufmerksamkeit berechnet werden m\u00fcssen, die wir alle Beziehungen innerhalb der Eingangssektion berechnen m\u00fcssen. Jetzt sehen wir, was passiert, wenn wir eine Blockwise-Code haben, die durch die Einschr\u00e4nkung der Inhaltsaktivit\u00e4t der Tokens funktioniert, so dass sie nur andere Tokens in der N\u00e4he sehen k\u00f6nnen. Der Text wird in Chunken gelesen, die drastisch die Anzahl der Berechnungen auf der Encoderseite reduzieren k\u00f6nnen, aber nicht die Cross-Attention des Decoders verbessern, wie jedes Input-Token an den Decoders \u00fcbertragen wird. Diese Methode ist oft als Fusion in Decoder bezeichnet. Die Verbesserung kann hier als Ver\u00e4nderung einer der Abh\u00e4ngigkeiten von AND auf eine andere konstante M, die die Blockgr\u00f6\u00dfe darstellt, interpretiert werden. Unsere Schl\u00fcsselbeobachtung ist, dass die meisten Tokens irrelevant f\u00fcr eine Vielzahl von Aufgaben sind und fast vollst\u00e4ndig ignoriert werden k\u00f6nnen. Dies ist auf der Slide vorbildlich, wo nur Teile der Eintr\u00e4ge f\u00fcr den gew\u00fcnschten Ausgang relevant sind. Zum Beispiel kann man einen Artikel einmal lesen und die wichtigsten Teile mit einem Highlight markieren und dann eine Zusammenfassung auf der Grundlage dieser Teile nur aus der mittleren Phase erzeugen. Die Kosten der Aufmerksamkeit und der Entscheidung, ob der aktuelle Token wesentlich ist, um die Zusammenfassung zu erzeugen, sind daher billig und h\u00e4ngen nur von der Darstellung des Tokens ab. Die Kombination der hervorgehobenen Tokens ist dank unserer TopKey-Betreiber m\u00f6glich, und ihre Kosten sind unvergesslich. Die Kosten f\u00fcr die Erstellung einer Zusammenfassung aus einem abgeschnittenen Eingang sind auch viel niedriger als in der Vanilla-Modell, wenn der gesamte Eingang ber\u00fccksichtigt wird. Aber hier ist eine Frage. Wie man wichtige Tokens und Backpropagate Gradients durch diese Auswahl ausw\u00e4hlt? Das grundlegende Problem, das wir l\u00f6sen, besteht darin, den trainierbaren Auswahlmechanismus vorzuschlagen, der es dem Gradient erm\u00f6glichen kann, w\u00e4hrend des Trainings zur\u00fcckpropagiert zu werden, so dass das Netzwerk lernen kann, die wichtigsten Tokens zu w\u00e4hlen. genauer gesagt, angesichts bestimmter Einbindungen und ihrer Score, die aus einer einfachen linearen Schicht gewonnen werden, ist die Aufgabe, die h\u00f6chste Score-Inbindungen zur\u00fcckzukehren. Zun\u00e4chst ist die Sequenz verwechselt und Paare vorbereitet, damit die h\u00f6chste Score-Vektor mit dem niedrigsten Score-Vektor genommen wird. Nachfolgend werden die Gewichtszunahmen mit erh\u00f6hten Softmax \u00dcbersch\u00e4tzungen berechnet. Nach jedem Runde des Turniers, neue Vektoren und Punkte werden als eine lineare Kombination dieser Paare mit den erworbenen Gewicht zusammengestellt. So kurz, wir kombinieren sie linear durch die Durchf\u00fchrung von softmax \u00fcber ihre Punkte, und w\u00e4hrend die Kombination von zwei Tokens, einige L\u00e4rm kann produziert werden. Aber es erm\u00f6glicht auch, dass die Gradienten auf alle Eingabeins\u00e4tze verbreitet werden. Kurz gesagt, eine trainable Top K die wir vorschlagen, basiert auf der Ausf\u00fchrung eines Turniers \u00e4hnlichen weichen Auswahl an jedem Schritt. Und aus einer anderen Perspektive, die repr\u00e4sentative Kombination folgt der Coderschicht. Zun\u00e4chst wird jede Repr\u00e4sentation gesch\u00e4tzt und dann nur diejenigen mit den h\u00f6chsten Punkten auf die n\u00e4chste Schicht \u00fcbertragen. Verschl\u00fcsselung kann als Standard-Transformer-Architektur nur durchgef\u00fchrt werden. Von der vollst\u00e4ndigen Eingabe. Es ist jedoch m\u00f6glich, Text in Bl\u00f6cken zu verarbeiten, die fester L\u00e4nge haben, und weltweit die beste Repr\u00e4sentation zu w\u00e4hlen. Hier ein Beispiel f\u00fcr die nach der Codierung eingef\u00fchrte Repr\u00e4sentation. Dies hat einen direkten Einfluss auf die Kosten der grossen Aufmerksamkeit, die nicht auf den Eingang abh\u00e4ngt, sondern auf die konstante K, die die gepolizierte Lane darstellt. Dieser st\u00e4ndig informiert, wie viele Vertretungen ausgew\u00e4hlt werden und an den Decoder \u00fcbertragen werden, und erzeugt eine Zusammenfassung aus einem abgeschnittenen Text ist deutlich billiger als die vorherige L\u00f6sung da die Sequenzl\u00e4nge durch eine gro\u00dfe Fraktion abgeschnitten werden kann. Zum Beispiel haben wir erfolgreich K von 16 oder sogar 60 mal 4 oder sogar 64 mal kleiner als der Wert von n in unserem Experiment verwendet. Bitte beachten Sie, dass die positiven Auswirkungen von Blockwise-Coding und Selbstaufmerksamkeit erhalten werden. Die Berechnungskosten der Aufmerksamkeit h\u00e4ngen vom Quadrat des Eingangsl\u00e4nges ab. Die Verringerung der Eingabe fr\u00fcher w\u00e4hrend des Codierungsprozesses kann die Kosten f\u00fcr die Pyramidion-Modell erheblich senken. Wir verk\u00fcrzen die Gr\u00f6\u00dfe der Repr\u00e4sentation auf der Produktion jedes gew\u00e4hlten Schicht, was zu einer exponentiellen Reduktion der Berechnungskosten als die Verschl\u00fcsselung der Einnahmen f\u00fchrt. Wie Sie sehen k\u00f6nnen, ist die Gesamtbetragskosten eines vollst\u00e4ndigen Coders hier weniger als doppelt die Kosten des vollst\u00e4ndigen ersten Schichtes. Wenn die B\u00fcndelung fr\u00fcher eingef\u00fchrt wird, ist die Summe aller purpuren Quadraten so auf eine konstante, nicht abh\u00e4ngig von der Anzahl der Schichten L, sondern auf die konstante C gebunden. C, die durch die Platzierung der B\u00fcndelschichten innerhalb des Netzwerks beeinflusst werden kann. Unsere Verbesserungen wurden auf 8000 Tokens langen Eintr\u00e4gen benchmarkt, und die Figur zeigt, dass beim B\u00fcndeln die beste Skalierbarkeit f\u00fcr die Tiefe des Netzwerks erreicht wird. Hier kann man bemerken, dass die Pyramide von 24 Schichten zu trainieren billiger sein kann. als eine zweistufige Vanille-Transformator auf solchen langen Eing\u00e4ngen zu trainieren. Nicht zu erw\u00e4hnen, wie leicht der Vanille-Transformator f\u00fcr so lange Eingabe aus dem Ged\u00e4chtnis gehen kann. Die qualitative Vergleich unserer trainierten tiefen Pyramide mit anderen Baselinen wird auf dem langen Dokument-Summarisierungs-Task durchgef\u00fchrt, in dem der K\u00f6rper eines Algorithms gegeben wird. Artikel aus Archive oder PubMed, die Aufgabe ist es abstrakt zu erzeugen. Wie man Blockwise sehen kann, die unsere Baseline auf der Ebene der j\u00fcngsten state-of-the-art Modelle durchgef\u00fchrt wird, w\u00e4hrend die tiefe Pyramide beibeh\u00e4lt oder verbessert die Leistung dieser wettbewerbsf\u00e4higen Baseline. Gleichzeitig ist unser Modell 80% schneller zu trainieren und \u00fcber 450% schneller im Vergleich zur Blockwise-Basislinie. Beide Modelle haben viel niedrige Parameterzahlen und wurden von Schnitt auf den ausgew\u00e4hlten Aufgaben ausgebildet. Um eine \u00e4hnliche Leistung zu erzielen, mussten wir mehr Parameter und Vor-Training-Grundmodelle und zus\u00e4tzliche Sprachvor-Training-Objektive verwenden, um eine \u00e4hnliche Leistung zu erzielen. Wir bitten Sie, unsere vollst\u00e4ndige Dokumente zu lesen und unseren GitHub-Code zu verwenden. Danke f\u00fcr die Beobachtung. Danke f\u00fcr die Beobachtung.", "delays": [3000.0, 3000.0, 3000.0, 3500.0, 4000.0, 5000.0, 5000.0, 5000.0, 5000.0, 5000.0, 6000.0, 6000.0, 7000.0, 7000.0, 8000.0, 8000.0, 8000.0, 11000.0, 14000.0, 14000.0, 14000.0, 14000.0, 14000.0, 14500.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 15000.0, 16000.0, 16500.0, 16500.0, 19500.0, 19500.0, 19500.0, 19500.0, 20500.0, 20500.0, 20500.0, 20500.0, 23000.0, 23000.0, 23000.0, 23000.0, 23000.0, 23000.0, 23000.0, 24000.0, 24000.0, 24000.0, 24000.0, 25500.0, 25500.0, 25500.0, 25500.0, 25500.0, 27500.0, 27500.0, 27500.0, 27500.0, 28500.0, 28500.0, 28500.0, 28500.0, 28500.0, 29000.0, 29500.0, 30000.0, 30000.0, 30500.0, 31500.0, 31500.0, 32000.0, 32000.0, 32000.0, 33500.0, 33500.0, 36500.0, 36500.0, 36500.0, 36500.0, 36500.0, 38500.0, 38500.0, 38500.0, 39000.0, 39000.0, 39000.0, 39000.0, 40500.0, 41000.0, 41000.0, 41000.0, 41500.0, 42000.0, 42000.0, 43500.0, 43500.0, 43500.0, 43500.0, 45000.0, 45000.0, 45000.0, 45000.0, 45000.0, 45500.0, 45500.0, 47000.0, 47000.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 51500.0, 54500.0, 55500.0, 55500.0, 55500.0, 55500.0, 55500.0, 55500.0, 55500.0, 55500.0, 56500.0, 57500.0, 57500.0, 58000.0, 58000.0, 61000.0, 61000.0, 61000.0, 61000.0, 61000.0, 63500.0, 64000.0, 64000.0, 64500.0, 64500.0, 64500.0, 66500.0, 67000.0, 67000.0, 67500.0, 67500.0, 68000.0, 68500.0, 69000.0, 69500.0, 69500.0, 71000.0, 71000.0, 71000.0, 72000.0, 72000.0, 80000.0, 80000.0, 80000.0, 80000.0, 80000.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 81500.0, 82500.0, 82500.0, 83000.0, 84000.0, 85500.0, 85500.0, 85500.0, 85500.0, 91500.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92000.0, 92500.0, 92500.0, 92500.0, 92500.0, 94000.0, 94000.0, 94500.0, 94500.0, 95500.0, 96000.0, 97500.0, 98000.0, 98000.0, 100000.0, 101000.0, 101000.0, 101000.0, 101000.0, 101000.0, 101500.0, 101500.0, 104000.0, 104000.0, 104000.0, 104000.0, 104000.0, 105500.0, 105500.0, 106000.0, 106000.0, 108000.0, 108000.0, 108000.0, 108000.0, 109000.0, 109000.0, 109000.0, 110000.0, 110000.0, 110000.0, 110000.0, 112000.0, 115000.0, 115000.0, 115000.0, 115000.0, 115000.0, 115000.0, 115000.0, 115000.0, 115000.0, 116000.0, 117000.0, 117000.0, 117000.0, 117500.0, 117500.0, 117500.0, 118000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 120000.0, 123500.0, 124000.0, 124000.0, 124000.0, 124000.0, 124000.0, 124000.0, 124000.0, 124000.0, 124000.0, 127000.0, 127000.0, 127000.0, 127000.0, 127000.0, 127000.0, 128000.0, 128000.0, 128500.0, 128500.0, 128500.0, 129000.0, 129000.0, 129500.0, 131000.0, 131000.0, 131000.0, 131500.0, 132000.0, 133500.0, 133500.0, 133500.0, 133500.0, 133500.0, 134000.0, 137000.0, 137000.0, 137000.0, 137000.0, 137500.0, 137500.0, 138000.0, 138000.0, 139500.0, 141000.0, 141000.0, 141000.0, 141000.0, 142000.0, 142000.0, 142000.0, 142500.0, 144000.0, 144000.0, 144000.0, 144000.0, 144000.0, 146500.0, 146500.0, 147000.0, 147000.0, 149000.0, 149500.0, 151000.0, 151000.0, 151000.0, 151000.0, 151500.0, 152000.0, 152000.0, 152000.0, 154000.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 159500.0, 160500.0, 160500.0, 160500.0, 160500.0, 161000.0, 161000.0, 162500.0, 163000.0, 165000.0, 165000.0, 165000.0, 165000.0, 165500.0, 165500.0, 166000.0, 168000.0, 168000.0, 168000.0, 168000.0, 168000.0, 173000.0, 173000.0, 173000.0, 173500.0, 173500.0, 173500.0, 173500.0, 173500.0, 173500.0, 174000.0, 175500.0, 176000.0, 181500.0, 181500.0, 181500.0, 181500.0, 181500.0, 183000.0, 183500.0, 183500.0, 183500.0, 184000.0, 185000.0, 185500.0, 186000.0, 186000.0, 187500.0, 187500.0, 187500.0, 187500.0, 188500.0, 189000.0, 189500.0, 189500.0, 189500.0, 191000.0, 191000.0, 191000.0, 191000.0, 191000.0, 191500.0, 191500.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 192000.0, 194500.0, 194500.0, 195500.0, 195500.0, 196000.0, 197000.0, 197000.0, 197000.0, 197500.0, 197500.0, 197500.0, 198000.0, 198500.0, 198500.0, 199000.0, 199500.0, 199500.0, 200500.0, 200500.0, 201000.0, 201000.0, 201000.0, 201500.0, 201500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 205500.0, 209000.0, 209500.0, 209500.0, 209500.0, 209500.0, 209500.0, 209500.0, 209500.0, 211000.0, 215000.0, 215000.0, 215000.0, 215000.0, 215000.0, 215000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 218000.0, 219500.0, 219500.0, 219500.0, 219500.0, 219500.0, 219500.0, 221000.0, 221000.0, 221500.0, 221500.0, 226000.0, 226000.0, 226000.0, 226500.0, 226500.0, 226500.0, 226500.0, 226500.0, 228000.0, 228000.0, 228000.0, 228000.0, 228500.0, 228500.0, 229500.0, 232000.0, 232000.0, 232000.0, 232000.0, 233000.0, 233000.0, 233000.0, 233000.0, 233000.0, 234000.0, 234500.0, 234500.0, 237500.0, 237500.0, 237500.0, 237500.0, 238000.0, 239500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 241500.0, 242000.0, 242000.0, 246000.0, 246000.0, 246000.0, 246000.0, 246000.0, 246000.0, 246000.0, 251500.0, 251500.0, 251500.0, 251500.0, 251500.0, 251500.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 253000.0, 254000.0, 254000.0, 254000.0, 254000.0, 254500.0, 254500.0, 256500.0, 256500.0, 257500.0, 257500.0, 257500.0, 259000.0, 259500.0, 259500.0, 260000.0, 261000.0, 261000.0, 261000.0, 261500.0, 262500.0, 263500.0, 264000.0, 264000.0, 264000.0, 264000.0, 264000.0, 272000.0, 272000.0, 272000.0, 272000.0, 272000.0, 272000.0, 272000.0, 272000.0, 272000.0, 273000.0, 273000.0, 276500.0, 276500.0, 276500.0, 276500.0, 276500.0, 276500.0, 277000.0, 278500.0, 278500.0, 280000.0, 280000.0, 280000.0, 281500.0, 281500.0, 281500.0, 286000.0, 286000.0, 286000.0, 286000.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 291500.0, 292500.0, 295500.0, 296500.0, 296500.0, 296500.0, 298000.0, 298000.0, 298000.0, 298000.0, 298500.0, 298500.0, 299000.0, 299500.0, 302500.0, 302500.0, 304000.0, 304000.0, 304000.0, 305000.0, 305000.0, 305000.0, 305000.0, 307000.0, 307000.0, 307000.0, 307000.0, 307000.0, 309000.0, 309500.0, 310500.0, 310500.0, 310500.0, 311500.0, 314000.0, 314000.0, 315000.0, 315000.0, 315000.0, 315000.0, 315500.0, 315500.0, 315500.0, 317000.0, 318000.0, 318000.0, 318000.0, 318000.0, 318000.0, 320000.0, 320500.0, 320500.0, 320500.0, 320500.0, 321500.0, 324500.0, 324500.0, 324500.0, 324500.0, 324500.0, 326000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 328000.0, 329000.0, 329000.0, 329500.0, 329500.0, 330000.0, 332500.0, 332500.0, 332500.0, 332500.0, 332500.0, 333500.0, 334000.0, 334500.0, 336000.0, 336000.0, 336000.0, 336000.0, 338000.0, 338000.0, 340000.0, 340000.0, 341000.0, 341000.0, 341000.0, 341000.0, 341500.0, 341500.0, 342000.0, 342000.0, 344500.0, 344500.0, 344500.0, 344500.0, 350000.0, 350000.0, 350000.0, 350000.0, 350000.0, 350000.0, 350000.0, 350000.0, 351500.0, 351500.0, 351500.0, 356000.0, 356000.0, 356000.0, 356000.0, 356000.0, 356000.0, 356000.0, 357000.0, 358500.0, 358500.0, 358500.0, 358500.0, 358500.0, 358500.0, 358500.0, 359000.0, 364000.0, 364000.0, 364000.0, 364000.0, 364000.0, 364000.0, 364000.0, 367500.0, 367500.0, 367500.0, 368500.0, 368500.0, 368500.0, 368500.0, 368500.0, 368500.0, 368500.0, 368500.0, 370500.0, 370500.0, 370500.0, 370500.0, 371500.0, 372000.0, 372500.0, 372500.0, 373000.0, 375500.0, 376000.0, 376000.0, 376000.0, 376000.0, 376500.0, 376500.0, 376500.0, 376500.0, 377500.0, 378000.0, 378000.0, 378500.0, 379000.0, 379500.0, 380000.0, 380000.0, 383000.0, 383000.0, 383000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 384000.0, 389000.0, 389000.0, 389000.0, 389000.0, 389000.0, 389500.0, 389500.0, 389500.0, 389500.0, 390000.0, 391500.0, 391500.0, 392500.0, 392500.0, 392500.0, 393500.0, 393500.0, 394000.0, 395500.0, 395500.0, 395500.0, 395500.0, 403000.0, 403000.0, 403000.0, 403000.0, 403000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 404000.0, 408000.0, 408000.0, 408000.0, 411000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412000.0, 412500.0, 418000.0, 418000.0, 419000.0, 419000.0, 419000.0, 419000.0, 419000.0, 419000.0, 419000.0, 419000.0, 419000.0, 421000.0, 422000.0, 422000.0, 422000.0, 426000.0, 426000.0, 426000.0, 427000.0, 427500.0, 427500.0, 427500.0, 427500.0, 427500.0, 427500.0, 427500.0, 427500.0, 427500.0, 429500.0, 429500.0, 429500.0, 429500.0, 430500.0, 430500.0, 430500.0, 431000.0, 434500.0, 434500.0, 434500.0, 434500.0, 434500.0, 434500.0, 434500.0, 435500.0, 435500.0, 436000.0, 436000.0, 436500.0, 437000.0, 438500.0, 439000.0, 439000.0, 439500.0, 440000.0, 442000.0, 442000.0, 442500.0, 442500.0, 442500.0, 442500.0, 443000.0, 445000.0, 445000.0, 447500.0, 447500.0, 449000.0, 449000.0, 449000.0, 449000.0, 449000.0, 449000.0, 449000.0, 449000.0, 449500.0, 449500.0, 450000.0, 451500.0, 451500.0, 452500.0, 452500.0, 454000.0, 454000.0, 454000.0, 454000.0, 454000.0, 455000.0, 456000.0, 456000.0, 456000.0, 456000.0, 456000.0, 456000.0, 456000.0, 458500.0, 458500.0, 459500.0, 459500.0, 460500.0, 460500.0, 468500.0, 468500.0, 468500.0, 468500.0, 468500.0, 468500.0, 468500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 469500.0, 473500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 477500.0, 478500.0, 478500.0, 478500.0, 479500.0, 479500.0, 479500.0, 479500.0, 479500.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 480000.0, 484500.0, 484500.0, 484500.0, 484500.0, 492000.0, 492000.0, 492000.0, 492000.0, 492000.0, 492000.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 496500.0, 498500.0, 499000.0, 499000.0, 499000.0, 499000.0, 499500.0, 499500.0, 500000.0, 500000.0, 500500.0, 501000.0, 502000.0, 502000.0, 502000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 504000.0, 508000.0, 508500.0, 508500.0, 508500.0, 508500.0, 508500.0, 508500.0, 513000.0, 513000.0, 513000.0, 513000.0, 513000.0, 516000.0, 516000.0, 517000.0, 517000.0, 517000.0, 518000.0, 518000.0, 518000.0, 518000.0, 518000.0, 518500.0, 519000.0, 520000.0, 521000.0, 521500.0, 522000.0, 522000.0, 522000.0, 523000.0, 523000.0, 523000.0, 524000.0, 524000.0, 524500.0, 525000.0, 525500.0, 526500.0, 528000.0, 528000.0, 530500.0, 531000.0, 531000.0, 533000.0, 534500.0, 534500.0, 535000.0, 535000.0, 535500.0, 536500.0, 537000.0, 537500.0, 544000.0, 544000.0, 544000.0, 544000.0, 545000.0, 545000.0, 545500.0, 546000.0, 546500.0, 546500.0, 546500.0, 546500.0, 552000.0, 552000.0, 552000.0, 552000.0, 552000.0, 552000.0, 552000.0, 555000.0, 555000.0, 555000.0, 556000.0, 556000.0, 556000.0, 558500.0, 559000.0, 559000.0, 559000.0, 559000.0, 566000.0, 566000.0, 566000.0, 571500.0, 571500.0, 571500.0, 571500.0, 571500.0, 571500.0, 571500.0, 571500.0, 572500.0, 572500.0, 572500.0, 572500.0, 573000.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 575500.0, 576000.0, 576000.0, 576000.0, 576000.0, 577237.3125, 577237.3125, 577237.3125, 577237.3125], "elapsed": [4254.062652587891, 4254.062652587891, 4254.062652587891, 5089.926958084106, 6003.673315048218, 7815.786361694336, 7815.786361694336, 7815.786361694336, 7815.786361694336, 7815.786361694336, 9793.38812828064, 9793.38812828064, 11789.300680160522, 11789.300680160522, 13771.47364616394, 13771.47364616394, 13771.47364616394, 20571.370601654053, 27443.22109222412, 27443.22109222412, 27443.22109222412, 27443.22109222412, 27443.22109222412, 28833.9741230011, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 30383.687496185303, 33139.52040672302, 34612.560987472534, 34612.560987472534, 42692.4946308136, 42692.4946308136, 42692.4946308136, 42692.4946308136, 45905.87830543518, 45905.87830543518, 45905.87830543518, 45905.87830543518, 53108.24227333069, 53108.24227333069, 53108.24227333069, 53108.24227333069, 53108.24227333069, 53108.24227333069, 53108.24227333069, 56418.3132648468, 56418.3132648468, 56418.3132648468, 56418.3132648468, 58700.10995864868, 58700.10995864868, 58700.10995864868, 58700.10995864868, 58700.10995864868, 61842.39840507507, 61842.39840507507, 61842.39840507507, 61842.39840507507, 63819.51189041138, 63819.51189041138, 63819.51189041138, 63819.51189041138, 63819.51189041138, 64751.68561935425, 65789.11328315735, 66769.32692527771, 66769.32692527771, 67780.12132644653, 69759.47213172913, 69759.47213172913, 70891.89863204956, 70891.89863204956, 70891.89863204956, 74076.23028755188, 74076.23028755188, 80466.49146080017, 80466.49146080017, 80466.49146080017, 80466.49146080017, 80466.49146080017, 85287.06121444702, 85287.06121444702, 85287.06121444702, 86588.71650695801, 86588.71650695801, 86588.71650695801, 86588.71650695801, 90281.4428806305, 91590.08741378784, 91590.08741378784, 91590.08741378784, 92958.19282531738, 94332.69166946411, 94332.69166946411, 98902.87780761719, 98902.87780761719, 98902.87780761719, 98902.87780761719, 105820.54710388184, 105820.54710388184, 105820.54710388184, 105820.54710388184, 105820.54710388184, 107379.95600700378, 107379.95600700378, 112324.19204711914, 112324.19204711914, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 120977.4398803711, 125989.0022277832, 128009.92035865784, 128009.92035865784, 128009.92035865784, 128009.92035865784, 128009.92035865784, 128009.92035865784, 128009.92035865784, 128009.92035865784, 129769.93012428284, 131720.6289768219, 131720.6289768219, 132773.17833900452, 132773.17833900452, 138946.98023796082, 138946.98023796082, 138946.98023796082, 138946.98023796082, 138946.98023796082, 144054.41880226135, 145217.05317497253, 145217.05317497253, 146450.98447799683, 146450.98447799683, 146450.98447799683, 150789.479970932, 151946.27928733826, 151946.27928733826, 153118.00861358643, 153118.00861358643, 155039.5905971527, 156700.33597946167, 157949.93996620178, 159234.6489429474, 159234.6489429474, 162958.08601379395, 162958.08601379395, 162958.08601379395, 165524.28817749023, 165524.28817749023, 179946.2330341339, 179946.2330341339, 179946.2330341339, 179946.2330341339, 179946.2330341339, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 182822.0031261444, 184853.72638702393, 184853.72638702393, 185934.166431427, 188006.04128837585, 191062.26086616516, 191062.26086616516, 191062.26086616516, 191062.26086616516, 209353.42359542847, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 210882.21836090088, 212189.8069381714, 212189.8069381714, 212189.8069381714, 212189.8069381714, 215780.95293045044, 215780.95293045044, 217066.33758544922, 217066.33758544922, 219706.53986930847, 221115.80514907837, 223287.95218467712, 224157.90510177612, 224157.90510177612, 227272.63164520264, 228974.21288490295, 228974.21288490295, 228974.21288490295, 228974.21288490295, 228974.21288490295, 229886.4026069641, 229886.4026069641, 234063.72952461243, 234063.72952461243, 234063.72952461243, 234063.72952461243, 234063.72952461243, 236874.79162216187, 236874.79162216187, 237894.02723312378, 237894.02723312378, 241929.9075603485, 241929.9075603485, 241929.9075603485, 241929.9075603485, 244044.98410224915, 244044.98410224915, 244044.98410224915, 246443.5200691223, 246443.5200691223, 246443.5200691223, 246443.5200691223, 251057.20376968384, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 259942.94118881226, 262358.35814476013, 265039.63112831116, 265039.63112831116, 265039.63112831116, 266368.66426467896, 266368.66426467896, 266368.66426467896, 267699.79882240295, 273619.96722221375, 273619.96722221375, 273619.96722221375, 273619.96722221375, 273619.96722221375, 273619.96722221375, 278786.29994392395, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 279964.43009376526, 284902.8081893921, 284902.8081893921, 284902.8081893921, 284902.8081893921, 284902.8081893921, 284902.8081893921, 286900.35796165466, 286900.35796165466, 287968.01924705505, 287968.01924705505, 287968.01924705505, 289012.0770931244, 289012.0770931244, 290076.33447647095, 293236.9713783264, 293236.9713783264, 293236.9713783264, 294341.62163734436, 295492.5067424774, 299068.02344322205, 299068.02344322205, 299068.02344322205, 299068.02344322205, 299068.02344322205, 300233.9255809784, 308987.62106895447, 308987.62106895447, 308987.62106895447, 308987.62106895447, 311026.49879455566, 311026.49879455566, 312415.8773422241, 312415.8773422241, 316399.4736671448, 320338.25492858887, 320338.25492858887, 320338.25492858887, 320338.25492858887, 323214.1511440277, 323214.1511440277, 323214.1511440277, 324596.8949794769, 328917.4163341522, 328917.4163341522, 328917.4163341522, 328917.4163341522, 328917.4163341522, 332714.2016887665, 332714.2016887665, 333557.1708679199, 333557.1708679199, 336686.0029697418, 337583.24432373047, 340310.2231025696, 340310.2231025696, 340310.2231025696, 340310.2231025696, 341282.3951244354, 342278.24234962463, 342278.24234962463, 342278.24234962463, 347936.0029697418, 357940.72461128235, 357940.72461128235, 357940.72461128235, 357940.72461128235, 357940.72461128235, 357940.72461128235, 357940.72461128235, 360311.8019104004, 360311.8019104004, 360311.8019104004, 360311.8019104004, 361423.3295917511, 361423.3295917511, 364744.2057132721, 365970.88289260864, 372375.0276565552, 372375.0276565552, 372375.0276565552, 372375.0276565552, 373575.3254890442, 373575.3254890442, 374802.9363155365, 379939.9676322937, 379939.9676322937, 379939.9676322937, 379939.9676322937, 379939.9676322937, 387872.6668357849, 387872.6668357849, 387872.6668357849, 388895.0991630554, 388895.0991630554, 388895.0991630554, 388895.0991630554, 388895.0991630554, 388895.0991630554, 389818.2077407837, 392411.7810726166, 393432.452917099, 403311.98620796204, 403311.98620796204, 403311.98620796204, 403311.98620796204, 403311.98620796204, 406378.6885738373, 407491.18185043335, 407491.18185043335, 407491.18185043335, 408544.9695587158, 410606.63414001465, 411748.44908714294, 412958.3258628845, 412958.3258628845, 416428.3366203308, 416428.3366203308, 416428.3366203308, 416428.3366203308, 418758.0578327179, 420047.38426208496, 421325.17766952515, 421325.17766952515, 421325.17766952515, 425239.95065689087, 425239.95065689087, 425239.95065689087, 425239.95065689087, 425239.95065689087, 426570.104598999, 426570.104598999, 428005.27715682983, 428005.27715682983, 428005.27715682983, 428005.27715682983, 428005.27715682983, 428005.27715682983, 433513.947725296, 433513.947725296, 435114.12143707275, 435114.12143707275, 436052.7994632721, 437932.6329231262, 437932.6329231262, 437932.6329231262, 438900.8078575134, 438900.8078575134, 438900.8078575134, 439898.0131149292, 440819.1945552826, 440819.1945552826, 441832.3438167572, 442904.91580963135, 442904.91580963135, 445050.5783557892, 445050.5783557892, 446183.72654914856, 446183.72654914856, 446183.72654914856, 447322.368144989, 447322.368144989, 455931.67090415955, 455931.67090415955, 455931.67090415955, 455931.67090415955, 455931.67090415955, 455931.67090415955, 464441.4668083191, 466009.00387763977, 466009.00387763977, 466009.00387763977, 466009.00387763977, 466009.00387763977, 466009.00387763977, 466009.00387763977, 470173.67219924927, 481862.36238479614, 481862.36238479614, 481862.36238479614, 481862.36238479614, 481862.36238479614, 481862.36238479614, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 488709.25211906433, 491366.7941093445, 491366.7941093445, 491366.7941093445, 491366.7941093445, 491366.7941093445, 491366.7941093445, 494265.4263973236, 494265.4263973236, 495255.75518608093, 495255.75518608093, 503471.97556495667, 503471.97556495667, 503471.97556495667, 504578.2570838928, 504578.2570838928, 504578.2570838928, 504578.2570838928, 504578.2570838928, 508015.5999660492, 508015.5999660492, 508015.5999660492, 508015.5999660492, 509283.8866710663, 509283.8866710663, 511566.7746067047, 517287.935256958, 517287.935256958, 517287.935256958, 517287.935256958, 519973.530292511, 519973.530292511, 519973.530292511, 519973.530292511, 519973.530292511, 522670.0472831726, 524055.9995174408, 524055.9995174408, 535059.4594478607, 535059.4594478607, 535059.4594478607, 535059.4594478607, 536548.0563640594, 541197.4246501923, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 545486.2976074219, 546344.1073894501, 546344.1073894501, 553243.804693222, 553243.804693222, 553243.804693222, 553243.804693222, 553243.804693222, 553243.804693222, 553243.804693222, 564174.0474700928, 564174.0474700928, 564174.0474700928, 564174.0474700928, 564174.0474700928, 564174.0474700928, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 567828.1366825104, 570317.2018527985, 570317.2018527985, 570317.2018527985, 570317.2018527985, 574372.9865550995, 574372.9865550995, 580220.1111316681, 580220.1111316681, 582797.3148822784, 582797.3148822784, 582797.3148822784, 586530.8930873871, 587928.2596111298, 587928.2596111298, 589298.2563972473, 592169.2955493927, 592169.2955493927, 592169.2955493927, 593574.4798183441, 596546.3171005249, 599708.464384079, 601422.3728179932, 601422.3728179932, 601422.3728179932, 601422.3728179932, 601422.3728179932, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 614809.4911575317, 616655.3776264191, 616655.3776264191, 623940.7768249512, 623940.7768249512, 623940.7768249512, 623940.7768249512, 623940.7768249512, 623940.7768249512, 624966.2821292877, 629924.5138168335, 629924.5138168335, 633858.020067215, 633858.020067215, 633858.020067215, 637154.7183990479, 637154.7183990479, 637154.7183990479, 646821.323633194, 646821.323633194, 646821.323633194, 646821.323633194, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 657127.3875236511, 658620.8200454712, 663319.534778595, 665109.4143390656, 665109.4143390656, 665109.4143390656, 667750.550031662, 667750.550031662, 667750.550031662, 667750.550031662, 668756.9596767426, 668756.9596767426, 669768.3773040771, 670725.0912189484, 676193.2728290558, 676193.2728290558, 679481.8572998047, 679481.8572998047, 679481.8572998047, 681832.3740959167, 681832.3740959167, 681832.3740959167, 681832.3740959167, 688175.7497787476, 688175.7497787476, 688175.7497787476, 688175.7497787476, 688175.7497787476, 692734.0443134308, 693943.7479972839, 696276.5421867371, 696276.5421867371, 696276.5421867371, 699976.8142700195, 704925.2676963806, 704925.2676963806, 706525.9711742401, 706525.9711742401, 706525.9711742401, 706525.9711742401, 707385.7333660126, 707385.7333660126, 707385.7333660126, 709760.8752250671, 711450.8242607117, 711450.8242607117, 711450.8242607117, 711450.8242607117, 711450.8242607117, 714906.4705371857, 715850.8825302124, 715850.8825302124, 715850.8825302124, 715850.8825302124, 717635.6976032257, 723696.968793869, 723696.968793869, 723696.968793869, 723696.968793869, 723696.968793869, 727118.2713508606, 732014.2569541931, 732014.2569541931, 732014.2569541931, 732014.2569541931, 732014.2569541931, 732014.2569541931, 732014.2569541931, 734428.4019470215, 734428.4019470215, 735687.6482963562, 735687.6482963562, 737031.3878059387, 746585.4911804199, 746585.4911804199, 746585.4911804199, 746585.4911804199, 746585.4911804199, 749247.0462322235, 750581.3412666321, 751885.6153488159, 756144.623041153, 756144.623041153, 756144.623041153, 756144.623041153, 759145.055770874, 759145.055770874, 769744.0686225891, 769744.0686225891, 772986.8259429932, 772986.8259429932, 772986.8259429932, 772986.8259429932, 774013.1208896637, 774013.1208896637, 774993.4251308441, 774993.4251308441, 779791.4700508118, 779791.4700508118, 779791.4700508118, 779791.4700508118, 790796.3857650757, 790796.3857650757, 790796.3857650757, 790796.3857650757, 790796.3857650757, 790796.3857650757, 790796.3857650757, 790796.3857650757, 794158.659696579, 794158.659696579, 794158.659696579, 804724.996805191, 804724.996805191, 804724.996805191, 804724.996805191, 804724.996805191, 804724.996805191, 804724.996805191, 807412.2025966644, 811219.3334102631, 811219.3334102631, 811219.3334102631, 811219.3334102631, 811219.3334102631, 811219.3334102631, 811219.3334102631, 813999.1612434387, 823599.8511314392, 823599.8511314392, 823599.8511314392, 823599.8511314392, 823599.8511314392, 823599.8511314392, 823599.8511314392, 829123.9902973175, 829123.9902973175, 829123.9902973175, 831055.9277534485, 831055.9277534485, 831055.9277534485, 831055.9277534485, 831055.9277534485, 831055.9277534485, 831055.9277534485, 831055.9277534485, 834787.4901294708, 834787.4901294708, 834787.4901294708, 834787.4901294708, 836594.202041626, 837621.4473247528, 838623.6281394958, 838623.6281394958, 839621.5524673462, 844520.2097892761, 845835.7882499695, 845835.7882499695, 845835.7882499695, 845835.7882499695, 846991.3611412048, 846991.3611412048, 846991.3611412048, 846991.3611412048, 849156.2876701355, 850313.821554184, 850313.821554184, 851452.8818130493, 852635.5743408203, 853894.7315216064, 855400.5315303802, 855400.5315303802, 864217.693567276, 864217.693567276, 864217.693567276, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 867101.6170978546, 874642.9989337921, 874642.9989337921, 874642.9989337921, 874642.9989337921, 874642.9989337921, 875538.4464263916, 875538.4464263916, 875538.4464263916, 875538.4464263916, 876444.8943138123, 879003.734588623, 879003.734588623, 880810.1041316986, 880810.1041316986, 880810.1041316986, 882722.8698730469, 882722.8698730469, 883731.3976287842, 886730.9486865997, 886730.9486865997, 886730.9486865997, 886730.9486865997, 901812.9436969757, 901812.9436969757, 901812.9436969757, 901812.9436969757, 901812.9436969757, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 904954.4012546539, 914379.964351654, 914379.964351654, 914379.964351654, 919006.4952373505, 921127.9218196869, 921127.9218196869, 921127.9218196869, 921127.9218196869, 921127.9218196869, 921127.9218196869, 922065.6340122223, 931668.3096885681, 931668.3096885681, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 934183.4921836853, 938220.0894355774, 940473.8237857819, 940473.8237857819, 940473.8237857819, 952571.6619491577, 952571.6619491577, 952571.6619491577, 955523.7696170807, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 957083.7135314941, 962674.5154857635, 962674.5154857635, 962674.5154857635, 962674.5154857635, 965521.166563034, 965521.166563034, 965521.166563034, 967000.2846717834, 973678.1208515167, 973678.1208515167, 973678.1208515167, 973678.1208515167, 973678.1208515167, 973678.1208515167, 973678.1208515167, 975338.3750915527, 975338.3750915527, 976226.9814014435, 976226.9814014435, 977150.226354599, 978068.1190490723, 980806.9036006927, 981792.886018753, 981792.886018753, 982758.8851451874, 983765.2032375336, 987768.2783603668, 987768.2783603668, 989008.531332016, 989008.531332016, 989008.531332016, 989008.531332016, 990152.1356105804, 994415.1301383972, 994415.1301383972, 1001045.494556427, 1001045.494556427, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1006306.4670562744, 1007630.9452056885, 1007630.9452056885, 1008952.8288841248, 1012836.3194465637, 1012836.3194465637, 1015683.5420131683, 1015683.5420131683, 1019954.585313797, 1019954.585313797, 1019954.585313797, 1019954.585313797, 1019954.585313797, 1022745.6614971161, 1025803.7238121033, 1025803.7238121033, 1025803.7238121033, 1025803.7238121033, 1025803.7238121033, 1025803.7238121033, 1025803.7238121033, 1029583.8894844055, 1029583.8894844055, 1031512.2678279877, 1031512.2678279877, 1033351.6952991486, 1033351.6952991486, 1048171.2567806244, 1048171.2567806244, 1048171.2567806244, 1048171.2567806244, 1048171.2567806244, 1048171.2567806244, 1048171.2567806244, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1051102.5879383087, 1065291.9840812683, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1075776.8814563751, 1078970.4313278198, 1078970.4313278198, 1078970.4313278198, 1082318.8681602478, 1082318.8681602478, 1082318.8681602478, 1082318.8681602478, 1082318.8681602478, 1083997.679233551, 1083997.679233551, 1083997.679233551, 1083997.679233551, 1083997.679233551, 1083997.679233551, 1083997.679233551, 1094341.9392108917, 1094341.9392108917, 1094341.9392108917, 1094341.9392108917, 1108359.6003055573, 1108359.6003055573, 1108359.6003055573, 1108359.6003055573, 1108359.6003055573, 1108359.6003055573, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1118891.8573856354, 1123715.2886390686, 1125205.239057541, 1125205.239057541, 1125205.239057541, 1125205.239057541, 1126520.0037956238, 1126520.0037956238, 1127846.7676639557, 1127846.7676639557, 1129240.0319576263, 1130657.2818756104, 1133339.0309810638, 1133339.0309810638, 1133339.0309810638, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1143944.1978931427, 1149861.1433506012, 1150818.4442520142, 1150818.4442520142, 1150818.4442520142, 1150818.4442520142, 1150818.4442520142, 1150818.4442520142, 1158447.0303058624, 1158447.0303058624, 1158447.0303058624, 1158447.0303058624, 1158447.0303058624, 1163946.8178749084, 1163946.8178749084, 1165926.573753357, 1165926.573753357, 1165926.573753357, 1168035.0272655487, 1168035.0272655487, 1168035.0272655487, 1168035.0272655487, 1168035.0272655487, 1169070.906162262, 1170151.50308609, 1172379.5018196106, 1174622.5833892822, 1175822.5450515747, 1177020.7901000977, 1177020.7901000977, 1177020.7901000977, 1179458.120584488, 1179458.120584488, 1179458.120584488, 1181887.5653743744, 1181887.5653743744, 1183085.1197242737, 1184319.1237449646, 1185571.1691379547, 1188082.9598903656, 1191809.9992275238, 1191809.9992275238, 1198830.7180404663, 1199669.5384979248, 1199669.5384979248, 1202808.8145256042, 1205406.1102867126, 1205406.1102867126, 1206421.6248989105, 1206421.6248989105, 1207388.168811798, 1209253.5178661346, 1210271.9628810883, 1211227.218389511, 1224344.8016643524, 1224344.8016643524, 1224344.8016643524, 1224344.8016643524, 1226799.4644641876, 1226799.4644641876, 1228002.5136470795, 1229232.3393821716, 1230541.697025299, 1230541.697025299, 1230541.697025299, 1230541.697025299, 1243803.495645523, 1243803.495645523, 1243803.495645523, 1243803.495645523, 1243803.495645523, 1243803.495645523, 1243803.495645523, 1248444.6234703064, 1248444.6234703064, 1248444.6234703064, 1250217.2014713287, 1250217.2014713287, 1250217.2014713287, 1254742.4614429474, 1255921.1509227753, 1255921.1509227753, 1255921.1509227753, 1255921.1509227753, 1270802.3130893707, 1270802.3130893707, 1270802.3130893707, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1282536.6098880768, 1285009.9742412567, 1285009.9742412567, 1285009.9742412567, 1285009.9742412567, 1286246.765613556, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1295741.3325309753, 1297026.2632369995, 1297026.2632369995, 1297026.2632369995, 1297026.2632369995, 1299015.385810852, 1299015.385810852, 1299015.385810852, 1299015.385810852], "prediction_length": 1155, "reference": "Hallo, mein Name ist Micha\u0142 Pietruszka und es ist mir eine Freude, Ihnen das Paper mit dem Titel Sparsifying Transformer Models with Trainable Representation Pooling vorzustellen. Die Arbeit wurde bei Applica KI in Zusammenarbeit mit Lukasz Borchmann und Lukasz Garncarek durchgef\u00fchrt. Lassen Sie mich mit den Problemen beginnen, die in unserer Arbeit abgehandelt werden. Unsere Methode funktioniert gut f\u00fcr die F\u00e4lle, in denen lange Eingaben ber\u00fccksichtigt werden. Grob gesagt ist sie f\u00fcr Aufgaben und Eingaben von \u00fcber zweitausend Token gedacht, wo die Zieltexte k\u00fcrzer als die vorgegebenen Eingaben sind. Dies f\u00fchrt zu einigen spezifischen Anwendungen in der NLP. Man kann sich zum Beispiel vorstellen, dass ein langes Dokument zusammengefasst, klassifiziert und eine Frage dar\u00fcber beantwortet werden muss und Informationen oder einige Schl\u00fcssels\u00e4tze extrahiert werden m\u00fcssen. Erinnern wir uns an den Vanilla-Transformer und sein Problem mit der Aufmerksamkeitskomplexit\u00e4t, die vom Quadrat der Eingabezeile abh\u00e4ngt. Im Vanilla-Transformer m\u00fcssen bei voller Aufmerksamkeitskonnektivit\u00e4t die Relationen jedes Token zu jedem anderen Token berechnet werden. Die rechnerische Komplexit\u00e4t der Aufmerksamkeit h\u00e4ngt von der Reihe der Schichten l, der Sequenzl\u00e4nge n, einer weiteren Sequenzl\u00e4nge und der Dimensionalit\u00e4t der Repr\u00e4sentationen ab. \u00c4hnlich verh\u00e4lt es sich mit der Cross-Attention des Decoders zu diesem Bild auf der rechten Seite, wobei der einzige Unterschied darin besteht, dass die Ziel-Token in diesem Fall auf die Eingabe-Token gerichtet sind. Das sieht man in dieser Formel. Der BLEU-Score stellt Relationen dar, die berechnet werden m\u00fcssen. Im Falle der vollst\u00e4ndigen Aufmerksamkeit m\u00fcssen wir jede Relation innerhalb der Eingabe-Sequenz berechnen. Jetzt sehen wir, was passiert, wenn wir einen blockweisen Encoder haben, der die Konnektivit\u00e4t der Token so einschr\u00e4nkt, dass sie nur andere Token in der N\u00e4he sehen k\u00f6nnen. Der Text wird in Bl\u00f6cken gelesen, was die Reihe der Berechnungen auf der Encoder-Seite drastisch reduzieren kann. Aber die Cross-Attention des Decoders wird so nicht verbessert, da jedes Eingabe-Token ohnehin an den Decoder weitergegeben wird. Diese Methode wird oft als Fusion im Decoder bezeichnet. Die Verbesserung hier kann so interpretiert werden, dass eine der Abh\u00e4ngigkeiten von n durch eine andere Konstante m ersetzt wird, die die Blockgr\u00f6\u00dfe darstellt. Unsere wichtigste Beobachtung ist, dass die meisten Token f\u00fcr eine Vielzahl von Aufgaben irrelevant sind und fast vollst\u00e4ndig vernachl\u00e4ssigt werden k\u00f6nnen. Dies ist beispielhaft auf der Folie dargestellt. Nur ein Teil der Eingaben ist f\u00fcr die gew\u00fcnschte Ausgabe relevant. Zum Beispiel. Man kann einen Artikel einmal lesen, die wichtigsten Teile mit einem Textmarker markieren und dann eine Zusammenfassung erstellen, die nur auf diesem Teil aus der mittleren Phase basiert. Die Kosten f\u00fcr die Hervorhebung und die Entscheidung, ob das aktuelle Token f\u00fcr die Erstellung der Zusammenfassung wesentlich ist, sind somit gering und h\u00e4ngen nur von der Repr\u00e4sentation des Token ab. Das Pooling der hervorgehobenen Token ist m\u00f6glich. Das ist unserem Top-k-Operator zu verdanken und die Kosten sind vernachl\u00e4ssigbar. Die Kosten f\u00fcr die Erstellung einer Zusammenfassung aus einer gek\u00fcrzten Eingabe sind ebenfalls viel niedriger als beim Vanilla-Modell, wenn die gesamte Eingabe ber\u00fccksichtigt wird. Aber hier stellt sich eine Frage. Wie kann man wichtige Token ausw\u00e4hlen und Gradienten zu dieser Auswahl zur\u00fcckverfolgen? Das zugrundeliegende wesentliche Problem, das wir l\u00f6sen, besteht darin, einen trainierbaren Auswahlmechanismus vorzuschlagen. Dieser erm\u00f6glicht es, dass der Gradient w\u00e4hrend des Trainings r\u00fcckverfolgt werden kann, sodass das Netzwerk lernen kann, die wichtigsten Token auszuw\u00e4hlen. Pr\u00e4ziser ausgedr\u00fcckt: Angesichts einiger Unterwert-Einbettungen, die aus einer einfachen linearen Schicht stammen, besteht die Aufgabe darin, die Einbettungen mit dem h\u00f6chsten Score zu ermitteln. Zun\u00e4chst wird die Sequenz permutiert, und es werden Paare gebildet, sodass der h\u00f6her bewertete Vektor mit dem niedriger bewerteten zusammengebracht wird. Anschlie\u00dfend werden die Gewichtungen mithilfe von einer verst\u00e4rkten Softmax \u00fcber die Scores berechnet. Nach jeder Turnierrunde werden neue Vektoren und Scores als lineare Kombination dieser Paare mit den erhaltenen Gewichtungen zusammengestellt. Kurz gesagt kombinieren wir sie linear, indem wir eine Softmax \u00fcber ihre Scores durchf\u00fchren. W\u00e4hrend man zwei Token kombiniert, kann auch schlechte Qualit\u00e4t erzeugt werden. Aber auch die \u00dcbertragung der Gradienten auf alle eingegebenen Einbettungen wird erm\u00f6glicht. Kurz gesagt wollen wir ein trainierbares Top-k vorschlagen, das eine turnier\u00e4hnliche Soft-Selection bei jedem Schritt ausf\u00fchrt. Aus einem anderen Blickwinkel betrachtet, folgt das Repr\u00e4sentationspooling auf die Encoder-Schicht. Zun\u00e4chst wird jede Repr\u00e4sentation bewertet. Dann werden nur jene mit den h\u00f6chsten Scores an die n\u00e4chste Ebene weitergegeben. Die Kodierung kann wie in der Standard-Transformer-Architektur auf die volle L\u00e4nge der Eingabe durchgef\u00fchrt werden. Es ist jedoch m\u00f6glich, Text in Bl\u00f6cken mit fester L\u00e4nge zu verarbeiten und global die beste Repr\u00e4sentation zu w\u00e4hlen. Hier ist ein Beispiel f\u00fcr das nach dem Encoder eingef\u00fchrte Repr\u00e4sentationspooling. Dies hatte einen direkten Einfluss auf die Ursache der Cross-Attention. Diese h\u00e4ngt nicht von der L\u00e4nge der Eingabe N ab, sondern von der Konstante K, welche die gepoolte L\u00e4nge darstellt. Diese Konstante gibt an, wie viele Repr\u00e4sentationen ausgew\u00e4hlt und an den Decoder \u00fcbergeben werden. Die Erstellung einer Zusammenfassung aus einem k\u00fcrzeren Text ist wesentlich billiger als die fr\u00fchere L\u00f6sung. Die Sequenzl\u00e4nge kann um einen gro\u00dfen Faktor verk\u00fcrzt werden. Wir haben beispielsweise in unseren Experimenten k erfolgreich sechzehnmal oder sogar vierundsechzigmal kleiner als den Wert von n verwendet. Bitte beachten Sie, dass die positive Wirkung von blockweiser Kodierung und selbst\u00e4ndiger Aufmerksamkeit anhaltend ist. Vergessen Sie nicht, dass die Rechenkosten der Aufmerksamkeit vom Quadrat der eingegebenen L\u00e4nge abh\u00e4ngen. Die Reduzierung der Eingabe zu einem fr\u00fcheren Zeitpunkt w\u00e4hrend des Kodierungsprozesses kann die Kosten erheblich senken. F\u00fcr das Pyramidion-Modell haben wir die Gr\u00f6\u00dfe der Repr\u00e4sentation bei der Ausgabe jeder ausgew\u00e4hlten Schicht eingegrenzt, was zu einer exponentiellen Verringerung der Rechenkosten f\u00fchrt, wenn die Kodierung fortschreitet. Wie Sie sehen k\u00f6nnen, sind die gesamten Rechenkosten eines vollst\u00e4ndigen Encoders hier weniger als doppelt so hoch als die Kosten der ersten Schicht in voller Gr\u00f6\u00dfe. Wenn das Pooling fr\u00fcher eingef\u00fchrt wird, wird die Summe aller lila Quadrate auf eine Konstante begrenzt, die nicht von der Anzahl der Schichten abh\u00e4ngt. Die Konstante c kann durch die Anordnung der Pooling-Schichten innerhalb des Netzwerks beeinflusst werden. Unsere Verbesserungen wurden mit Eingaben in der L\u00e4nge von achttausend Token verglichen. Die Abbildung zeigt, dass die beste Skalierbarkeit f\u00fcr die Tiefe des Netzwerks erreicht wird, wenn das Pooling aktiviert ist. Hier kann man feststellen, dass bei solchen langen Eingaben das Training des Pyramidions mit 24 Schichten billiger sein kann als das Training eines zweischichtigen Vanilla-Transformers. Ganz zu schweigen davon, wie schnell ein Vanilla-Transformer bei einer so langen Eingabe ungen\u00fcgend Speicher haben kann. Der qualitative Vergleich unseres Trend-Pyramidions mit anderen Baselines wird anhand der langen Aufgabe mit der Zusammenfassung des Dokuments durchgef\u00fchrt. Alternativ kann der Hauptteil eines Artikels von arXiv oder PubMed herangezogen werden, wo die Aufgabe darin besteht, eine Zusammenfassung zu erstellen. Man kann also sehen, dass der blockweise Ansatz, der unsere Baseline darstellt, auf modernsten Modellen basiert, w\u00e4hrend das Pyramidion die Leistung dieser konkurrenzf\u00e4higen Baseline beibeh\u00e4lt oder verbessert. Gleichzeitig ist unser Modell zu achtzig Prozent schneller zu trainieren und zu \u00fcber vierhundertf\u00fcnfzig Prozent schneller bei der Inferenz im Vergleich zur blockweisen Baseline. Beide Modelle haben viel weniger Parameter und wurden von Grund auf f\u00fcr die ausgew\u00e4hlten Aufgaben trainiert. Fr\u00fchere Ans\u00e4tze zur Erzielung einer \u00e4hnlichen Leistung mussten mehr Parameter verwenden sowie vortrainierte grundlegende Modelle und zus\u00e4tzliche Vortrainingsziele bei der Sprache nutzen. Wir laden Sie ein, unser vollst\u00e4ndiges Paper zu lesen und unseren GitHub-Code zu verwenden. Vielen Dank f\u00fcrs Zuschauen.", "source": ["2/acl_6060/dev/full_wavs/2022.acl-long.590.wav", "samplerate: 16000 Hz", "channels: 1", "duration: 09:37.237 min", "format: WAV (Microsoft) [WAV]", "subtype: Signed 16 bit PCM [PCM_16]"], "source_length": 577237.3125}
